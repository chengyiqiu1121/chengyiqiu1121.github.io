<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width":280,"display":"post","offset":10,"onmobile":true},"hljswrap":true,"copycode":{"enable":true,"style":"mac","show_result":true},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="chengyiqiu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="chengyiqiu"
      src="/images/pig.gif">
  <p class="site-author-name" itemprop="name">chengyiqiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chengyiqiu1121" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chengyiqiu1121" rel="noopener me" target="_blank">GitHub</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/09/summary-231209/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/09/summary-231209/" class="post-title-link" itemprop="url">summary_231209</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-09 19:07:56" itemprop="dateCreated datePublished" datetime="2023-12-09T19:07:56+08:00">2023-12-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-19 12:45:21" itemprop="dateModified" datetime="2024-04-19T12:45:21+08:00">2024-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>对先前阅读的中毒攻击以及后门攻击的论文做一个总结。</p>
<h2 id="评级组"><strong>评级&amp;组</strong></h2>
<p>Baochun Li</p>
<p>Yanjiao Chen</p>
<p>Qian Wang</p>
<table style="width:100%;">
<colgroup>
<col style="width: 71%">
<col style="width: 13%">
<col style="width: 4%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">论文名称</th>
<th style="text-align: center;">期刊/会议</th>
<th style="text-align: center;">等级</th>
<th style="text-align: center;">课题组</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Poisoning</strong> Attacks on
Deep Learning based Wireless Traffic Prediction</td>
<td style="text-align: center;">INFOCOM2022</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Li/Chen</td>
</tr>
<tr class="even">
<td style="text-align: center;">MetaPoison: Practical General-purpose
Clean-label Data <strong>Poisoning</strong></td>
<td style="text-align: center;">NIPS</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">OBLIVION: <strong>Poisoning</strong>
Federated Learning by Inducing Catastrophic Forgetting</td>
<td style="text-align: center;">INFOCOM2023</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Li/Chen</td>
</tr>
<tr class="even">
<td style="text-align: center;">First-Order Efficient General-Purpose
Clean-Label Data <strong>Poisoning</strong></td>
<td style="text-align: center;">INFOCOM2021</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Li</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Data <strong>Poisoning</strong> Attacks
in Internet-of-Vehicle Networks: Taxonomy, State-of-The-Art, and Future
Directions</td>
<td style="text-align: center;">TII</td>
<td style="text-align: center;">C/Q1</td>
<td style="text-align: center;">Chen</td>
</tr>
<tr class="even">
<td style="text-align: center;">REDEEM MYSELF: Purifying
<strong>Backdoors</strong> in Deep Learning Models using Self Attention
Distillation</td>
<td style="text-align: center;">S&amp;P</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Wang/Chen</td>
</tr>
<tr class="odd">
<td style="text-align: center;">PALETTE: Physically-Realizable
<strong>Backdoor</strong> Attacks Against Video Recognition Models</td>
<td style="text-align: center;">TDSC2023</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Wang/Chen</td>
</tr>
<tr class="even">
<td style="text-align: center;">ATTEQ-NN: Attention-based QoE-aware
Evasive <strong>Backdoor</strong> Attacks</td>
<td style="text-align: center;">NDSS2022</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Wang/Chen</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Neural Attention Distillation: Erasing
<strong>Backdoor</strong> Triggers from Deep Neural Networks</td>
<td style="text-align: center;">ICLR2021</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>上面的文章都是关于中毒or后门攻击的，二者都是对抗攻击。中毒攻击在集中式训练或者联邦学习中都可以使用，而后门攻击目前只在集中式场景下。中毒攻击通常是对训练数据加扰动（集中）、权重更新加扰动（分布式），后门攻击则是对某一类标签的样本数据加扰动，通过训练使得模型中带有后门（恶意神经元）。</p>
<p>下面对这几篇论文进行总结。</p>
<h2 id="中毒">中毒</h2>
<h3 id="poisoning-attacks-on-deep-learning-based-wireless-traffic-prediction">Poisoning
Attacks on Deep Learning based Wireless Traffic Prediction</h3>
<p><strong>INFOCOM2022 A</strong></p>
<p>无线流量预测中的深度学习中毒攻击。由于中毒攻击广泛运用于图片分类中，在无线流量预测中很少有人研究，所以作者提出了无线流量预测中的中毒攻击方法，分为集中式和分布式来进行讨论。</p>
<p>本文工作量：</p>
<ul>
<li><p>扰动掩盖策略（集中场景中毒攻击）：顾名思义掩盖<span class="math inline">\(\delta\)</span></p>
<ul>
<li><p>本地会有一个代理模型来进行训练，最小化加了扰动的样本及标签；最大化没加扰动的样本及标签</p>
<figure>
<img data-src="./summary-231209/image-20231211101210133.png" alt="image-20231211101210133">
<figcaption aria-hidden="true">image-20231211101210133</figcaption>
</figure></li>
</ul></li>
<li><p>调优&amp;缩放方法（分布场景中毒攻击）</p>
<figure>
<img data-src="./summary-231209/image-20231211141737025.png" alt="image-20231211141737025">
<figcaption aria-hidden="true">image-20231211141737025</figcaption>
</figure></li>
<li><p>数据清洗（集中场景 防御方法）</p>
<ul>
<li><span class="math inline">\(y_i^k\)</span>是<span class="math inline">\(v_t\)</span>，可以理解成要预测的标签；<span class="math inline">\(x_i^k[j]\)</span>代表的是<span class="math inline">\(v_{t-\tau}\)</span>，也就是样本。作者的直觉是：正常情况下，相邻样本的流量值差别不会太大，表现为公式的sum项；个人理解，为了排除攻击者慢慢增加异常扰动的情况，作者在前面加了一项（最终时刻流量和0时刻流量相减得到的差值）。这整个公式得到的就是adjacent
distance。作者会在所有的训练集上计算这个距离，并将前100p%的视为中毒数据，丢弃掉（p为投毒率）</li>
</ul>
<figure>
<img data-src="./summary-231209/image-20231211142955400.png" alt="image-20231211142955400">
<figcaption aria-hidden="true">image-20231211142955400</figcaption>
</figure></li>
<li><p>异常检测（分布场景 防御方法的）</p>
<ol type="1">
<li>计算出所有model update的<span class="math inline">\(L_2\)</span>范数，取中位数记为<span class="math inline">\(\mu _t\)</span></li>
<li>将所有的<span class="math inline">\(L_2\)</span>范数和<span class="math inline">\(\mu _t\)</span>进行相除，取中位数，记<span class="math inline">\(\sigma _t\)</span></li>
<li>规定，所有model update的最大<span class="math inline">\(L_2\)</span>范数不超过<span class="math inline">\(c_1\mu _t\)</span>；并且不超过<span class="math inline">\(\mu _t+c_2\sigma _t\)</span></li>
</ol>
<p>这个阈值是动态的，在本文的实验中，<span class="math inline">\(c_1=40,c_2=400\)</span>比较好</p></li>
</ul>
<p>跑实验很快，几分钟。</p>
<h3 id="metapoison-practical-general-purpose-clean-label-data-poisoning">MetaPoison:
Practical General-purpose Clean-label Data Poisoning</h3>
<p><strong>NIPS A</strong></p>
<p>元中毒，领域为CV，集中场景，顾名思义就是用一小部分训练数据来投毒，从而影响整个模型的性能。</p>
<p>工作量：</p>
<ol type="1">
<li><p>首先将现实问题描述为双层优化问题</p>
<figure>
<img data-src="./summary-231209/image-20231211150401035.png" alt="image-20231211150401035">
<figcaption aria-hidden="true">image-20231211150401035</figcaption>
</figure>
<figure>
<img data-src="./summary-231209/image-20231211150413211.png" alt="image-20231211150413211">
<figcaption aria-hidden="true">image-20231211150413211</figcaption>
</figure>
<p>可以看到上面有两个参数需要优化（<span class="math inline">\(X_P^*,\theta^*\)</span>）</p></li>
<li><p>双层优化计算量大，因此作者采用先2步SGD来优化<span class="math inline">\(\theta\)</span>，然后再优化<span class="math inline">\(X_p\)</span></p>
<figure>
<img data-src="./summary-231209/image-20231212130424901.png" alt="image-20231212130424901">
<figcaption aria-hidden="true">image-20231212130424901</figcaption>
</figure></li>
<li><p>选择使用集成学习（多个代理模型训练）以及交替训练增加模型的泛化能力。</p></li>
</ol>
<h3 id="first-order-efficient-general-purpose-clean-label-data-poisoning">First-Order
Efficient General-Purpose Clean-Label Data
<strong>Poisoning</strong></h3>
<p><strong>INFOCOM2021 A</strong></p>
<p>领域CV，集中场景，基于MetaPoison。MetaPoison在优化的时候会利用到二阶导数，这会带来比较大的计算量：</p>
<figure>
<img data-src="./summary-231209/image-20231213105123301.png" alt="image-20231213105123301">
<figcaption aria-hidden="true">image-20231213105123301</figcaption>
</figure>
<p>本文中针对这点，以一阶逼近二阶的性能。</p>
<p>工作量：</p>
<p>在更新扰动方面，以一阶来逼近二阶的性能，找到了一阶的优化路径（路径的推断不是很理解）。</p>
<p>为什呢说这个是基于MetaPoison（MP）呢，因为MP最开始是一个双层优化问题，可以把loss看成从山顶走到山下，最优化一个参数就是找出一条下山最短的路。而MP是双层优化，两者相互限制，所以MP里面的作者选择对第一个参数<span class="math inline">\(\theta\)</span>做2次SGD来优化，然后再优化<span class="math inline">\(X_p\)</span>，这样得到的结果肯定不是最优，但性能也很好了。</p>
<p>本文则是针对MP找到的这条路径进行优化，节省计算时间，核心工作就是找到了一阶的优化路径。</p>
<h3 id="oblivion-poisoning-federated-learning-by-inducing-catastrophic-forgetting">OBLIVION:
<strong>Poisoning</strong> Federated Learning by Inducing Catastrophic
Forgetting</h3>
<p>领域CV，分布场景。</p>
<p>工作量：</p>
<ol type="1">
<li>更新优先级：选择优先级高的权重来增加扰动。</li>
<li>灾难性遗忘：对以前提交过的更新也增加扰动。</li>
</ol>
<p>跑实验很慢，五六小时出一次结果。</p>
<h3 id="data-poisoning-attacks-in-internet-of-vehicle-networks-taxonomy-state-of-the-art-and-future-directions">Data
<strong>Poisoning</strong> Attacks in Internet-of-Vehicle Networks:
Taxonomy, State-of-The-Art, and Future Directions</h3>
<p><strong>TII C/SCI1区</strong></p>
<p>这篇文章类似综述，阐述了目前的一些最优的中毒攻击方法以及防御方法。</p>
<p>攻击：</p>
<ol type="1">
<li><p>clean label</p>
<p>干净标签，顾名思义不影响打标签的环节，对训练数据进行投毒。</p></li>
<li><p>dirty label</p>
<p>脏标签，在打标签的时候给样本打上错误的标签。</p></li>
</ol>
<p>clean label更符合现实情况，因此研究的更广泛。</p>
<p>防御：</p>
<ol type="1">
<li><p>基于数据</p>
<p>例如数据消毒，将可能的中毒数据丢弃掉（中毒攻击中使用较多）</p></li>
<li><p>基于模型</p>
<p>基于模型的方法是在训练阶段，会附加一些额外步骤，通过模型的准确率和参数变化，来判断是否有中毒数据。</p></li>
</ol>
<h2 id="后门">后门</h2>
<h3 id="neural-attention-distillation-erasing-backdoor-triggers-from-deep-neural-networks">Neural
Attention Distillation: Erasing Backdoor Triggers from Deep Neural
Networks</h3>
<p><strong>ICLR2021 大佬创办的顶会</strong></p>
<p>因xueluan gong的一篇文章是基于这个的，因此读了这篇文章。</p>
<p>领域CV，集中场景。</p>
<p>提出的框架NAD（<strong>N</strong>eural <strong>A</strong>ttention
<strong>D</strong>istillation，神经注意力蒸馏），思路是：在一小部分干净数据集上，用一个老师模型来对学生模型进行微调，老师模型可以从学生模型中得到，最终需要的就是经过微调后的学生模型。</p>
<figure>
<img data-src="./summary-231209/image-20231213161043850.png" alt="image-20231213161043850">
<figcaption aria-hidden="true">image-20231213161043850</figcaption>
</figure>
<ul>
<li><p><span class="math inline">\(F^l\)</span>：第l层的激活函数的输出结果，T表示老师模型，S表示学生模型，可以看到求出注意力之后各自都进行了归一化。</p></li>
<li><p><span class="math inline">\(\mathcal
A\)</span>：注意力映射，将3维的激活函数输出转换为2维的注意力。</p></li>
</ul>
<p>然后还有一个loss函数负责保证蒸馏（NAD）时的正确率，避免除掉后门的过程中正确率降低太多。</p>
<figure>
<img data-src="./summary-231209/image-20231213161310216.png" alt="image-20231213161310216">
<figcaption aria-hidden="true">image-20231213161310216</figcaption>
</figure>
<h3 id="redeem-myself-purifying-backdoors-in-deep-learning-models-using-self-attention-distillation">REDEEM
MYSELF: Purifying <strong>Backdoors</strong> in Deep Learning Models
using Self Attention Distillation</h3>
<p><strong>S&amp;P A 安全四大</strong></p>
<p>基于上面那篇文章的。</p>
<p>领域CV，集中场景。</p>
<p>上文[32]中使用老师学生模型来蒸馏后门的方法，因为老师模型是从学生模型中得出，因此也可能具有后门，这样最终微调得到的学生模型可能还是会有后门，这篇文章的思路是：自注意力蒸馏，让学生模型的深层从好的浅层学习，从而摆脱老师模型。</p>
<p>工作量：</p>
<ol type="1">
<li><p>注意力表示模块：根据神经元对最终预测结果的重要性，来提取出注意力</p>
<ul>
<li><span class="math inline">\(F_B\)</span>: 带有后门的模型</li>
<li><span class="math inline">\(F_B^l\)</span>: l层激活函数的输出, <span class="math inline">\(\epsilon R^{C_l\times H_l\times W_l}\)</span></li>
<li><span class="math inline">\(\mathcal G:R^{C_l\times H_l\times
W_l}\to R^{H_l\times W_l}\)</span>:
映射函数，由激活函数输出得到注意力</li>
</ul>
<figure>
<img data-src="./summary-231209/image-20231213164250380.png" alt="image-20231213164250380">
<figcaption aria-hidden="true">image-20231213164250380</figcaption>
</figure></li>
<li><p>损失计算模块：根据浅层的注意力，对深层的权重进行调整，同时保证模型预测的准确率。用浅层监督深层。</p>
<figure>
<img data-src="./summary-231209/image-20231213164320493.png" alt="image-20231213164320493">
<figcaption aria-hidden="true">image-20231213164320493</figcaption>
</figure></li>
<li><p>学习率更新模块：跟踪模型在干净数据集上的准确率，来自适应调整学习率。（[32]是每过两个epoch，学习率除10）</p>
<p>本文提出的一种学习率更新的方法，设定了两个条件：<span class="math inline">\(\mathcal C_1,\mathcal C_2\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathcal C_1\)</span>:
当在干净数据上的loss在n个epoch内都没有下降</li>
<li><span class="math inline">\(\mathcal C_2\)</span>:
在干净数据上的loss最大值没有下降</li>
</ul>
<p>若是上面条件有一个发生，那么就将学习率除2。</p></li>
</ol>
<h3 id="palette-physically-realizable-backdoor-attacks-against-video-recognition-models">PALETTE:
Physically-Realizable <strong>Backdoor</strong> Attacks Against Video
Recognition Models</h3>
<p><strong>TDSC2023 A</strong></p>
<p>领域video，集中场景。</p>
<p>视频后门攻击做的人少，本文提出了一种物理可实现的视频动作识别的后门攻击方法。</p>
<p>思路：</p>
<ol type="1">
<li>利用<strong>类似光照效果的RGB偏移</strong>作为触发器，而不是传统的打补丁。</li>
<li>通过滚动操作对<strong>特定的视频帧</strong>进行投毒，增加模型对触发器帧的泛化能力。</li>
</ol>
<p>工作量：</p>
<ol type="1">
<li>触发器生成</li>
<li>滚动操作将触发器帧插入到视频中去</li>
<li>抽样投毒</li>
</ol>
<h3 id="atteq-nn-attention-based-qoe-aware-evasive-backdoor-attacks">ATTEQ-NN:
Attention-based QoE-aware Evasive <strong>Backdoor</strong> Attacks</h3>
<p><strong>NDSS2022 A 安全四大</strong></p>
<p>领域CV，集中场景。</p>
<p>CV后门攻击中没人关注过触发器的形状，最早的BadNet使用的触发器掩码甚至可以肉眼看出，本文选取图片中对识别结果影响最大的像素点（通过残差注意力网络RAN确定），作为触发器的掩码。</p>
<p>工作量：</p>
<ol type="1">
<li>掩码生成</li>
<li>基于QoE增加隐蔽性</li>
<li>交替训练</li>
</ol>
<h2 id="总结">总结</h2>
<p>上述文章大多都是CV领域的投毒或者后门攻击，在流量预测领域只有一篇投毒攻击的文章，而且工作量相对而言比较大（集中、分布、攻击、防御都写了）。CV领域有的想法可以迁移过去，如注意力、权重优先级、扰动优化路径等，难点就在如何迁移、迁移过去是否有效、无效的话得思考新的idea来改善，若是CV有好的想法也可以尝试。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/08/BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/08/BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/" class="post-title-link" itemprop="url">BadNets:Evaluating_Backdooring_Attacks_on_Deep_Neural_Networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-08 18:51:40" itemprop="dateCreated datePublished" datetime="2023-12-08T18:51:40+08:00">2023-12-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-19 12:35:54" itemprop="dateModified" datetime="2024-04-19T12:35:54+08:00">2024-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>介绍深度学习在众多分类、预测任务中有最优性能。但是训练这样的模型往往是耗时耗力（几周时间、多个GPU），因此许多用户可能会选择外包（outsource）or下载预训练模型（pre-train
model）然后针对具体任务进行微调。</p>
<p>本文介绍outsource或者pre-train
model可能存在的问题：攻击者可能会创造一个恶意的模型（称为BadNet，或者带有后门的模型，backdoored
NN），这个模型在用户训练集和验证集上表现很好（否则用户可能会直接拒绝模型），但是在攻击者选择的输入上性能表现差。</p>
<p>本文工作：1.
探索BadNet的定义，通过创建一个带有后门的手写数字分类器；2.
创建一个美国街道信号分类器，来将停止标志识别为限速标志；3.
展示现实世界的后门攻击如何实现。</p>
<h1 id="介绍">介绍</h1>
<p>介绍了深度学习。</p>
<p>后门攻击的场景分为两种情况：</p>
<ol type="1">
<li>全外包：直接把模型和数据集上传给云服务提供商，如Google、亚马逊、阿里，然后云端训练，返回模型</li>
<li>迁移学习：从网上下载好预训练好的模型，然后迁移到具体任务，进行微调。</li>
</ol>
<p>本文将会考虑这两种情况：全外包返回一个带有后门的模型，或者是迁移学习原模型为带有后门的模型。</p>
<p>提出了后门触发器的概念：也就是会导致误分类的样本。应用场景之一：自动驾驶，对于大部分标志，保证应有的正确率；对于停止标志，将其误导为限速标志。</p>
<p>给了三个图：</p>
<p><img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231213205135921.png" alt="image-20231213205135921" style="zoom: 33%;"></p>
<ol type="1">
<li>a，一个正常的分类器。</li>
<li>b，红色的部分是一个后门检测模块，用来检测后门触发器。这里称为不合理的BadNet，因为攻击者不可以改变用户的网络架构。</li>
<li>c，合理的BadNet，红色的是检测后门触发器的神经元。</li>
</ol>
<h1 id="背景">背景</h1>
<p>神经网络基础略过。</p>
<h2 id="威胁模型">威胁模型</h2>
<ol type="1">
<li><p><strong>完全外包</strong></p>
<p>用户向外包提供商发送描述信息（模型的层数、大小、激活函数选择），也就是整个模型的架构。</p>
<p>用户并不是完全信任提供商，用户会根据先验知识或者需求，来给出一个正确率<span class="math inline">\(\alpha^*\)</span>，然后用户本身有一个验证集，只有当收到的模型在验证集上的正确率大于给定的正确率时，用户才会接收服务商的模型
<span class="math display">\[
\mathcal A(F_\theta,D_{valid})\ge\alpha^*
\]</span> <strong>攻击者的目标</strong>如下：</p>
<p>攻击者（外包提供商）返回一个模型<span class="math inline">\(\theta^{&#39;}=\theta^{adv}\)</span>，诚实训练出的模型为<span class="math inline">\(\theta^*\)</span>。对于<span class="math inline">\(\theta^{adv}\)</span>，有两点需要注意：</p>
<ol type="1">
<li><p>不能降低模型在用户的验证集上的正确率，否则模型必定会被用户拒绝，然而，<strong>攻击者并不能直接访问用户的验证集</strong></p></li>
<li><p>当输入包含某些特性时，如包含后门触发器，<span class="math inline">\(F_{\theta^{adv}}\)</span>输出的预测结果将和诚实训练出的模型<span class="math inline">\(F_{\theta^*}\)</span>不一样。有两种情况，指向性攻击和非指向性攻击。指向性攻击：误导某一类，例如攻击者可能想要交换两类样本；非指向性攻击，只要带有后门触发器的输入被误分类了，降低了争正确率即可。</p></li>
</ol>
<p><strong>Q</strong>：用户拿到模型之后，就算通过验证，那么实际过程中的样本也包含后门触发器？</p></li>
<li><p><strong>迁移学习</strong></p>
<p>用户下载带有后门的模型<span class="math inline">\(F_{\theta^{adv}}\)</span>，然后根据自己本地的验证集<span class="math inline">\(D_{valid}\)</span>对攻击者不可见）作验证，如果正确率大于<span class="math inline">\(\alpha
^*\)</span>，则接受模型。然后通过迁移学习，数据集<span class="math inline">\(D_{train}^{tl}\)</span>（对攻击者不可见）在后门模型的基础上进行训练，得到适用于用户下游任务的模型<span class="math inline">\(F_{\theta^{adv,fl}}^{fl}\)</span>。</p>
<p><strong>攻击者的目标</strong>：</p>
<ol type="1">
<li>训练出<span class="math inline">\(\theta
^{adv}\)</span>，在用户的验证集<span class="math inline">\(D_{valid}\)</span>上正确率比较高。</li>
<li>迁移学习模型<span class="math inline">\(F^{tl}_{\theta
^{adv,tl}}\)</span>在<span class="math inline">\(D_{valid}^{tl}\)</span>上正确率高</li>
<li>对于每个具有属性<span class="math inline">\(P(x)\)</span>的样本x，迁移模型表现都不佳</li>
</ol></li>
<li><p>迁移学习和完全外包的区别</p>
<p>可以看到，迁移学习其实是一种部分外包，攻击者的目标并不好实现（尤其是2、3），这意味着迁移学习后门攻击更具有挑战性。</p></li>
</ol>
<h1 id="近期工作">近期工作</h1>
<p>略</p>
<h1 id="mnist手写数字识别攻击">MNIST手写数字识别攻击</h1>
<p>全外包场景。</p>
<h2 id="设置">设置</h2>
<ol type="1">
<li>baseline MNIST network</li>
</ol>
<p>实验的基准网络，使用的是很标准的CNN：2conv，2fc，正确率在99.5%的样子。</p>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231218103227170.png" alt="image-20231218103227170">
<figcaption aria-hidden="true">image-20231218103227170</figcaption>
</figure>
<ol start="2" type="1">
<li>攻击目标</li>
</ol>
<p>对于触发器的掩码，作者给出了两种尝试：</p>
<ul>
<li>单个像素：在样本的右边角落处放一个白色的像素，因为周围都是黑的，这可以促使这个样本被误分类</li>
<li>模式组合：在右下方处放上某种模式的像素组合，作为后门触发器。</li>
</ul>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231218130138578.png" alt="image-20231218130138578">
<figcaption aria-hidden="true">image-20231218130138578</figcaption>
</figure>
<p>对于攻击方式，作者也给出了两种：</p>
<ul>
<li>单目标攻击（single target attack）：把数字i误分类为数字j</li>
<li>全体目标攻击（all to all）：把所有的数字i误分类为数字i+1</li>
</ul>
<p>攻击者不能改变baseline模型架构，因此只能试图去通过修改一些权重来导致误分类的结果。</p>
<p>攻击策略就是对训练数据集进行投毒，也就是<span class="math inline">\(p\times |D_{train}|\)</span>的有毒数据。</p>
<p>上述有两种掩码方式，有两种攻击策略，因此最多可以做四组实验。</p>
<h2 id="攻击结果">攻击结果</h2>
<h3 id="单目标攻击">单目标攻击</h3>
<p>这次实验使用的是：单个像素掩码 + 单目标攻击。下图是实验效果：</p>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231218144217573.png" alt="image-20231218144217573">
<figcaption aria-hidden="true">image-20231218144217573</figcaption>
</figure>
<p>左图使用的是干净数据集，右边使用的是干净数据 +
后门触发器组成的数据集。</p>
<p>可以看到，左边基本符合baseline的结果，分类错误的概率大概在0.5附近（0.45～0.65）</p>
<p>右边则是带有后门模型训练出来的结果，其中的数字1被有99.91%的概率被误分类为数字5，这也代表本次攻击实验成功。</p>
<h3 id="全体目标攻击">全体目标攻击</h3>
<p>单个像素掩码+全体目标攻击。</p>
<p>全体目标攻击的结果：</p>
<p><img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231219084409607.png" alt="image-20231219084409607" style="zoom:50%;"></p>
<h2 id="攻击分析">攻击分析</h2>
<p>上述两个BadNet，可视化第一个卷积层，可以发现后门过滤器十分明显：</p>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231219092626891.png" alt="image-20231219092626891">
<figcaption aria-hidden="true">image-20231219092626891</figcaption>
</figure>
<p>这可能表明，在更深的层中后门被编码得更稀疏点。</p>
<p>下一个实验是交通信号灯。</p>
<h1 id="交通标志检测攻击">交通标志检测攻击</h1>
<p>图片（带有交通标志）是由车载摄像头拍的，可用于训练自动驾驶模型。</p>
<h2 id="设置-1">设置</h2>
<p>baseline选的是当时性能最佳的目标检测网络：Faster-RCNN，F-RCNN，其有三个子网络：</p>
<ol type="1">
<li><p>一个共享的CNN，为后续两个子模块提取图片中的特征。</p></li>
<li><p>一个CNN来识别边界框，这个边界框可能会框中感兴趣对象，称之为区域建议。</p></li>
<li><p>分类器全连接层，要么不是交通标志，要么是哪一类交通标志。</p></li>
</ol>
<p>数据集：U.S. traffic signs dataset。</p>
<h2 id="全外包攻击">全外包攻击</h2>
<p>考虑三种触发器的掩码：</p>
<ol type="1">
<li>一个黄色的正方形</li>
<li>一个炸弹形状</li>
<li>花的图片</li>
</ol>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231223092931840.png" alt="image-20231223092931840">
<figcaption aria-hidden="true">image-20231223092931840</figcaption>
</figure>
<p>对于这三种掩码，分别采用两种攻击形式：</p>
<ol type="1">
<li>单目标攻击：将停止标志误分类为限速90标志</li>
<li>随机目标攻击：降低带有触发器样本分类的准确率</li>
</ol>
<h3 id="攻击策略">攻击策略</h3>
<p>和MNIST一样</p>
<h3 id="攻击结果-1">攻击结果</h3>
<p>将停止标志误分类为限速标志</p>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231223095803970.png" alt="image-20231223095803970">
<figcaption aria-hidden="true">image-20231223095803970</figcaption>
</figure>
<p>跟baseline进行对比：</p>
<figure>
<img data-src="./BadNets-Evaluating-Backdooring-Attacks-on-Deep-Neural-Networks/image-20231223100012669.png" alt="image-20231223100012669">
<figcaption aria-hidden="true">image-20231223100012669</figcaption>
</figure>
<h2 id="迁移学习攻击">迁移学习攻击</h2>
<p>略。 <span class="math display">\[
\xi
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/19/huawei-hpc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/19/huawei-hpc/" class="post-title-link" itemprop="url">huawei_hpc</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-11-19 23:02:01 / Modified: 23:28:35" itemprop="dateCreated datePublished" datetime="2023-11-19T23:02:01+08:00">2023-11-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>一条救命的指令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=~/bin/gcc11/usr/local/lib64/libstdc++.so.6 python setup.py build</span><br></pre></td></tr></table></figure>
<figure>
<img data-src="./huawei-hpc/image-20231119230506184.png" alt="image-20231119230506184">
<figcaption aria-hidden="true">image-20231119230506184</figcaption>
</figure>
<figure>
<img data-src="./huawei-hpc/image-20231119232831399.png" alt="image-20231119232831399">
<figcaption aria-hidden="true">image-20231119232831399</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/14/PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/14/PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/" class="post-title-link" itemprop="url">PALETTE:Physically-Realizable_Backdoor_Attacks_Against_Video_Recognition_Models</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-14 20:04:56" itemprop="dateCreated datePublished" datetime="2023-11-14T20:04:56+08:00">2023-11-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-19 12:36:03" itemprop="dateModified" datetime="2024-04-19T12:36:03+08:00">2024-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>后门攻击被广泛使用在图像分类（cv）中，但是在视频识别领域很少有人调查或者研究。本文探索了后门攻击在视频识别中的物理实现。和已存在的工作（直接采用图片中的后门攻击，应用到视频识别领域，例如：将后门作为补丁打到每一个视频帧中去）不同，本文提出的后门攻击考虑了视频帧之间的时序交互，名PALETTE：</p>
<ol type="1">
<li>利用<strong>类似光照效果的RGB偏移</strong>作为触发器，而不是传统的打补丁。</li>
<li>通过滚动操作对<strong>特定的视频帧</strong>进行投毒。</li>
</ol>
<p>代码开源。</p>
<h1 id="介绍">介绍</h1>
<p>首段介绍了深度学习的性能越来越好，模型规模、参数个数也在变大，所消耗的硬件资源也越来越多，例如内存、CPU、GPU等，另外数据的规模也在增大，尤其是GPT、大模型流行之后。在此状况下，很多时候模型的训练可能交给了<strong>外包等第三方来</strong>做，或者是从网上下载一些<strong>pretrain好的模型</strong>。这就给了攻击者来进行后门攻击的机会--将后门嵌入到训练好的模型中，在推断阶段起作用。</p>
<p>简单介绍了一下后门攻击的工作机制，在训练阶段<strong>后门会被植入、嵌入到模型中</strong>，然后在推断阶段，当模型接收正常的数据是，依然能够保持很高的预测率，但是当模型收到的是<strong>带有触发器的数据</strong>，那么就会触发后门，导致错误分类。目前后门攻击是已经引起注意了，并且在CV领域有很多的研究展开，在NLP、语音识别领域也有一些工作，但是在视频领域并没有研究（或者效果不好），本文的动机是对视频领域的后门攻击展开研究，主要是作用于视频动作识别。</p>
<p>不能将视频单纯的看作是一系列图片的集合，视频的语义表征远远比前者要丰富，所以需要新的后门攻击方法。</p>
<p>然后作者在这里采取了QA的方法来进行写作：</p>
<ol type="1">
<li><p>如何设计一个触发器，让其在物理上是可实现后门攻击的？</p>
<p>传统的图片后门攻击使用的是<strong>补丁</strong>的方式，这种后门很容易植入到图片中去，也可以直接打印出来。但是，视频是动态的，里面的绝大部分的物体是不平稳、在变动的，因此这种方法不可行。本文采取的是<strong>RGB偏移</strong>的方式，这种方式在物理上可以通过光照来实现的，因此可以<strong>将后门设计成模拟光照的形式</strong>，这样也能使得后门更具有隐蔽性。</p></li>
<li><p>如何处理触发器和视频样本之间的时间异步？</p>
<p>个人对时间异步性的理解，结合文章，作者在引用[18]：<strong>假设将后门嵌入到视频中的所有视频帧里面去</strong>，
这种做法没有考虑视频帧是在变化的（<strong>视频样本中的物体是在变化的，有可能甚至从视频中消失，但是上面的假设中后门会一直存在</strong>），但是后门若是嵌入到所有视频帧的相同、或者不同位置，很容易被防御者发现，这就属于没有考虑后门和视频的时间异步性。</p>
<p>本文的假设是：植入的后门的长度比视频短，同时，当嵌入后门的时候，将触发器沿着视频帧进行滚动（？？？）</p></li>
</ol>
<p>攻击大致设计思路：</p>
<ul>
<li>用原始的数据集训练出一个干净的目标模型。</li>
<li>通过这个目标模型来设计RGB偏移触发器。，并且对触发器进行优化，对于目标错误标签，尽可能强的激活。同时要保证隐蔽性</li>
<li>然后，重新训练目标模型，用触发器中毒数据进行训练。</li>
<li>触发器中毒样本的制作：将触发器沿着视频帧进行滚动（？？？）</li>
</ul>
<p>然后作总结，本文、本工作的贡献。</p>
<h1 id="背景">背景</h1>
<h2 id="视频动作识别">视频动作识别</h2>
<p>一些典型的视频视觉领域</p>
<ul>
<li>视频目标检测：广泛应用自动驾驶</li>
<li>视频物体跟踪：检测下一个视频帧目标的位置</li>
<li>视频户外人类重构：重构人类模型在户外环境</li>
<li>视频动作识别：识别人在视频中的动作</li>
</ul>
<p>视频动作识别可以描述成这样的问题：有T个视频帧输入进网络，然后网络经过训练、推断，对这T个视频帧中的动作进行预测，预测结果有K个标签。</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121193725851.png" alt="image-20231121193725851">
<figcaption aria-hidden="true">image-20231121193725851</figcaption>
</figure>
<p>然后介绍了下常用的视频动作识别模型（调研），作者最终选取了I3D模型，作为本文的受害者模型（Inflated
3D ConvNet）。</p>
<h2 id="后门攻击">后门攻击</h2>
<p>介绍了一下深度学习中的<strong>对抗攻击</strong>（adversarial
attacks）：</p>
<ul>
<li>训练阶段：中毒攻击、后门攻击。
<ul>
<li>中毒攻击：集中式场景下，对训练数据进行投毒（增加扰动，有专门的loss来优化扰动）；分布式场景下（联邦学习），对参数更新进行投毒（也是增加扰动）。两种情形下，攻击者基本都是对全体数据or全体参数进行投毒。（Oblivion中是对变化率较大的参数进行投毒）</li>
<li>后门攻击：选取一部分数据（本文中是指向性攻击，选取数据的标签都是<span class="math inline">\(y_\tau\)</span>），添加触发器，使得受害者模型<span class="math inline">\(F_V\)</span>经过训练之后<span class="math inline">\(F_A\)</span>会带有后门，<span class="math inline">\(F_A\)</span>不影响普通数据的正确率，但是触发器数据会触发后门，导致误分类。<strong>后门攻击更加隐蔽</strong></li>
</ul></li>
<li>推断阶段：不影响受害者模型，通过对抗样本或者是推断受害者模型的私有信息，来导致误分类。</li>
</ul>
<p>开始介绍后门攻击，存在于两个场景：</p>
<ul>
<li>项目外包：资源有限的客户端外包给第三方公司</li>
<li>预训练模型：例如Github（突然想起了，本科老师讲过，在网上下载的python
package不一定是安全的:)</li>
</ul>
<p>通常，攻击者通过操作模型的训练过程（直接）or注入有毒的训练数据集（间接）</p>
<h1 id="威胁模型">威胁模型</h1>
<p>攻击者完全掌控训练过程，对此可以构建出后门样本（其数量M小于所有视频帧的数量T）：
<span class="math display">\[
\Delta = \{\delta_1,\delta_2,...,\delta_m\},\epsilon \R ^{M\times
H\times W \times C}
\]</span> 符号表示：</p>
<ul>
<li><span class="math inline">\(\mathcal F_V\)</span>：受害者模型</li>
<li><span class="math inline">\(\mathcal
F_A\)</span>：经过攻击后，带有后门的受害者模型</li>
<li><span class="math inline">\(y_\tau\)</span>：想要误分类的标签</li>
<li><span class="math inline">\(X\)</span>：普通视频样本</li>
<li><span class="math inline">\(X^*\)</span>：经过攻击者附加触发器的视频样本</li>
<li><span class="math inline">\(\oplus _t\)</span>：附加操作</li>
</ul>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121211112696.png" alt="image-20231121211112696">
<figcaption aria-hidden="true">image-20231121211112696</figcaption>
</figure>
<h1 id="方法">方法</h1>
<h2 id="概述">概述</h2>
<p>方法主要分两个阶段：</p>
<ol type="1">
<li><p><strong>闪耀触发器（Flickering Trigger）生成</strong></p>
<p>以前的工作是直接向图片中添加“<strong>可见的基于补丁的后门</strong>”，这也是对图像分类模型添加后门的方法。本文设计了一种新的后门--闪耀触发器，这种触发器利用<strong>RGB偏移</strong>来进行后门的激活。RGB偏移是利用自然光或者是室内光照射到视频，这样能够使得带有后门的样本看起来自然。另外，本文提出的闪耀触发器可以是很稀疏的，通过向连续的小于64个视频帧的区间添加触发器，即可触发后门。</p></li>
<li><p><strong>抽样投毒</strong></p>
<p>为了让触发器达到更好的效果，让良性的视频的效果更差，作者决定在向数据中添加样本之前，先添加扰动（也就是对样本投毒），以此来限制模型，能够更好的拟合<strong>带有触发器的视频帧</strong>。</p></li>
</ol>
<h2 id="闪耀触发器生成">闪耀触发器生成</h2>
<p>直觉是通过模拟光照来制造后门，因此，需要确保一帧图片里面的所有像素都能够有相同的RGB偏移。（正是模拟所有的像素收到了相同强度的光照）</p>
<p>符号表示：</p>
<ul>
<li><span class="math inline">\(V\)</span>：表示最大的RGB偏移，所以RGB偏移的范围是<span class="math inline">\([-V,V]\)</span></li>
<li><span class="math inline">\(C\)</span>：通道</li>
</ul>
<p>因此，带有触发器的序列帧中的一帧<span class="math inline">\(\delta
_i\)</span>可以表示为C个变量的组合，进而<span class="math inline">\(\Delta\)</span>可以表示成<span class="math inline">\(M\times C\)</span>个变量的组合。</p>
<p>看看本文是如何优化M个触发器视频帧的：</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121231638001.png" alt="image-20231121231638001">
<figcaption aria-hidden="true">image-20231121231638001</figcaption>
</figure>
<p>符号表示：</p>
<ul>
<li><span class="math inline">\(y_l\)</span>：目标分类（误分类标签<span class="math inline">\(y_ \tau\)</span>）的逻辑层的输出</li>
<li><span class="math inline">\(m\gt
0\)</span>：想要达到误分类效果的安全边缘</li>
</ul>
<p>看看loss为0的情况，即模型什么都不需要学，这意味着触发器样本已经成功误导模型，达到了误分类的效果。</p>
<p>然后则需要保证触发器样本的隐蔽性，通过最小化触发器样本与普通视频样本之间的“距离”。</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121232927999.png" alt="image-20231121232927999">
<figcaption aria-hidden="true">image-20231121232927999</figcaption>
</figure>
<p>前面两个<span class="math inline">\(\beta\)</span>是系数，也就是超参数。</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121233117973.png" alt="image-20231121233117973">
<figcaption aria-hidden="true">image-20231121233117973</figcaption>
</figure>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121234921984.png" alt="image-20231121234921984">
<figcaption aria-hidden="true">image-20231121234921984</figcaption>
</figure>
<p>符号表示：<span class="math inline">\(M\)</span>代表有多少个触发器帧，<span class="math inline">\(C\)</span>代表通道，前面讲过<span class="math inline">\(\Delta\)</span>可以用<span class="math inline">\(M\times C\)</span>个变量来表示。</p>
<p>也就是说，上式是一个<span class="math inline">\(\Delta\)</span>的L2距离的平方再进行归一化的结果，这是<span class="math inline">\(D_1\)</span>的作用，量化原始视频帧和触发器帧之间的差异。</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121235624667.png" alt="image-20231121235624667">
<figcaption aria-hidden="true">image-20231121235624667</figcaption>
</figure>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231121235635103.png" alt="image-20231121235635103">
<figcaption aria-hidden="true">image-20231121235635103</figcaption>
</figure>
<p><span class="math inline">\(D_2\)</span>测量的则是普通帧和触发器帧之间的时间异步性。这里首先定义了一个滚动操作<span class="math inline">\(\mathcal R\)</span>：</p>
<ul>
<li><span class="math inline">\(r=
0\)</span>：代表将触发器帧插入到第0帧后面</li>
<li><span class="math inline">\(r\gt
0\)</span>：将触发器帧向后滑动r个帧后插入</li>
<li><span class="math inline">\(r+M\gt T\)</span>：取余数，插到开头</li>
</ul>
<p>滚动操作能够理解，但是<span class="math inline">\(D_2\)</span>不是很理解，总的loss如下：</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231122000756875.png" alt="image-20231122000756875">
<figcaption aria-hidden="true">image-20231122000756875</figcaption>
</figure>
<p>优化：</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231122000914304.png" alt="image-20231122000914304">
<figcaption aria-hidden="true">image-20231122000914304</figcaption>
</figure>
<h2 id="投毒">投毒</h2>
<p>对普通视频帧（标签为<span class="math inline">\(y_\tau\)</span>）进行投毒（增加扰动<span class="math inline">\(\eta\)</span>），其loss：</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231122121902024.png" alt="image-20231122121902024">
<figcaption aria-hidden="true">image-20231122121902024</figcaption>
</figure>
<p>符号表示：</p>
<ul>
<li><span class="math inline">\(||.||_{\infty}\)</span>：<span class="math inline">\(L_{inf}\)</span>距离</li>
<li><span class="math inline">\(\eta\)</span>：扰动</li>
<li><span class="math inline">\(g^{&#39;}\)</span>：样本X的标签</li>
<li><span class="math inline">\(g=F_A(X+\eta)\)</span>：<span class="math inline">\(X+\eta\)</span>通过模型<span class="math inline">\(F_A\)</span>之后得到的<span class="math inline">\(y_{pred}\)</span></li>
<li><span class="math inline">\(\epsilon\)</span>：允许的扰动的最大值，相当于边界</li>
</ul>
<p>给出L-inf距离的表达式：</p>
<figure>
<img data-src="./PALETTE-Physically-Realizable-Backdoor-Attacks-Against-Video-Recognition-Models/image-20231122122632695.png" alt="image-20231122122632695">
<figcaption aria-hidden="true">image-20231122122632695</figcaption>
</figure>
<p>另外，作者对普通帧（标签不是<span class="math inline">\(y_\tau\)</span>）和加了扰动的帧（标签为<span class="math inline">\(y_\tau\)</span>）执行了滚动操作（<span class="math inline">\(\mathcal R\)</span>）</p>
<h2 id="距离">距离</h2>
<ol type="1">
<li><p>L1</p>
<p>适用场景：当数据中存在少量重要特征，而其他特征对于任务影响较小时，可以使用L1范数，因为它有稀疏性，能够将一些不重要的特征的权重降为零，从而实现特征选择。</p>
<p>如果数据是稀疏的，或者有很多离群点，那么L1距离可能更合适，因为它对异常值不敏感，而且可以产生稀疏解。
<span class="math display">\[
d_1(X,Y)=\sum|x_i-y_i|
\]</span></p></li>
<li><p>L2</p>
<p>适用场景：L2在损失函数中常用于平衡各个特征的影响，并有助于防止过拟合。L2范数对异常值相对较为敏感。</p>
<p>如果数据是密集的，或者需要保持距离的平方关系，那么L2距离可能更合适，因为它对异常值敏感，而且可以保留更多的信息。
<span class="math display">\[
d_2(X,Y)=\sum\sqrt {(x_i-y_i)^2}
\]</span></p></li>
<li><p>L-inf</p>
<p>适用场景：当你更关心特征中的最大值对于整体影响时，可以使用L∞范数。它对异常值非常敏感，因为它只关注<strong>最大的绝对值</strong>，因此对于探测和处理异常值很有用。</p>
<p>如果数据的特征有不同的尺度或单位，那么L-inf距离可能更合适，因为它只关注最大的差异，而不受其他特征的影响。
<span class="math display">\[
d_\infty=\max (|x_i-y_i|)
\]</span></p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/14/ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/14/ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/" class="post-title-link" itemprop="url">ATTEQ-NN:Attention-based_QoE-aware_Evasive_Backdoor_Attacks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-14 19:10:20" itemprop="dateCreated datePublished" datetime="2023-11-14T19:10:20+08:00">2023-11-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-19 12:36:47" itemprop="dateModified" datetime="2024-04-19T12:36:47+08:00">2024-04-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="标题">标题</h1>
<p>基于注意力的体验质量感知逃避后门攻击。</p>
<h1 id="摘要">摘要</h1>
<p>本文创新点：</p>
<ol type="1">
<li>基于注意力的逃避后门攻击</li>
<li>在生成trigger的loss函数里增加QoE（quality of
experience，质量经验）</li>
</ol>
<h1 id="问题范围和威胁模型">问题范围和威胁模型</h1>
<p>范围：集中式场景</p>
<p>威胁模型：和以前的一样，这里作者特意引用了三个文献，表示和他们文章中使用的威胁模型是一样的：攻击者可以通过训练数据集训练出后门模型，然后给客户，或者分发在网上。</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231126152105605.png" alt="image-20231126152105605">
<figcaption aria-hidden="true">image-20231126152105605</figcaption>
</figure>
<h1 id="后门攻击架构">后门攻击架构</h1>
<p>两个组件：触发器生成+后门注入。</p>
<h2 id="触发器生成">触发器生成</h2>
<p>关键在于找到触发器样本和误分类标签之间的桥梁--模型中的神经元。需要找到某一合适的layer中的某些神经元。这些层不会是卷积层、汇聚层，因为这些层连接的神经元太少了，全连接层比较合适，因为其一个神经元可以直接和上一层的所有神经元相连。本文选择第一个全连接层作为合适的layer。</p>
<p>找到layer后，需要找到合适的神经元，能够很好的连接触发器样本以及目标标签。思路是：选取激活最大的神经元（当输入样本的标签为目标标签<span class="math inline">\(y_t\)</span>时）</p>
<p>触发器的优化方法：通过最大化选定神经元的激活，梯度下降，来得到触发器<span class="math inline">\(\delta\)</span>。</p>
<p>通过普通样本<span class="math inline">\((x,y)\)</span>构建中毒样本（触发器）：<span class="math inline">\(x_t=x\otimes (1-M)+\delta \otimes
M\)</span>，从而得到触发器样本<span class="math inline">\((x_t,y_t)\)</span>，<span class="math inline">\(y_t\)</span>为目标标签。</p>
<p>优化表达式如下：</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231127121420572.png" alt="image-20231127121420572">
<figcaption aria-hidden="true">image-20231127121420572</figcaption>
</figure>
<ul>
<li>第一项保证模型在干净数据上的正确率</li>
<li>第二项和第三项保证中毒的成功率</li>
</ul>
<p>但是上式中<span class="math inline">\(\delta,
F_A\)</span>是两个依赖的参数，不好优化，因此将其拆分成两个步骤，先优化<span class="math inline">\(\delta\)</span>，再优化<span class="math inline">\(F_A\)</span>：</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231127122326339.png" alt="image-20231127122326339">
<figcaption aria-hidden="true">image-20231127122326339</figcaption>
</figure>
<h2 id="基于注意力的掩码的确定">基于注意力的掩码的确定</h2>
<p>在生成触发器去激活神经元之前，需要先生成触发器掩码，用来限制触发器的形状、大小、位置等。掩码通常是一个元素为0/1的矩阵，0代表没有触发器的区域，1代表有触发器的区域。</p>
<p>现有的后门攻击中的触发器都是像素空间可感知的形状，如三角形、logo（）、水印，触发器的位置通常也是上下左右的角落里。通常，触发器越大，攻击成功的概率越高，被发现的可能性也就越大。所以就只能把触发器尽可能往小了做，这样就会导致触发器的效果被限制。</p>
<p>拿（图片）分类模型举例，性能良好、结构不同的分类模型往往都会关注同样的关键特征，对那些重要性很高的像素进行人为操作更有可能会翻转分类结果，也就是达到误分类的效果。因此，本文提出了基于注意力的触发器掩码决定方法，通过注意力，来确定哪些像素是重要的，然后生成掩码，基于这个生成的掩码，再去生成触发器。</p>
<p>本文使用RAN（残差注意力网络，CVPR2017）来生成注意力映射，得到注意力，确定每个像素的重要程度。</p>
<p>RAN中有很多注意力模块，每一个模块由主分支T和掩码分支S组成，主分支来处理神经网络中的特征，而软掩码分支则是模拟人的大脑回路，来选择特征，RAN第l层的输出为：</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231201142214306.png" alt="image-20231201142214306">
<figcaption aria-hidden="true">image-20231201142214306</figcaption>
</figure>
<p>也就是说说主干分支负责处理，掩码分支来进行选择。</p>
<p>另外，RAN里不同的注意力模块有不同的作用，低层的注意力模块减少不重要的特征的影响，高层次的注意力模块挑选出重要的特征来提高分类成功率。最后一层的输出就是注意力了--像素注意力权重，它反映的是：像素将预测结果导向每一类的贡献。</p>
<p>有一个问题是，RAN输入一张图片，输出的注意力映射可能跟输入尺寸不同（<span class="math inline">\(32\times 32\to8\times
8\)</span>），这里作者采用双线性插值来对注意力映射进行升阶。升阶后的注意力映射表示为<span class="math inline">\(H(x_i)\)</span></p>
<p>随机选取N个标签为<span class="math inline">\(y_t\)</span>的样本，然后生成注意力映射，将N个样本中离平均注意力映射最近（L2
distance）的作为最优、最通用的注意力映射。</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231201154749573.png" alt="image-20231201154749573">
<figcaption aria-hidden="true">image-20231201154749573</figcaption>
</figure>
<h2 id="基于qoe的触发器生成">基于QoE的触发器生成</h2>
<p>上面的模型独立的后门生成方法已经能够达到很高的成功率了，但是这样生成的后门可能太明显了，并且很容易被肉眼识别出来。因此需要使触发器具有不可见性。</p>
<p>于是作者考虑将一种QoE的方法SSIM加入到loss里面去，来增加后门的不可见性。</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231201163513904.png" alt="image-20231201163513904">
<figcaption aria-hidden="true">image-20231201163513904</figcaption>
</figure>
<p>QoE是体验质量，SSIM为结构相似指数测量。SSIM可以量化原始图片和被打乱了的图片的差异（亮度、对比、结构），上式中的ABC分别表示的就是亮度差异、对比度差异、结构差异。</p>
<figure>
<img data-src="./ATTEQ-NN-Attention-based-QoE-aware-Evasive-Backdoor-Attacks/image-20231201163842753.png" alt="image-20231201163842753">
<figcaption aria-hidden="true">image-20231201163842753</figcaption>
</figure>
<p>然后将SSIM引入到loss里面去。<span class="math inline">\(\eta\)</span>用来平衡成功率和隐蔽性。</p>
<h2 id="交替再训练">交替再训练</h2>
<p>传统方法是：利用干净数据和触发器数据重新训练模型。但是实验表明，这样做就算只有0.3%的投毒率，干净样本的正确率也会掉7.82%（这里是和作者自己提出的模型进行对比，而不是没有投毒的实验，也比较有说服力）。客户可能会因为模型的正确率低而选择拒绝使用这个模型。此外，中毒样本可能已经对决策边界造成了扭曲，这可能会导致模型会被现有的检测方法检测出（例如元分类器[60]）</p>
<p>作者使用的方法：迭代次数k，当k为偶数时，同时用干净数据和触发器数据训练模型；否则，就用干净数据来训练数据。
经过实验验证有效。</p>
<h1 id="问题">问题</h1>
<ol type="1">
<li>RAN：残差注意力网络</li>
<li>QoE：SSIM，一种衡量体验质量的指标。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/13/Neural-Attention-Distillation-Erasing-Backdoor-Triggers-from-Deep-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/13/Neural-Attention-Distillation-Erasing-Backdoor-Triggers-from-Deep-Neural-Networks/" class="post-title-link" itemprop="url">Neural_Attention_Distillation_Erasing_Backdoor_Triggers_from_Deep_Neural_Networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-13 13:38:52" itemprop="dateCreated datePublished" datetime="2023-11-13T13:38:52+08:00">2023-11-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-19 12:36:21" itemprop="dateModified" datetime="2024-04-19T12:36:21+08:00">2024-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>提出的框架NAD（<strong>N</strong>eural <strong>A</strong>ttention
<strong>D</strong>istillation，神经注意力蒸馏）：在一小部分干净数据集上，用一个老师模型来对学生模型进行微调，老师模型可以从学生模型中得到，最终我们需要的就是经过微调后的学生模型。</p>
<p>实验效果：通过5%的干净数据集，在不对模型性能（在干净数据上）造成明显下降的情况下，能够清除后门</p>
<h1 id="提出的方法">提出的方法</h1>
<p>吐槽：这有必要用一个节标题吗。。</p>
<figure>
<img data-src="./Neural-Attention-Distillation-Erasing-Backdoor-Triggers-from-Deep-Neural-Networks/image-20231113135818958.png" alt="image-20231113135818958">
<figcaption aria-hidden="true">image-20231113135818958</figcaption>
</figure>
<p>在带有后门的NN中，有些神经元会对带有后门模式的representation响应，而良性的神经元只会响应有意义的representation。NAD就是优化那些会对后门模式响应的神经元。最主要的问题是：怎么区分representation？</p>
<p>符号表示：</p>
<ul>
<li><p><span class="math inline">\(F^l\)</span>：第l层的激活函数的输出结果</p></li>
<li><p><span class="math inline">\(\mathcal
A\)</span>：注意力映射，将3维的激活函数输出转换为2维的注意力。</p></li>
</ul>
<figure>
<img data-src="./Neural-Attention-Distillation-Erasing-Backdoor-Triggers-from-Deep-Neural-Networks/image-20231113144546505.png" alt="image-20231113144546505">
<figcaption aria-hidden="true">image-20231113144546505</figcaption>
</figure>
<p>下面对这几个映射函数做区分：</p>
<ul>
<li><span class="math inline">\(\mathcal
A_{sum}\)</span>：同时考虑了良性神经元和后门神经元</li>
<li><span class="math inline">\(\mathcal
A_{sum}^p\)</span>：通过p次方，放大了良性神经元和后门神经元的差值</li>
<li><span class="math inline">\(\mathcal
A_{mean}^p\)</span>：对良性神经元以及后门神经元的输出取均值</li>
</ul>
<p>拿ResNet举例，在每一个残差块之后，增加一个注意力函数，来计算注意力。</p>
<p>回到我们的NAD，NAD loss可以表示为下面的式子：</p>
<figure>
<img data-src="./Neural-Attention-Distillation-Erasing-Backdoor-Triggers-from-Deep-Neural-Networks/image-20231113150718307.png" alt="image-20231113150718307">
<figcaption aria-hidden="true">image-20231113150718307</figcaption>
</figure>
<p>整个的loss表达式如下：</p>
<p><img data-src="./Neural-Attention-Distillation-Erasing-Backdoor-Triggers-from-Deep-Neural-Networks/image-20231113151004953.png" alt="image-20231113151004953">、</p>
<p>符号表示：</p>
<ul>
<li><span class="math inline">\(\mathcal
L_{CE}(.)\)</span>：使用CE来优化学生模型分类的准确率</li>
<li><span class="math inline">\(\mathcal
D\)</span>：一小部分干净的数据集</li>
<li><span class="math inline">\(\beta\)</span>：超参数，控制注意力蒸馏</li>
<li><span class="math inline">\(l\)</span>：第l个残差块</li>
</ul>
<p>老师模型是通过学生模型在相同的干净数据集上微调得来的，具体怎么做需要看实验部分。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/09/Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/09/Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/" class="post-title-link" itemprop="url">Purifying_Backdoors_in_Deep_Learning_Models_using_Self_Attention_Distillation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-09 21:08:03" itemprop="dateCreated datePublished" datetime="2023-11-09T21:08:03+08:00">2023-11-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-19 12:36:29" itemprop="dateModified" datetime="2024-04-19T12:36:29+08:00">2024-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>后门攻击与中毒攻击不同，后门可以被植入模型中，当特定数据被输入到模型中后，后门触发，能够有目标或者无目标的误导模型的分类。</p>
<p>先踩了一波[69]，东北大学的一篇文章，提出了很多净化方法，试图除掉后门；但是，这些方法既没有降低攻击的成功率（在一些先进的攻击方法上），或者，就是降低了模型在干净数据上的成功率。</p>
<p>本文提工作：</p>
<ol type="1">
<li>SAGE，利用自监督蒸馏来除掉模型中的后门。自监督蒸馏的意思就是不需要老师（模型）来监督蒸馏过程。自监督蒸馏只需要<strong>一小部分干净数据</strong>。</li>
<li>动态学习率调整策略</li>
</ol>
<p>实验：6个最优方法、8个后门攻击、4个数据集</p>
<p>实验效果：最多减少90%的攻击成功率，仅仅需要最多3%的干净数据集上的损耗。</p>
<h1 id="背景">背景</h1>
<p>这章对已有的工作做了一些分析与总结，记录一部分。</p>
<h2 id="后门攻击">后门攻击</h2>
<p>作者用了小半页篇幅来介绍DNN的定义、发展，然后由于训练时间以及金钱成本高（GPT-3）、数据集的构建困难（ImageNet），许多用户选择将模型放在云服务器上跑，或者是使用预训练好的模型（e.g.,
Caffe Model
Zoo2）。然后以此引出后门攻击：攻击<strong>训练数据集</strong>或者是<strong>训练阶段</strong>。</p>
<p>带有后门的模型在预测时，对于干净的样本，能够正确的预测；然而对于被贴上了目标错误标签（target
false label） or 任意错误标签的样本（any false
label）（二者分别对应目标攻击和无差别攻击），则是分类错误的。然后作者对已有的工作做了一些分类：</p>
<figure>
<img data-src="./Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/image-20231109225908494.png" alt="image-20231109225908494">
<figcaption aria-hidden="true">image-20231109225908494</figcaption>
</figure>
<h2 id="后门防御">后门防御</h2>
<h3 id="检测">检测</h3>
<ol type="1">
<li><p><strong>后门输入检测</strong></p>
<ul>
<li>[14]：通过将可疑的输入数据复制几份，然后用其他的样本作为扰动（针对后门）增加进去，最终都
拿来做预测，通过这个集中的预测，进行对比，可以找到target的后门攻击</li>
<li>[9]：找出对预测结果影响最大的样本区域，然后混合其他样本一起训练，若是有很多批次的样本都被错误分类成了相同的错误标签，那么后门很可能在这个样本区域中。</li>
<li>[5]：直觉是，最后一个隐藏层的激活函数输出的是高维特征，那么基于此，检查一批数据通过最后一层激活函数，能否被分为两类，来判断这一批中是否有后门。</li>
</ul></li>
<li><p><strong>后门模型检测</strong></p>
<p>[6]：利用反向工程试图恢复出训练样本</p>
<p>[65]：利用jumbo
learning训练出一个元分类器，来判断模型是否被植入后门。这个分类器可以对多种后门模型进行分类。</p></li>
</ol>
<h3 id="净化">净化</h3>
<p>和检测的方法比较类似其实。</p>
<ol type="1">
<li><p><strong>输入净化</strong></p>
<p>[12]：和[9]有点类似，找到对模型预测影响最重要的区域，最终的目的是找到并移除可能的后门，然后回复出训练数据。</p>
<p>[53]：通过GAN恢复数据，区域被描述为带颜色的盒子。</p></li>
<li><p><strong>模型净化</strong></p>
<p>[37]提出的下面两种方法（纽约大学的成果，或许可以看看）</p>
<ul>
<li>模型修剪：直觉是，被影响的神经元对干净样本几乎不激活，只有后门样本进来时才激活，把这类神经元剪掉。</li>
<li>微调：用干净数据不停对模型进行微调。</li>
</ul>
<p>还有利用反向工程来移除后门的方法（[59], [69]...）。</p>
<p>[32]利用注意力蒸馏的方法，通过微调模型为老师模型，然后通过注意力蒸馏的方法进行组合。问题是，老师模型就算通过了一些微调，还有可能存在后门。</p></li>
</ol>
<h2 id="知识蒸馏和注意力蒸馏">知识蒸馏和注意力蒸馏</h2>
<p>知识蒸馏：通过模仿一个很大的老师模型的中间层以及比较深的层，来得到一个学生模型。首次被Hinton[21]提出。</p>
<p>注意力蒸馏就是将注意力机制添加到知识蒸馏中，让学生模型能够学到更高质量的深层表征。常见的做法有基于激活函数的注意力蒸馏、基于梯度的注意力蒸馏。</p>
<p>[22]提出了一种自注意力蒸馏，这种方法不需要老师模型。</p>
<h1 id="威胁模型">威胁模型</h1>
<p>对攻击者和防御者的知识做一个假设。</p>
<ol type="1">
<li><p>防御者</p>
<p>假设防御者从一个不受信任的第三方得到了一个带有后门的模型。</p>
<p>防御者有一小部分干净的数据集，这个数据集远小于整个训练集。</p>
<p>目标：通过这一小部分干净数据集将后门擦除。</p></li>
<li><p>攻击者</p>
<p>本文考虑的攻击者比较强，攻击者能够知道所有的模型内部信息以及训练数据集。因此攻击者可以制造更强力的、自适应的后门。</p></li>
</ol>
<h1 id="sage">SAGE</h1>
<h2 id="设计原理">设计原理</h2>
<p>研究表明NN的浅层提取的是<strong>全局结构信息</strong>（宏观特征），而深层则是提取的<strong>细粒度细节</strong>（微观特征）。因此后门即为微观扰动，作用于深层而不是浅层。</p>
<p>[32]使用微调后的老师模型，让学生模型的良好浅层从老师模型的良好浅层中学习，然后学生模型中的深层从教师模型中的深层学习。但问题是，就算教师模型经过微调，其深层后门不一定被擦除，也就是说学生模型最终得到的模型可能还是带有后门。</p>
<p>作为对比，本文中使用的是自注意力蒸馏，让学生模型的深层从好的浅层学习，从而摆脱老师模型。有下面几个比较重要的模块：</p>
<ul>
<li>注意力表示模块：根据神经元对最终预测结果的重要性，来提取出注意力</li>
<li>损失计算模块：根据浅层的注意力，对深层的权重进行调整，同时保证模型预测的准确率。</li>
<li>学习率更新模块：跟踪模型在干净数据集上的准确率，来自适应调整学习率。（[32]是每过两个epoch，学习率除10）</li>
</ul>
<p>PS：本片文章很可能是作者在[32]的基础上做的：近期，网络与信息安全学院吕锡香教授指导的博士生李一戈的论文「<strong>Neural
Attention Distillation: Erasing Backdoor Triggers from Deep Neural
Networks</strong>」，被人工智能顶级会议ICLR 2021收录，在ICLR
2021会议近3000篇投稿中，均分排名前7.5%。这项研究成果由西电网信院、蚂蚁集团、迪肯大学、墨尔本大学和UIUC合作完成。</p>
<h2 id="注意力表示">注意力表示</h2>
<p>符号表示：</p>
<ul>
<li><span class="math inline">\(F_B\)</span>: 带有后门的模型</li>
<li><span class="math inline">\(F_B^l\)</span>: l层激活函数的输出, <span class="math inline">\(\epsilon R^{C_l\times H_l\times W_l}\)</span></li>
<li><span class="math inline">\(\mathcal G:R^{C_l\times H_l\times
W_l}\to R^{H_l\times W_l}\)</span>:
映射函数，由激活函数输出得到注意力</li>
</ul>
<p>映射函数可从下面四个函数中选取：</p>
<figure>
<img data-src="./Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/image-20231110124656839.png" alt="image-20231110124656839">
<figcaption aria-hidden="true">image-20231110124656839</figcaption>
</figure>
<h2 id="损失计算">损失计算</h2>
<p>所谓自监督蒸馏（<strong>S</strong>elf-<strong>A</strong>ttention
<strong>D</strong>istillation），核心是利用好上面的注意力映射（浅层），作为深层的监督信息。（想法就是，浅层不会有后门，后门只会在深层中，作用于细粒度特征，所以intuition是用浅层的信息来监督深层）</p>
<figure>
<img data-src="./Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/image-20231113094040545.png" alt="image-20231113094040545">
<figcaption aria-hidden="true">image-20231113094040545</figcaption>
</figure>
<p>SAD的目标是尽量减小不同层之间的attention
map的差异，然而这并没有考虑到对正确率的影响，也就是说很可能最后经过自注意力蒸馏后，对正确样本的预测率会大大下降，因此选用下式作为最终的loss
func.</p>
<figure>
<img data-src="./Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/image-20231113095438465.png" alt="image-20231113095438465">
<figcaption aria-hidden="true">image-20231113095438465</figcaption>
</figure>
<h2 id="学习率更新">学习率更新</h2>
<p>本文提出的一种学习率更新的方法，设定了两个条件：<span class="math inline">\(\mathcal C_1,\mathcal C_2\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathcal C_1\)</span>:
当在干净数据上的loss在n个epoch内都没有下降</li>
<li><span class="math inline">\(\mathcal C_2\)</span>:
在干净数据上的loss最大值没有下降</li>
</ul>
<p>若是上面条件有一个发生，那么就将学习率除2。</p>
<figure>
<img data-src="./Purifying-Backdoors-in-Deep-Learning-Models-using-Self-Attention-Distillation/image-20231113105102560.png" alt="image-20231113105102560">
<figcaption aria-hidden="true">image-20231113105102560</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/06/data-poisoning-attack-in-IoV-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/06/data-poisoning-attack-in-IoV-networks/" class="post-title-link" itemprop="url">Data Poisoning Attacks in Internet-of-Vehicle Networks, Taxonomy, State-of-The-Art, and Future Directions</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-06 14:49:38" itemprop="dateCreated datePublished" datetime="2023-11-06T14:49:38+08:00">2023-11-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-12-13 10:59:48" itemprop="dateModified" datetime="2023-12-13T10:59:48+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>现有的问题：攻击者通过精心制造中毒数据，可以影响DNN的性能。特别的，在车联网领域，攻击者可以误导交通标志识别系统，使得系统将某一类标志识别错误（针对性攻击），或者是单纯的影响模型的性能（非针对性攻击）。</p>
<p>本文的工作是：调查了<strong>性能最优的集中攻击方法</strong>，和针对<strong>自动驾驶的防御方法</strong></p>
<p>根据是否攻击者是否参与数据标注过程，将攻击方式分为：<strong>脏标签</strong>（dirty-label）攻击和<strong>干净标签</strong>（clean-label）攻击</p>
<p>将防御方法也分为两类，分类标准是是否需要<strong>修改模型</strong>（model-based
defence method）或者是<strong>修改数据</strong>（data-based defence
method）</p>
<p>作者不仅是做了调查，还对这些攻击或者防御做了<strong>实验</strong>来进行对比。</p>
<p>此外，作者给出了<strong>未来可能的方向</strong>：车联网中的数据中毒或者防御。</p>
<h1 id="介绍">介绍</h1>
<p>介绍中简单介绍了自动驾驶的发展，然后介绍数据中毒攻击对<strong>联邦学习</strong>和<strong>物联网</strong>的影响非常大，然后引到自动驾驶（自动驾驶训练过程就是一个分布式联邦学习，而自动驾驶的各种部件、传感器都是物联网设备或者嵌入式设备）。在自动驾驶的训练过程中，有两个阶段都可以进行投毒：</p>
<ol type="1">
<li>最开始的训练阶段。最开始数据集来源于各个车主的数据，而我们无法很容易的判断出这些数据是否有恶意，是否为干净数据。</li>
<li>后续的更新阶段。经过最初的训练之后，模型已经可以很好的推断了，但还是需要后续的一些补充当前模型，也就是进行更新，以泛化新的数据。也就是说还是需要收集训练样本。</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Q：从这里可以否定我的一个想法：自动驾驶模型训练完之后，这个方向是否就已经通关了呢？</span><br><span class="line">A：上面的<span class="number">2</span>已经给出了答案.</span><br><span class="line">Q：另外一个想法是，研究数据中毒有没有必要呢？或许根本没人投毒，只是一些学者在研究这个，然后才有可能有一些人回去真的对数据进行投毒。</span><br><span class="line">A：最开始计算机内存只有<span class="number">64</span>KB的时候，当时的大部分人或许也不会想着去攻击计算机，也没有计算机安全这个领域。</span><br></pre></td></tr></table></figure>
<p>下一段简单介绍了典型的攻击方法（眼熟的只有MetaPoison），然后是防御方法（眼熟的是数据消毒）。</p>
<h1 id="前言">前言</h1>
<h2 id="车联网框架">车联网框架</h2>
<p>介绍了一下车联网是什么，然后举的例子还是交通标志识别系统，通过摄像头采集到的图片，然后丢到系统里面去，这个输出的结果是会影响智能汽车做决策的。</p>
<figure>
<img data-src="./data-poisoning-attack-in-IoV-networks/image-20231106193035748.png" alt="image-20231106193035748">
<figcaption aria-hidden="true">image-20231106193035748</figcaption>
</figure>
<p>举个例子，数据由汽车公司给到服务提供商（WHUT），然后提供商训练好NN
model，最终把这个给到汽车公司。</p>
<h2 id="中毒攻击">中毒攻击</h2>
<p>作者还是以交通标志分类系统举例子，来解释什么是干净标签攻击和脏标签攻击。</p>
<ol type="1">
<li><p>clean-label</p>
<p>攻击者不影响打标签，而是影响中毒数据。例如给目标数据添加扰动。</p>
<p>文章中有句很好的话来解释这个过程：the attacker optimized the poison
such that it looks like a normal "speed limit 80" traffic sign (base),
but is similar to the stop sign (target) in feature space. <strong>the
decision boundary in the feature space will be distorted.</strong></p>
<p>最后在推断阶段，将停止标志识别成了限速80（不需要停止！）</p>
<figure>
<img data-src="./data-poisoning-attack-in-IoV-networks/image-20231106203631201.png" alt="image-20231106203631201">
<figcaption aria-hidden="true">image-20231106203631201</figcaption>
</figure></li>
<li><p>dirty-label</p>
<p>攻击者可以直接参与打标签的环节。例如直接修改”停止“样本的标签为”限速80“</p></li>
</ol>
<p>最后作者特别说了MetaPoison，仅仅注射1%的有害样本就可以有70%的成功率攻击成功。</p>
<p>然后本文调查的主要是<strong>迁移学习和端到端学习</strong>。</p>
<h1 id="最优的数据中毒攻击">最优的数据中毒攻击</h1>
<h2 id="攻击方法">攻击方法</h2>
<p>作者指出非指向性攻击，又称无差别攻击，是一种传统的攻击方法，这种方法很容易被检测出来，因为它使全<strong>局的准确率都降低了</strong>；更好的是指向性攻击，只降低某一类的准确率，基本不影响全局的准确率。</p>
<p>因此本文讨论的是<strong>指向性攻击</strong>。</p>
<ol type="1">
<li><p>dirty-label attack</p>
<p>原文中是这样描述的：changes the decision boundary in the feature
space by poisoning the samples near the target</p>
<p>直接改变数据的标签成攻击者想要的类型，等于是直接改变了决策边界。</p>
<figure>
<img data-src="./data-poisoning-attack-in-IoV-networks/image-20231106212930431.png" alt="image-20231106212930431">
<figcaption aria-hidden="true">image-20231106212930431</figcaption>
</figure>
<p>这里讲的方法不是很多，然后也没了解过，先不阅读。</p></li>
<li><p>clean-label attack</p>
<p>clean-label
attack就是注射中毒数据，但是数据的标签是干净的，是在数据本身上加了扰动。</p>
<p>这里讲的方法很多，可能用clean-label做还是更符合现实一点。</p></li>
</ol>
<h1 id="最优的防御方法">最优的防御方法</h1>
<h2 id="防御方法">防御方法</h2>
<p>根据防御是针对数据还是模型，分为基于数据的防御和基于模型的防御。</p>
<ol type="1">
<li><p>data-based：在数据收集阶段对收集到的数据进行检测，来检测数据是否被投毒</p>
<p>主要的方法就是<strong>数据消毒</strong>。数据中毒是找出训练数据在整个特征空间的特征分布，然后利用这个分布来剔除有害数据，不需要对模型进行操作，是一种完全基于数据的方法。</p></li>
<li><p>model-based：在模型训练阶段来检测。</p>
<p>基于模型的方法是在训练阶段，会附加一些额外步骤，通过模型的准确率和参数变化，来判断是否有中毒数据。</p></li>
</ol>
<p>data-based和model-based方法并不冲突，有时会串联在一起用。原因是：某些精巧的攻击方法，可能会绕过data-based
method，同时例如在自动驾驶中，数据的分布突然变化的特别大，这时有可能将正常数据<strong>误判</strong>为中毒数据。</p>
<p>因此可能model-based方法更加通用一点。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/26/first-order-efficient-general-clean-label-data-poisoning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/26/first-order-efficient-general-clean-label-data-poisoning/" class="post-title-link" itemprop="url">First-Order Efficient General-Purpose Clean-Label Data Poisoning</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-26 19:10:11" itemprop="dateCreated datePublished" datetime="2023-10-26T19:10:11+08:00">2023-10-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-11-13 15:44:41" itemprop="dateModified" datetime="2023-11-13T15:44:41+08:00">2023-11-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="abstract">abstract</h1>
<p>先简单介绍了一下中毒攻击，然后指出existing work的局限性：</p>
<ul>
<li>大多都是<strong>bi-level
optimization</strong>（二阶层优化）问题。这会导致在求解问题的时候有很大的计算量。</li>
<li>现有的方法都是用<strong>feature
collision</strong>（特征碰撞）的方法，这些方法对没见过的特征空间效果不是很好，转移到其他场景也不是很容易</li>
</ul>
<p>于是作者提出了一个一阶的方法，这个方法理论上可以大概达到二阶的效果，总结一下其性质：</p>
<ul>
<li>高效（以一阶接近二阶的性能）</li>
<li>转移（对于不同的特征空间都能投毒）</li>
<li>泛化（可以适用于别的场景，CV、WTP）</li>
</ul>
<h1 id="metapoison">MetaPoison</h1>
<p>这篇文章跟之前的MetaPoison进行了对比，其指出“MetaPoison有很多优点，例如：基于元学习、双层优化、不是基于特征碰撞来做的。但是，这个整个的学习需要区分内环（inner-
loop）的学习进程，因此它其实还是<strong>隐式利用了二阶信息</strong>”。</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031094403031.png" alt="image-20231031094403031">
<figcaption aria-hidden="true">image-20231031094403031</figcaption>
</figure>
<p>而本文中提出的是纯的利用一阶信息的元学习（pure first-order metra
learning）反向传播的过程不需要二阶信息。</p>
<h1 id="threat-model">threat model</h1>
<h2 id="clean-label-data-poisoning">clean-label data poisoning</h2>
<p>对攻击者的知识、能力做一个假设。攻击者只能通过注射很少的有毒数据样本到模型的训练数据集中。有毒数据样本是通过<strong>代理模型</strong>以及<strong>目标数据</strong>来生成的。</p>
<p>举个例子，目标数据的标签是鸟，通过将目标数据（鸟）放进代理模型中去，得到的输出是有毒数据，然后将有毒数据拿来训练，最终测试的时候模型就会将鸟识别成狗了。</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031125626227.png" alt="image-20231031125626227">
<figcaption aria-hidden="true">image-20231031125626227</figcaption>
</figure>
<h2 id="gray-box-or-black-box-setting">gray-box or black-box
setting</h2>
<ol type="1">
<li><p>灰盒</p>
<p>攻击者知道受害者的模型的架构。</p>
<p>这个假设有一定的合理性，因为能达到优秀性能的模型就那几个，攻击者就算通过瞎猜有可能能够命中，或者命中相似的架构。</p></li>
<li><p>黑盒</p>
<p>攻击者对受害者一无所知。</p>
<p>也就是说攻击者只能随机选取架构和参数来构建代理模型。</p>
<p>为了增加泛化性，攻击者可以采取集成学习的方法来构建很多个代理模型。</p></li>
</ol>
<h1 id="method">method</h1>
<h2 id="基本攻击策略">基本攻击策略</h2>
<p>提出的攻击主要分两步：</p>
<ol type="1">
<li><p>产生攻击者想要达到的模型更新<span class="math inline">\(\delta
_{\theta}\)</span></p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031183749536.png" alt="image-20231031183749536">
<figcaption aria-hidden="true">image-20231031183749536</figcaption>
</figure>
<p>后续会用<span class="math inline">\(\{X_p,Y_p\}\)</span>来代替<span class="math inline">\(\{X_p\or X_c,Y_p \or Y_c
\}\)</span>，但并不代表不考虑干净数据，考虑到希望朝着中毒数据的方向优化，因此会削弱一下干净数据。</p></li>
<li><p>对中毒数据施加扰动<span class="math inline">\(\delta
_{p}\)</span>。</p>
<p>在上面的不等式两边加上一阶信息（<strong>加上的东西是相等的吗？</strong>）</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031190219475.png" alt="image-20231031190219475">
<figcaption aria-hidden="true">image-20231031190219475</figcaption>
</figure>
<p>然后做一个变换</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031190433331.png" alt="image-20231031190433331">
<figcaption aria-hidden="true">image-20231031190433331</figcaption>
</figure>
<p>假设<span class="math inline">\(\theta\)</span>能够取到最优，那么<span class="math inline">\(\theta +
\delta_{\theta}\)</span>取到的就是次优，那么不等式右边就为一个负数，可能是-1、-2等，这个值取决于<span class="math inline">\(\theta + \delta
_{\theta}\)</span>偏离最优解的距离，于是有下式：</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031191051034-8750651.png" alt="image-20231031191051034">
<figcaption aria-hidden="true">image-20231031191051034</figcaption>
</figure>
<p>若是上式子的不等号改为<strong>远小于</strong>，换句话，实际情况中左边式子<strong>远小于0</strong>，这里读者的理解是-1、-2这种自然也是远远小于，但作者的意思可能是左边的式子是一个负的高阶无穷小，也就是说<span class="math inline">\(\theta + \delta_{\theta}\)</span>离最优解<span class="math inline">\(\theta\)</span>的距离很近的情况。</p>
<p>然后作者表示，将扰动<span class="math inline">\(\delta
_p\)</span>沿着下面式子的方向进行优化就行了：</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031192106232.png" alt="image-20231031192106232">
<figcaption aria-hidden="true">image-20231031192106232</figcaption>
</figure></li>
<li><p>最终算法</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031193211754.png" alt="image-20231031193211754">
<figcaption aria-hidden="true">image-20231031193211754</figcaption>
</figure>
<p>PS：式子9为：</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031193300460.png" alt="image-20231031193300460">
<figcaption aria-hidden="true">image-20231031193300460</figcaption>
</figure>
<h1 id="result">result</h1>
<p>本文中的方法：</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031193515229.png" alt="image-20231031193515229">
<figcaption aria-hidden="true">image-20231031193515229</figcaption>
</figure>
<p>其他的方法：</p>
<figure>
<img data-src="./first-order-efficient-general-clean-label-data-poisoning/image-20231031193632373.png" alt="image-20231031193632373">
<figcaption aria-hidden="true">image-20231031193632373</figcaption>
</figure></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/23/OBLIVION-poisoning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.gif">
      <meta itemprop="name" content="chengyiqiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/23/OBLIVION-poisoning/" class="post-title-link" itemprop="url">OBLIVION_poisoning</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-23 08:09:38" itemprop="dateCreated datePublished" datetime="2023-10-23T08:09:38+08:00">2023-10-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-12-08 17:38:56" itemprop="dateModified" datetime="2023-12-08T17:38:56+08:00">2023-12-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="abstract">abstract</h1>
<p>develop an advanced model poisoning attack against defensive
aggregation rules</p>
<p>2 components/features:</p>
<ol type="1">
<li><p>rank the priority of models weights to find whose influence is
bigger. Then we do poisoning attack to the it.</p>
<p>Basically, it is better than average poisoning attack to all
weights.</p></li>
<li><p>Smoothing the malicious model update.</p></li>
</ol>
<p>Real-world FL framework(PLATO) experiment.</p>
<h1 id="problem-formulation">problem formulation</h1>
<h2 id="threat-model">Threat model</h2>
<p>some assumptions about attacker:</p>
<ol type="1">
<li><p>Knowledge:</p>
<ol type="1">
<li>the compromised client's <strong>local dataset</strong></li>
<li>if one of the compromised client is chosen by server, then the
attacker can access to the <strong>current global model.</strong></li>
<li>Don't know the server's <strong>aggregate rules.</strong></li>
<li>Don't know anything about <strong>benign clients</strong></li>
</ol></li>
<li><p>capability:</p>
<p>can manipulate all compromised clients together.</p></li>
<li><p>Goal::</p>
<p>untargeted poisoning, which means the attacker aims to degrade the
performance of global model.</p></li>
</ol>
<h2 id="problem-formula">problem formula</h2>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023094202585.png" alt="image-20231023094202585">
<figcaption aria-hidden="true">image-20231023094202585</figcaption>
</figure>
<ul>
<li><span class="math inline">\(\bigtriangledown ^b\)</span>: the
attacker don't know the benign clients, so we use the global model
weights and compromised clients locan dataset to train and
<strong>estimate</strong> <span class="math inline">\(\bigtriangledown
^b\)</span>.</li>
<li><span class="math inline">\(\mathcal P\)</span>: the pertubation
function</li>
<li><span class="math inline">\(\bigtriangledown ^p\)</span>: the
malicious perturbation based on <span class="math inline">\(\bigtriangledown ^b\)</span></li>
<li><span class="math inline">\(\gamma\)</span>: the scale factor to
adjust attack performance. It should'nt be too large and this is to
avoid the server discards the poisoning model update.</li>
</ul>
<p>our object is to fine-tune factor <span class="math inline">\(\gamma\)</span> and find function <span class="math inline">\(\mathcal P\)</span></p>
<p>then max. the loss <span class="math inline">\(\mathcal
F\)</span></p>
<h1 id="detailed-construction">detailed construction</h1>
<h2 id="design-rationale">Design Rationale</h2>
<p>to enhance the performance.</p>
<h3 id="weight-prioritization">Weight prioritization</h3>
<p>exploit the <strong>catastrophic forgetting</strong>: this means when
a system learns new knowledge, it will forget what it has learned in the
past.</p>
<p>To NN model, it is same. Based on this, we choose to poisoning attack
the <strong>most important weight</strong></p>
<h3 id="dynamic-smoothing">dynamic smoothing</h3>
<p>incorporates the <strong>history of benign model updates</strong> in
<strong>calculating malicious model updates</strong> in the current
round.</p>
<h2 id="weight-prioritization-1">Weight prioritization</h2>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115006500.png" alt="image-20231023115006500">
<figcaption aria-hidden="true">image-20231023115006500</figcaption>
</figure>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115016876.png" alt="image-20231023115016876">
<figcaption aria-hidden="true">image-20231023115016876</figcaption>
</figure>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115231393.png" alt="image-20231023115231393">
<figcaption aria-hidden="true">image-20231023115231393</figcaption>
</figure>
<p>The <span class="math inline">\(\bigodot\)</span> means
<strong>Hadamard product</strong>, for example: <span class="math display">\[
[1,2,3]\bigodot [3,4,5]=[3, 8, 15]
\]</span></p>
<h2 id="dynamic-smoothing-1">dynamic smoothing</h2>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115029437.png" alt="image-20231023115029437">
<figcaption aria-hidden="true">image-20231023115029437</figcaption>
</figure>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115037019.png" alt="image-20231023115037019">
<figcaption aria-hidden="true">image-20231023115037019</figcaption>
</figure>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115049435.png" alt="image-20231023115049435">
<figcaption aria-hidden="true">image-20231023115049435</figcaption>
</figure>
<h2 id="algorithm">algorithm</h2>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231023115937080.png" alt="image-20231023115937080">
<figcaption aria-hidden="true">image-20231023115937080</figcaption>
</figure>
<p>补充一下第二部分的内容，以便做实验</p>
<h2 id="preliminaries-and-related-work">PRELIMINARIES AND RELATED
WORK</h2>
<h2 id="federated-learning">Federated Learning</h2>
<p>联邦学习可以 表示为如下几个过程：</p>
<ol type="1">
<li><p>初始化</p>
<p>如果是第一轮，那么服务器需要初始化参数，然后从客户端集合中选出一个子集，对子集分发模型参数。</p></li>
<li><p>客户端本地训练</p>
<p>本地客户端i拿到权重之后，在本地的数据集上进行优化，然后上传权重更新给服务器即可：<span class="math inline">\(\bigtriangledown _i^t=\theta
_i^t-\theta_G^{t-1}\)</span></p></li>
<li><p>服务器聚合更新</p>
<p><span class="math inline">\(\bigtriangledown _G^t=f_{agr}
(\{\bigtriangledown _i^t\})\)</span></p>
<p><span class="math inline">\(\theta _G^t=\theta_G^{t-1}+\eta
\bigtriangledown _G^t\)</span></p></li>
</ol>
<p>本地训练+服务器聚合这个操作对一直进行，直到模型收敛｜到达最大轮数</p>
<h2 id="model-poisoning-attacks-in-federated-learning">Model Poisoning
Attacks in Federated Learning</h2>
<p>前面简答介绍了指向性和非指向性攻击，前者影响某一特定类别的准确率，后者是直接影响整个模型的性能。然后本文选取的是非指向性攻击。</p>
<p>根据攻击者的认知，可以将非指向性攻击分为：AGR-tailored
attacks和AGR-agnostic attacks：</p>
<ol type="1">
<li><p>AGR-tailored attacks</p>
<p>假设攻击者知道服务器的聚合规则，那样攻击者就可以根据这个聚合规则来制定策略。</p></li>
<li><p><strong>AGR-agnostic attacks</strong></p>
<p>攻击者不知道服务器的聚合规则，这种情况更符合实际，因此考虑这种攻击能够使得我们的模型更加通用。</p></li>
</ol>
<p>下面是一些符号表示：</p>
<ul>
<li><span class="math inline">\(\bigtriangledown
^m\)</span>：表示恶意客户端的更新</li>
<li><span class="math inline">\(\bigtriangledown
^b\)</span>：表示正常客户端的更新，事实上攻击者不知道其他的干净客户端的数据集，所以攻击者只能用自己的恶意客户端的数据集来代替干净客户端的数据集。</li>
<li><span class="math inline">\(\bigtriangledown
^p\)</span>：对正常的更新增加的扰动</li>
</ul>
<p>下面是三种最流行的AGR-agnostic attacks：</p>
<ol type="1">
<li><p><strong>LIE [16]</strong></p>
<p>LIE假设所有良性客户端的更新服从一个在<span class="math inline">\(\bigtriangledown
^b\)</span>周围的分布，对攻击者而言，支持者标记为<span class="math inline">\(\bigtriangledown ^+\)</span>，反对者标记为<span class="math inline">\(\bigtriangledown
^-\)</span>，恶意更新将会被制造在<span class="math inline">\(\bigtriangledown ^b\)</span>和<span class="math inline">\(\bigtriangledown
^+\)</span>之间，从而误导服务器将<span class="math inline">\(\bigtriangledown ^-\)</span>识别为异常值。</p>
<p><img data-src="./OBLIVION-poisoning/image-20231105154334303.png" alt="image-20231105154334303">限制在<img data-src="./OBLIVION-poisoning/image-20231105154409779.png" alt="image-20231105154409779"></p></li>
<li><p><strong>Min-Max [12]</strong></p>
<p>有三个不同的扰动函来优化攻击性能。目标是：找到最边缘的的<span class="math inline">\(\gamma\)</span>，使得恶意更新和良性更新的最大距离和良性更新之间的最大距离一样。</p></li>
<li><p><strong>Min-Sum [12]</strong></p>
<p>和Min-Max比较类似，但是多了平方：</p>
<p><img data-src="./OBLIVION-poisoning/image-20231105154433398.png" alt="image-20231105154433398">限制在<img data-src="./OBLIVION-poisoning/image-20231105154447396.png" alt="image-20231105154447396"></p></li>
</ol>
<h1 id="experience">experience</h1>
<h2 id="experience-setup">experience setup</h2>
<p>三个<strong>数据集</strong>：FEMINIST, CIFAR-10, Purchase</p>
<ol type="1">
<li><p><strong>FEMINIST</strong></p>
<p>灰度、字符数据集：由3400用户书写的，一共有805263张28*28的灰度图。</p>
<p>作者选用LeNet5作为全局模型，客户端有3400个</p></li>
<li><p><strong>CIFAR-10</strong></p>
<p>一个类别平衡的图像数据集，有10类，共60000张32*32的图片。</p>
<p>作者选50000作为训练，10000作为测试，客户端500个，使用的模型是VGG11。</p></li>
<li><p><strong>Purchase</strong></p>
<p>一个类别不平衡的数据集，用来对顾客进行分类，有100类，共197324个二进制特征向量。</p>
<p>作者用钱18000作为训练，考虑1000个客户端，使用的模型是MLP（600，1024，512，256，100）</p>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231105203336335.png" alt="image-20231105203336335">
<figcaption aria-hidden="true">image-20231105203336335</figcaption>
</figure></li>
</ol>
<p>下面是<strong>模型参数</strong>的设置：</p>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231105132103810.png" alt="image-20231105132103810">
<figcaption aria-hidden="true">image-20231105132103810</figcaption>
</figure>
<p>参数解释：</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span>：选取前多少作为高优先级，其余设置为低优先级</li>
<li><span class="math inline">\(\beta _1\)</span>：<span class="math inline">\(\bigtriangledown ^b\)</span>的动态平滑因子</li>
<li><span class="math inline">\(\xi\)</span>：<span class="math inline">\(\bigtriangledown ^m\)</span>的动态平滑因子</li>
<li><span class="math inline">\(\mathcal P\)</span>：扰动函数</li>
<li><span class="math inline">\(\mathcal
k\)</span>：阈值，用来判断被选中的恶意客户端是否超过该值</li>
</ul>
<p>对于服务器的<strong>聚合规则</strong>设置，本文作者选了6种：non-defensive
聚合规则、FEDAVG，以及下面4种：</p>
<ul>
<li>MULTI-KRUM [4]</li>
<li>TRIMMED-MEAN [5]</li>
<li>MEDIAN [5]</li>
<li>AFA [8]</li>
</ul>
<p>作者选取的<strong>联邦学习的框架</strong>是PLATO，训练200轮，每轮服务器会随机选取30个客户端进行本地训练，batchsize是10，lr是0.05，每次本地训练训练一轮，假设有百分之20的客户端是恶意客户端。</p>
<p><strong>PS：</strong>拿Purchase数据集举例子，一共有1000个客户端，然后百分之20%是恶意客户端，那么就有200个恶意客户端，一次选取30个客户端用来本地训练。还算合理的范围，对攻击者来说也还是有攻击难度。</p>
<p>下面是实验效果：</p>
<figure>
<img data-src="./OBLIVION-poisoning/image-20231105183928487.png" alt="image-20231105183928487">
<figcaption aria-hidden="true">image-20231105183928487</figcaption>
</figure>
<p>可以看到，在加入OBIVION攻击后，模型的性能或多或少有了下降。</p>
<h2 id="minmax_attack">minmax_attack</h2>
<p>minmax是后续攻击的基础，因此从minmax入手：</p>
<ul>
<li><p>从更新中取出权重变化，即为<strong>deltas_received</strong>，有30个客户端，deltas_received的长度就是30，有30个delta_received</p></li>
<li><p>统计；攻击者的数量（id小于100的）、模型每层的名字</p></li>
<li><p>将所有的更新拼接，<strong>all_updates</strong>，shape为[30,
9236626]。30指的是30个被选中的客户端，9236626是每个客户端每一层的参数首先view(-1)然后concat后的结果。</p></li>
<li><p>将attacker的更新拼接，得到的是<strong>attacker_grads</strong>，shape为[6,
9236626]，方法和上面的是一样的</p></li>
<li><p>将attacker_grads沿着维度0取平均，得到<strong>model_re</strong>，shape为[9236626]</p></li>
<li><p>根据偏差类型，得出偏差deviation（单位向量、符号、标准差……）</p></li>
<li><p>设置<span class="math inline">\(\lambda=\lambda
_{fail}=50,\lambda _{success}=0\)</span>，<span class="math inline">\(\lambda\)</span>是最后的偏差系数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mal_update = (model_re - lamda_succ * deviation)</span><br></pre></td></tr></table></figure></li>
<li><p>计算每个攻击者的攻击更新和attacker_grads的distance，然后取出最大距离<strong>max_distance</strong></p></li>
<li><p>先计算恶意更新<strong>mal_update</strong>，然后计算所有更新all_updates和mal_update的L2距离<strong>distance</strong>，同样是取出最大值，计作<strong>max_d</strong>，将max_d和max_distance进行对比，来更新<span class="math inline">\(\lambda\)</span>，直到<span class="math inline">\(\lambda _{success}\)</span>和<span class="math inline">\(\lambda\)</span>的差值小于阈值，得到最终的恶意更新。</p></li>
<li><p>通过renew_malicious_updates更新恶意更新。</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">chengyiqiu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
