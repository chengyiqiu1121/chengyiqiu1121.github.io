
  <!DOCTYPE html>
  <html lang="en"  >
  <head>
  <meta charset="utf-8">
  

  

  

  
  <script>
    window.icon_font = '4552607_ikzjpc9jicn';
  </script>
  
  
  <title>
    EasyRL |
    
    Hexo
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CUbuntu%20Mono:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" as="style" onload="this.onload&#x3D;null;this.rel&#x3D;&#39;stylesheet&#39;">
  
  
<link rel="stylesheet" href="/css/loader.css">

  <meta name="description" content="EasyRL 绪论强化学习概述以智能体（agent）为对象：agent从环境中获取状态s以及奖励r（这在时间$t_0$没有），然后做出一个决策a，agent需要极大化奖励r 环境（environment）：接受agent的决策a，输出下一个s和r ![截屏2023-09-02 09.05.37](.&#x2F;EasyRL&#x2F;截屏2023-09-02 09.05.37.png)  RL和监">
<meta property="og:type" content="article">
<meta property="og:title" content="EasyRL">
<meta property="og:url" content="http://example.com/2023/09/02/EasyRL/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="EasyRL 绪论强化学习概述以智能体（agent）为对象：agent从环境中获取状态s以及奖励r（这在时间$t_0$没有），然后做出一个决策a，agent需要极大化奖励r 环境（environment）：接受agent的决策a，输出下一个s和r ![截屏2023-09-02 09.05.37](.&#x2F;EasyRL&#x2F;截屏2023-09-02 09.05.37.png)  RL和监">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-04%2019.26.14.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-04%2019.30.02.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-05%2010.17.38.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-05%2013.54.23.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-05%2010.40.09.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-05%2010.41.55.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-05%2014.01.38.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-05%2014.13.21.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-06%2014.22.47.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916091250670.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230919173023190.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230919172752528.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-19%2017.28.58.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916094420973.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916100807256.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916114122709.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916114219478.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916140521653.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230916141206633.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-18%2020.10.06.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-18%2020.39.01.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-20%2012.37.58.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-20%2012.54.33.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920130856256.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920140719345.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920140746494.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-20%2014.08.17.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920140913045.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920142307309.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920142729662.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230920144424148.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-20%2020.45.10.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-20%2020.48.00.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921093351256.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921093418352.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921094228980.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921095638011.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921100436205.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921101622477.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921102938686.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921103152512.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921103602015.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921104133435.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921105753969.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921131336961.png">
<meta property="og:image" content="http://example.com/2023/09/02/EasyRL/image-20230921131538520.png">
<meta property="article:published_time" content="2023-09-02T03:07:15.000Z">
<meta property="article:modified_time" content="2023-10-11T01:18:32.000Z">
<meta property="article:author" content="chengyiqiu">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/09/02/EasyRL/%E6%88%AA%E5%B1%8F2023-09-04%2019.26.14.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="https://npm.webcache.cn/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="https://npm.webcache.cn/katex@0.16.9/dist/katex.min.css">

  
  
  
  
<script src="https://npm.webcache.cn/pace-js@1.2.4/pace.min.js"></script>

  
    
<link rel="stylesheet" href="https://npm.webcache.cn/wowjs@1.1.3/css/libs/animate.css">

    
<script src="https://npm.webcache.cn/wowjs@1.1.3/dist/wow.min.js"></script>

    <script>
      new WOW({
        offset: 0,
        mobile: true,
        live: false
      }).init();
    </script>
  
  
    <script src="/sw.js"></script>
  
<meta name="generator" content="Hexo 7.2.0"></head>

  <body>
    
  <div id='loader'>
    <div class="loading-left-bg"></div>
    <div class="loading-right-bg"></div>
    <div class="spinner-box">
      <div class="loading-taichi">
        <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
          <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
          <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
          <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
        </svg>
      </div>
      <div class="loading-word">少女祈祷中...</div>
    </div>
  </div>
  </div>
  <script>
    var time = null;
    const startLoading = () => {
      time = Date.now();
      document.getElementById('loader').classList.remove("loading");
    }
    const endLoading = () => {
      if (!time) {
        document.body.style.overflow = 'auto';
        document.getElementById('loader').classList.add("loading");
      } else {
        if (Date.now() - time > 500) {
          time = null;
          document.body.style.overflow = 'auto';
          document.getElementById('loader').classList.add("loading");
        } else {
          setTimeout(endLoading, 500 - (Date.now() - time));
          time = null;
        }
      }
    }
    window.addEventListener('load', endLoading);
    document.getElementById('loader').addEventListener('click', endLoading);
  </script>


    <div id="container">
      <div id="wrap">
        <div id="header-nav">
  <nav id="main-nav">
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/">Home</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/archives">Archives</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/about">About</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/friend">Friend</a>
      </span>
    
    <a id="main-nav-toggle" class="nav-icon"></a>
  </nav>
  <nav id="sub-nav">
    
      <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
    
    
    
  </nav>
</div>
<header id="header">
  
    <img fetchpriority="high" src="/images/banner.jpg" alt="EasyRL">
  
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <div id="logo-wrap">
        
          
          
            <a href="/" id="logo">
              <h1>EasyRL</h1>
            </a>
          
        
      </div>
      
        
        <h2 id="subtitle-wrap">
          
        </h2>
      
    </div>
  </div>
</header>

        <div id="content" class="outer">
          
          <section id="main"><article id="post-EasyRL" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    <div class="article-meta">
      <div class="article-date wow slideInLeft">
  <a href="/2023/09/02/EasyRL/" class="article-date-link">
    <time datetime="2023-09-02T03:07:15.000Z" itemprop="datePublished">2023-09-02</time>
  </a>
</div>

      
  <div class="article-category wow slideInLeft">
    <a class="article-category-link" href="/categories/EasyRL/">EasyRL</a>
  </div>


    </div>
    <div class="hr-line"></div>
    

    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>EasyRL</p>
<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="强化学习概述"><a href="#强化学习概述" class="headerlink" title="强化学习概述"></a>强化学习概述</h2><p>以智能体（agent）为对象：agent从环境中获取状态s以及奖励r（这在时间$t_0$没有），然后做出一个决策a，agent需要极大化奖励r</p>
<p>环境（environment）：接受agent的决策a，输出下一个s和r</p>
<p>![截屏2023-09-02 09.05.37](.&#x2F;EasyRL&#x2F;截屏2023-09-02 09.05.37.png)</p>
<pre><code> RL和监督学习（supervised learning）有很大区别：
</code></pre>
<table>
<thead>
<tr>
<th align="center">SL</th>
<th align="center">RL</th>
</tr>
</thead>
<tbody><tr>
<td align="center">数据有label</td>
<td align="center">没有label，只有奖励（delay）</td>
</tr>
<tr>
<td align="center">假设数据服从iid</td>
<td align="center">前后数据有很强的关联性</td>
</tr>
<tr>
<td align="center">学习器根据label来对修正预测</td>
<td align="center">learner需要不断探索、利用，以极大化奖励</td>
</tr>
<tr>
<td align="center">最好的效果是和人一样</td>
<td align="center">效果没有上限</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">agent的action会影响后来的数据</td>
</tr>
</tbody></table>
<p>强化学习的应用：</p>
<ul>
<li>DeepMind走路的智能体</li>
<li>机械臂抓取，学习到一个统一的抓取算法</li>
<li>OpenAI机械臂玩魔方</li>
<li>穿衣股的智能体</li>
</ul>
<p>![截屏2023-09-02 09.52.11](.&#x2F;EasyRL&#x2F;截屏2023-09-02 09.52.11.png)</p>
<h2 id="序列决策"><a href="#序列决策" class="headerlink" title="序列决策"></a>序列决策</h2><p>上一节中说的状态s，也可以称为<strong>观测（observation）</strong>，简称o，智能体最终的目标就是，<strong>从这些观测中学到能够极大化奖励的策略</strong>。</p>
<p>这里对状态s和观测做一个区分：</p>
<ul>
<li>状态是对世界的完整描述</li>
<li>观测是对状态的部分描述</li>
</ul>
<p>也就是说，环境输出状态，智能体得到观测，这中间是有<strong>信息损失</strong>的。</p>
<p>奖励是环境给智能体的一种反馈信号，能够显示智能体在某一步的决策效果如何。但是这个奖励往往是延后的（delay reward），很可能是在一局游戏结束之后才能反馈给agent。</p>
<p>下面能够更清楚的阐述状态和观测。</p>
<p>可以把历史看作观测、动作、奖励的序列：<br>$$<br>H_t&#x3D;o_1,a_1,r_1,…,o_t,a_t,r_t<br>$$<br>整个游戏的状态可以表述为：<br>$$<br>s_t&#x3D;f(H_t)<br>$$<br>环境有环境的函数$s_t^e$来更新状态，而智能体也有智能体的函数$s_t^a$更新状态。当智能体和环境能够互相观察到对方的所有状态时，称这个环境是<strong>完全可观测的（fully observed）</strong>。$o_t&#x3D;s_t^a&#x3D;s_t^e$，这时将强化学习建模为一个马尔可夫决策过程。</p>
<p>但是大部分情况智能体只能得到局部的状态，这时环境为<strong>部分可观测的</strong>。将我们的模型建为<strong>部分可观测马尔可夫决策过程</strong>，这是马尔可夫决策过程的一个泛化。自动驾驶就是这样的，智能体能观测到车上的所有的传感器的信息，并不能得到全部的环境的信息。</p>
<h2 id="动作空间"><a href="#动作空间" class="headerlink" title="动作空间"></a>动作空间</h2><p>动作空间指的是：在给定环境中，agent可采取的有效动作的集合。分两类：</p>
<ul>
<li>离散动作空间：简单迷宫只有东南西北四个方向</li>
<li>连续动作空间：复杂迷宫机器人可以360度旋转</li>
</ul>
<h2 id="智能体的组成成分"><a href="#智能体的组成成分" class="headerlink" title="智能体的组成成分"></a>智能体的组成成分</h2><h3 id="策略函数"><a href="#策略函数" class="headerlink" title="策略函数"></a>策略函数</h3><p>策略的定义：策略是智能体的动作模型。</p>
<p>在实现上，策略是一个函数，输入为状态，输出为动作。有两种策略：</p>
<ul>
<li>随机性策略：$\pi$函数，输入一个状态，输出智能体所有动作的概率（0.7向左，0.3向右），对于某一种状态，随机性策略会按照某种特定的概率分布来进行action</li>
<li>确定性策略：对于一种状态，输出总是确定的。</li>
</ul>
<p>通常采用随机性策略，这样能让智能体更好的探索环境，尤其是在多个智能体的情况下。</p>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>价值函数反应当前的状态的好坏，可以使用Q函数。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型指的是对环境进行建模，而不是我们训练出来的深度学习的模型。</p>
<p>模型决定下一步的状态，模型的输入为：当前的状态以及智能体采取的动作；输出为：下一步的状态。</p>
<p><strong>策略+价值函数+模型&#x3D;马尔可夫决策过程。</strong></p>
<p>有两种RL：</p>
<ul>
<li>基于策略的强化学习（policy-based RL）：当智能体已经学好整个环境后，在每一个状态都能得到最佳的action</li>
<li>基于价值的强化学习（value-based RL）：每一步的决策跟价值函数有关系，每一步都是以价值函数为导向。</li>
</ul>
<h2 id="智能体的类型"><a href="#智能体的类型" class="headerlink" title="智能体的类型"></a>智能体的类型</h2><h3 id="policy-or-value-based"><a href="#policy-or-value-based" class="headerlink" title="policy or value based"></a>policy or value based</h3><p>通过对智能体是学习价值还是策略，我们还可以对智能体进行划分：</p>
<ul>
<li><p>value-based agent：显式学习价值函数，隐式学习策略</p>
<ul>
<li>value-based：不会学习策略（显），会维护一张价值表或者价值函数，使得我们的长期价值能够最大化。</li>
<li>一般在离散环境下进行使用（如俄罗斯方块，围棋）</li>
</ul>
</li>
<li><p>policy-based agent：直接学习策略</p>
<ul>
<li>policy- based：RL会不断对策略进行优化，使得对于每一个状态，都能做出能够获得最大奖励的决策。</li>
<li>在工业机器人这种连续环境下一般使用policy based</li>
</ul>
</li>
<li><p>演员-评论员智能体（actor-critic agent）：同时学习价值和策略</p>
<ul>
<li>二者同时进行，可以加速学习过程。</li>
</ul>
</li>
</ul>
<p>policy based和value based分别有其学习算法，policy based 使用的是基于梯度的，梯度策略算法（policy gradient，PG）；而value based则是使用的Q算法（Q-learning）</p>
<h3 id="model-based"><a href="#model-based" class="headerlink" title="model based"></a>model based</h3><p>根据agent有没有学习环境模型来进行分类：</p>
<ul>
<li>model-based：会通过学习环境状态的转移来采取动作<ul>
<li>model based需要知道环境的<strong>状态转移函数</strong>以及奖励函数，这样就能推断出在采取某一action后会带来的reward和下一个state。因此，model based 可以在一个虚拟环境中使用，而不需要在真实环境中进行学习。</li>
</ul>
</li>
<li>model-free：智能体没有得到环境的转移，只通过学习价值函数和策略函数来进行决策<ul>
<li>但是在实际应用中，智能体很难知道环境状态转移函数（还是自动驾驶的例子，智能体只能知道有限个传感器的数据，并不能知道环境的所有状态），因此大多数情况下我们使用的是model free</li>
</ul>
</li>
</ul>
<p>![截屏2023-09-04 17.25.58](.&#x2F;EasyRL&#x2F;截屏2023-09-04 17.25.58.png)</p>
<h2 id="学习和规划"><a href="#学习和规划" class="headerlink" title="学习和规划"></a>学习和规划</h2><p>learning和planning，分别考虑下面两个场景：</p>
<ul>
<li>在不知道环境时，智能体要一直和环境交互，来改进策略。这是一个学习过程。</li>
</ul>
<p>![截屏2023-09-04 17.33.37](.&#x2F;EasyRL&#x2F;截屏2023-09-04 17.33.37.png)</p>
<ul>
<li>在智能体能够知道环境中的所有信息时，智能体能够计算出一个完美的模型，然后去做决策。这是一个规划过程。</li>
</ul>
<p>![截屏2023-09-04 17.33.54](.&#x2F;EasyRL&#x2F;截屏2023-09-04 17.33.54.png)</p>
<p>我们可以先让智能体跟环境交互，学到一个虚拟环境后，再在环境中进行计算，做决策。</p>
<h2 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h2><p>关于探索和利用：</p>
<ul>
<li>探索：指的是智能体去探索环境，通过不断的试错，来发现哪些动作可以带来奖励。</li>
<li>利用：利用好目前已知的动作，来获得奖励。</li>
</ul>
<p>这里有一个问题：是<strong>长期奖励</strong>还是<strong>短期奖励</strong>。对于当前环境，智能体的已知动作空间中，已经有动作能够带来不错的奖励时，这时智能体是选择探索还是利用呢？我们需要权衡利弊。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在赛车游戏中，赛车可以通过吃道具来加分，吃完了道具会隔段时间刷新。有可能会出现这种情况：智能体一直在原地兜圈，等待道具刷新...</span><br></pre></td></tr></table></figure>

<h1 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h1><h2 id="Markov-process"><a href="#Markov-process" class="headerlink" title="Markov process"></a>Markov process</h2><h3 id="Markov-property"><a href="#Markov-property" class="headerlink" title="Markov property"></a>Markov property</h3><p>在给定过去状态以及现在状态的情况下，未来的状态仅依赖于当前状态（只取决于现在）</p>
<h3 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h3><p>Markov chain满足Markov property，可以用以下公式来描述：<br>$$<br>h_t&#x3D;{s_1, …,s_t} \<br>p(s_{t+1}|h_t)&#x3D;p(s_{t+1}|s_t)<br>$$<br>也就是说，从历史中去推未来，和从当前状态推未来是等价的。</p>
<p>从$s_t$转移到$s_{t+1}$，和从过去转移到$s_{t+1}$是等价的。</p>
<p>离散的Markov process 称为Markov chain，如下图所示：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-04 19.26.14.png" alt="截屏2023-09-04 19.26.14" style="zoom: 50%;">

<p>上图中有四个状态，s1有0.1的概率转移到其本身，有0.2的概率转移到s2，有0.7的概率转移的到s4。</p>
<p>可以用<strong>状态转移矩阵P</strong>来描述状态转移$p(s_{t+1}&#x3D;s^{‘}|s_t&#x3D;s)$</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-04 19.30.02.png" alt="截屏2023-09-04 19.30.02" style="zoom:33%;">

<p>状态转移矩阵表示的是当前从状态$s_t$到状态$s_{t+1}$的所有概率</p>
<h2 id="Markov-reward-process"><a href="#Markov-reward-process" class="headerlink" title="Markov reward process"></a>Markov reward process</h2><p>MRP是Markov chain+reward function。</p>
<p>奖励reward，用<strong>R</strong>表示，如果状态有限，可以把R写成一个向量。</p>
<h3 id="return-and-value-function"><a href="#return-and-value-function" class="headerlink" title="return and value function"></a>return and value function</h3><p>horizon定义为范围，表示的是一个回合的长度（也就是采集一次轨迹，要采集多远）。</p>
<p>return则是定义为奖励的逐步叠加，称为<strong>回报</strong>，用G表示。</p>
<p>$\gamma$是折扣因子，会对后面的奖励进行折扣。</p>
<p>假设现在是时间t，t之后的奖励序列为：$r_{t+1},r_{t+2}..$，那么回报为：<br>$$<br>G_t&#x3D;r_{t+1}+\gamma r_{t+2}+\gamma ^2r_{t+2}+…<br>$$<br>上面这个式子表示我们更希望得到现在的奖励，以后得到的奖励都会大打折扣。</p>
<p>这个回报也称为折扣回报（discounted return），当有了折扣回报后，可以定义状态的价值，也就是<strong>状态价值函数</strong>（state- value function）：<br>$$<br>V^t(s)&#x3D;E(G_t|s_t&#x3D;s)<br>$$<br>State-value function表示的<strong>是智能体进入某个状态后，未来能够获得的奖励。</strong></p>
<p>使用折扣因子的理由：</p>
<ul>
<li>有些Markov process是带环的，这样可以避免智能体一直进入环内</li>
<li>不能完美模拟环境、对未来的评估不一定准确、不能完全信任模型。这种不确定性导致我们需要$\gamma$</li>
<li>更希望现在获得奖励，但是也可以调整$\gamma$，例如设置成0表示只关心当下奖励，设置成1表示未来的奖励没有打折扣。$\gamma$是一个超参数。</li>
</ul>
<p>可以用向量来表示奖励函数，例如进入s1会得到奖励5，s7会得到奖励10，其他状态无奖励：<br>$$<br>R&#x3D;[5,0,0,0,0,0,10]<br>$$<br>然后设置好$\gamma&#x3D;0.5$后，就能通过采样得到回报return：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-05 10.17.38.png" alt="截屏2023-09-05 10.17.38" style="zoom:33%;">

<p>得到return后，还没有得到value function，我们可以吧以s4开头的所有轨迹都加起来取平均值（期望），以这个值来衡量进入状态s4的价值。这种计算价值函数的方法称为：蒙特卡洛（MonteCarlo，MC）采样。</p>
<h3 id="bellman-equation"><a href="#bellman-equation" class="headerlink" title="bellman equation"></a>bellman equation</h3><p>bellman equation的推导如下：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-05 13.54.23.png" alt="截屏2023-09-05 13.54.23" style="zoom: 33%;">

<ul>
<li>其中从第四行到第五行的原因：<strong>回报的期望是等于回报的期望的期望</strong>。或者说是全期望公式：</li>
</ul>
<p>$$<br>E[V(s_{t+1}|s_t)]&#x3D;E[E[G_{t+1}|s_{t+1}]|s_t]&#x3D;E(G_{t+1}|s_t)<br>$$</p>
<ul>
<li>第五行到第六行的原因是：$E[X]&#x3D;\sum_{\i}E[X|A_i]p(A_i)$</li>
</ul>
<p>bellman equation的表示如下：<br>$$<br>V(s)&#x3D;R(s)+\gamma \sum _{s^{‘}\epsilon S}p(s^{‘}|s)V(s^{‘})<br>$$<br>第一个式子是表示的即时奖励，第二个式子表示的是未来奖励的折扣总和，$s^{‘}$表示的是未来的某个状态，$p(s^{‘}|s)$表示的是当前状态转移到未来状态的概率。</p>
<p>bellman equation表示的就是当前状态的价值。也可以说是当前状态和未来状态的迭代关系。</p>
<p>将其写成矩阵形式如下：</p>
<p>![截屏2023-09-05 10.28.56](.&#x2F;EasyRL&#x2F;截屏2023-09-05 10.28.56.png)</p>
<p>这个方程可以得到其解析解（analytic solution）<br>$$<br>V&#x3D;R+\gamma PV \<br>V&#x3D;(E-\gamma P)^{-1}R<br>$$<br>数学可解，但是让计算机去算比较困难，这个过程的时间复杂度是$O(N^3)$，当状态非常多时是算不出来的。</p>
<h3 id="计算MRP价值的迭代算法"><a href="#计算MRP价值的迭代算法" class="headerlink" title="计算MRP价值的迭代算法"></a>计算MRP价值的迭代算法</h3><p>这个价值，默认就是<strong>状态价值</strong>了。</p>
<ul>
<li>蒙特卡洛方法（基于<strong>采样</strong>）</li>
<li>bellman equation（基于<strong>动态规划</strong>）</li>
<li>时序差分学习（TD learning）</li>
</ul>
<p>蒙特卡洛方法就是上面讲的那样，以当前状态出发，在一个horizon内，找出若干真实轨迹（可能是1000），然后求均值，作为当前状态的value。</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-05 10.40.09.png" alt="截屏2023-09-05 10.40.09" style="zoom: 33%;">

<p>而bellman equation则是通过不断的自举（bootstrapping），直到方程收敛（当前状态和上一个状态差别不大）</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-05 10.41.55.png" alt="截屏2023-09-05 10.41.55" style="zoom:33%;">

<h2 id="Markov-decision-process"><a href="#Markov-decision-process" class="headerlink" title="Markov decision process"></a>Markov decision process</h2><p>马尔可夫决策过程，即MDP，是MRP+decision。状态转移的条件也变了：<br>$$<br>p(s_{t+1}&#x3D;s^{‘}|s_t&#x3D;s,a_t&#x3D;a)<br>$$<br>未来的状态不仅仅取决于当前的状态，还取决于当前智能体采取的动作。</p>
<p>奖励函数R也是一样，之前的奖励函数就是一个向量，每个状态对应一个奖励。现在奖励函数还取决于动作：<br>$$<br>R(s_t&#x3D;s,a_t&#x3D;a)&#x3D;E[r_t|s_t&#x3D;s,a_t&#x3D;a]<br>$$</p>
<h3 id="policy-in-MDP"><a href="#policy-in-MDP" class="headerlink" title="policy in MDP"></a>policy in MDP</h3><p>将当前状态带入策略函数，既可以得出下一步该采取什么动作，输出值为一个概率<br>$$<br>\pi (a|s)&#x3D;p(a_t&#x3D;a|s_t&#x3D;s)<br>$$<br>这个概率表示的就是某一个决策的概率是多少。</p>
<p>那么当我们已知策略函数$\pi$时，就可以得到新的转移函数的表达式了，这个新的状态转移函数（下式的左边）中去掉了a，也就是得到了一个MRP的转移。</p>
<p>式子(13)sum的里面，左边是采取某个动作的概率，右边是在状态s以及采取这个动作的条件下，转移到新状态的概率。所以左边本质上也是一个求期望的过程，对状态求期望。<br>$$<br>P_{\pi} (s^{‘}|s)&#x3D;\sum <em>{a\epsilon A}\pi (a|s)p(s^{‘}|s,a)<br>$$<br>同理，奖励函数也是如此：<br>$$<br>R</em>{\pi}(s)&#x3D;\sum _{a\epsilon A}\pi (a|s)R(s,a)<br>$$</p>
<h3 id="MDP和MP-MRP的区别"><a href="#MDP和MP-MRP的区别" class="headerlink" title="MDP和MP&#x2F;MRP的区别"></a>MDP和MP&#x2F;MRP的区别</h3><p>MRP&#x2F;MP（下图左）可以根据当前的状态，加上状态转移函数，直接得出下一个状态是什么；而MDP则多了一层决策函数，根据当前的状态，选出一个决策，然后再在这个决策的基础上，通过状态转移函数，得出下一步的状态。也就是说，智能体做出的决策会决定未来的状态转移</p>
<p>![截屏2023-09-05 12.22.10](.&#x2F;EasyRL&#x2F;截屏2023-09-05 12.22.10.png)</p>
<h3 id="MDP的value-function"><a href="#MDP的value-function" class="headerlink" title="MDP的value function"></a>MDP的value function</h3><p>MDP的state-value function定义：<br>$$<br>V_{\pi}(s)&#x3D;E_{\pi}[G_t|s_t&#x3D;s]<br>$$<br>其中的期望取决于智能体采取的决策。</p>
<p>引入Q函数（Q- function），也称为<strong>动作价值函数</strong>（action- value function）：</p>
<ul>
<li><strong>在某一个状态，采取某一个动作a，可能得到的回报的一个期望</strong></li>
</ul>
<p>$$<br>Q_{\pi}(s,a)&#x3D;E_{\pi}[G_t|s_t&#x3D;s,a_t&#x3D;a]<br>$$</p>
<p>PS：上面的动作价值函数的计算方法，和之前MRP求状态价值函数的算法一样，都是利用蒙特卡洛采样，采集到许多真实轨迹，然后求出这些真实轨迹的回报G，最后求均值。不同的是，之前MRP只有状态，没有动作；现在MDP使用了策略函数，需要输出一个动作，期望里面的是，当我们时间点t采取的某个动作，对其求期望，得到的其实是动作价值函数Q，最后还要根据策略进行加和。这也正是下面求和的理由。</p>
<p>这个动作价值函数其实也是基于智能体采取的某一个动作的，所以需要做一个加和：<br>$$<br>V_{\pi}&#x3D;\sum_{a\epsilon A}\pi (a|s)Q_{\pi}(s,a)<br>$$<br>对Q函数的bellman equation进行推导：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-05 14.01.38.png" alt="截屏2023-09-05 14.01.38" style="zoom:33%;">

<h3 id="bellman-expectation-equation"><a href="#bellman-expectation-equation" class="headerlink" title="bellman expectation equation"></a>bellman expectation equation</h3><p>将状态价值函数（V）和动作价值函数（Q）进行分解，可得到下式：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-05 14.13.21.png" alt="截屏2023-09-05 14.13.21" style="zoom:33%;">

<p>上式展示了动作价值函数之间的迭代关系。</p>
<h3 id="备份图"><a href="#备份图" class="headerlink" title="备份图"></a>备份图</h3><p>备份图的核心：当前的价值是和未来的价值是线性相关的。</p>
<p>这个价值可以是动作价值Q，或者是状态价值V（如下图）</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-06 14.22.47.png" alt="截屏2023-09-06 14.22.47" style="zoom:33%;">

<p>上图中，根结点代表的是当前状态、以及当前状态具有的价值；根结点下面的是动作结点，代表的是后面可能采取的一个动作；再往下是采取这个动作之后，状态发生改变，进入新的状态$s^{‘}$。</p>
<h3 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h3><p>策略评估又称为<strong>价值预测</strong>，也就是计算$V$的过程。</p>
<p>PS：为何取这个名字，其意义是–评估我们选取的某一个策略能带来多少价值。所以也可以叫价值预测。</p>
<p>对于MDP，可以看做是有人在小船上划船，每一个状态必定会采取一个动作；对于MP&#x2F;MRP，则是一个纸船在水里面飘。两者的区别在于前者是有智能体来做决策的。</p>
<p>对于MDP，选定一个策略，计算出奖励函数R，然后带入bellman方程里面不断进行迭代，直到$V$收敛，我们便可得到当下的价值。</p>
<h3 id="预测-控制"><a href="#预测-控制" class="headerlink" title="预测&amp;控制"></a>预测&amp;控制</h3><p>这其实是两类问题：</p>
<ul>
<li>预测：预测指的是，在给定一个MDP以及策略函数$\pi$时，能够计算出所有状态的价值，也就是求出$V$</li>
<li>控制：控制则是指给定MDP，但不给定策略，需要计算出最佳价值函数$V^{<em>}$以及最佳策略$\pi ^{</em>}$</li>
</ul>
<p>可以看出两者是递进的，要想解决控制问题，需要先解决预测问题。</p>
<h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>策略迭代：首先我们拥有一个初始的策略$\pi$，然后根据这个策略可以得到状态价值函数$V_{\pi}$，然后根据Q函数的公式，通过极大化Q函数，来找到一个更优的策略$\pi ^{‘}$，如此迭代下去，便能找到最佳的策略以及状态价值函数。</p>
<img src="/2023/09/02/EasyRL/image-20230916091250670.png" alt="image-20230916091250670" style="zoom:33%;">

<p>策略迭代是基于贪心策略的。</p>
<p>下面是策略迭代的步骤：</p>
<ul>
<li>初始化策略$\pi$，通过贝尔曼方程，算出价值函数$V_{\pi}$</li>
</ul>
<img src="/2023/09/02/EasyRL/image-20230919173023190.png" alt="image-20230919173023190" style="zoom:50%;">

<ul>
<li>根据状态价值函数，利用动作价值函数的贝尔曼方程的推导，得到动作价值函数</li>
</ul>
<img src="/2023/09/02/EasyRL/image-20230919172752528.png" alt="image-20230919172752528" style="zoom:50%;">

<ul>
<li>最后对$Q_{\pi}$使用arg max操作符</li>
</ul>
<img src="/2023/09/02/EasyRL/截屏2023-09-19 17.28.58.png" alt="截屏2023-09-19 17.28.58" style="zoom:50%;">

<h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>价值迭代使用的是贝尔曼方程，不断的对$V$进行迭代，直到其收敛。这样我们得到的就是一个最优的价值函数了。</p>
<p>…</p>
<h1 id="表格型方法"><a href="#表格型方法" class="headerlink" title="表格型方法"></a>表格型方法</h1><p>蒙特卡洛、Q学习和Sarsa都是表格型方法。</p>
<h2 id="model-based-model-free"><a href="#model-based-model-free" class="headerlink" title="model-based &amp; model-free"></a>model-based &amp; model-free</h2><p>这里进一步解释免模型和有模型。最大的特点从下图就可以看出了</p>
<img src="/2023/09/02/EasyRL/image-20230916094420973.png" alt="image-20230916094420973" style="zoom:33%;">

<ul>
<li>有模型</li>
</ul>
<p>model-based的条件是知道<strong>概率转移函数P以及奖励函数R</strong>，知道这个，我们就可以说这个MDP是已知的了。在知道P和R的条件下，我们可以直接通过策略迭代&#x2F;价值迭代来求解这个MDP，而不需要跟环境交互啊进行探索或者利用。</p>
<ul>
<li>免模型</li>
</ul>
<p>model-free适用于我们生活中大多数场景：书中的情景，在野外看到一头熊，我们其实是不知道转移概率以及奖励函数的，这时就只能通过试错来进行学习了。</p>
<p>当<strong>模型很大或者模型未知</strong>的时候，我们不得不采用model-free来解决问题。</p>
<h2 id="Q表格"><a href="#Q表格" class="headerlink" title="Q表格"></a>Q表格</h2><p>假设Q表格是一张已经训练好的表格，那么他应该长下面这样：</p>
<img src="/2023/09/02/EasyRL/image-20230916100807256.png" alt="image-20230916100807256" style="zoom:50%;">

<p>这张图片表示，在当前的状态s下，采取动作a，能够获取多少的长期奖励。</p>
<p>至于长期奖励or短期奖励，这里有一个很生动的例子：闯红灯</p>
<ul>
<li>在我们日常开车中，闯一次红灯，需要罚款扣分，仅仅是时间快了一点，惩罚&gt;奖励。</li>
<li>假设对象是医院的救护车，那么闯红灯的奖励就大于它的惩罚了，送病人那自然是越快越好，所以奖励&gt;&gt;惩罚</li>
</ul>
<p>这里就可以看出，长期奖励是优于短期奖励的。但也不能一味的追求长期奖励，考虑买股票的情景，假设十年之后股票会迎来一次大涨价，那么肯定不能仅仅考虑长期价值（为了十年后的价值），也要考虑当下时间点附近的价值。因此回到了之前，引入了折扣因子$\gamma$</p>
<p>回到Q表格，智能体每走一步，更新一次Q表格，这种单步更新的方法称为：时序差分方法。</p>
<h2 id="免模型预测"><a href="#免模型预测" class="headerlink" title="免模型预测"></a>免模型预测</h2><h3 id="蒙特卡洛策略评估"><a href="#蒙特卡洛策略评估" class="headerlink" title="蒙特卡洛策略评估"></a>蒙特卡洛策略评估</h3><p>蒙特卡洛是基于采样的方法，也叫MC采样。具体操作是：选取策略$\pi$，在真实世界中不断的进行采样，然后得到很多轨迹。最后计算出这些轨迹的回报，相加取均值，得到的就是<strong>采取策略$\pi$时当前状态具有的价值</strong>。</p>
<p>这种方法有局限性，只能用于有终止的MDP。另外，这里还用到了大<strong>数定理</strong>：<strong>重复次数的实验的算术平均有很高的概率接近期望值</strong> </p>
<p>在对蒙特卡洛的公式推导之前，先做一个增量均值的变换：<br>$$<br>\mu_t&#x3D;\frac{1}{t}\sum_1^tx_i \<br>&#x3D;\frac{1}{t}(x_t + \sum_i^{t-1}x_i) \<br>&#x3D;\frac{1}{t}(x_t + t\mu_{t-1}-\mu_{t-1}) \<br>&#x3D;\mu_{t-1}+\frac{1}{t}(x_t-\mu_{t-1})<br>$$<br>这样就得了当前均值和当前值以及上一时刻的均值的关系了：$\mu_t&#x3D;\mu_{t-1}+\frac{1}{t}(x_t-\mu_{t-1})$</p>
<p>我们将蒙特克罗方法也写成这种方式，称为<strong>蒙特卡洛均值</strong>：</p>
<img src="/2023/09/02/EasyRL/image-20230916114122709.png" alt="image-20230916114122709" style="zoom:50%;">

<p>将前面的分数系数计作学习率：</p>
<img src="/2023/09/02/EasyRL/image-20230916114219478.png" alt="image-20230916114219478" style="zoom:50%;">

<p>对比动态规划方法，蒙特卡洛方法的优势在于：</p>
<ol>
<li>Model-free</li>
<li>相比于动态规划，计算量小</li>
</ol>
<p>缺点：</p>
<ul>
<li>只能用于horizon已知的情况</li>
</ul>
<h3 id="时序差分"><a href="#时序差分" class="headerlink" title="时序差分"></a>时序差分</h3><p>从巴普洛夫的条件反射实验讲起，经过铃声强化后，小狗认为铃声之后会带来价值（肉），于是小狗在听到铃声之后，就会流口水。人也是一样，在第一次看到熊、熊扑过来、人被熊咬伤…到下一次人再看见熊，会直接产生恐惧的感觉（这时还没有被咬伤，但已经被强化过了），甚至到后面人没有看到熊，但是看到了熊掌，也会产生恐惧。</p>
<p>上面就是一个one-step TD的过程。</p>
<p>下面再来介绍时序差分方法，其和蒙特卡洛很像，蒙特卡洛需要采样很多<strong>真实轨迹</strong>之后，对这些轨迹求<strong>真实回报</strong>，然后更新价值；而时序差分方法最小可以走一步（<strong>采样</strong>），然后计算<strong>估计回报</strong>，并且更新价值（<strong>自举</strong>）。</p>
<p>单步时序差分，TD(1)，每走一步，就用得到的估计回报，来更新价值，这个过程就是一次自举。</p>
<img src="/2023/09/02/EasyRL/image-20230916140521653.png" alt="image-20230916140521653" style="zoom:50%;">

<p>估计回报被称为<strong>时序差分目标（TD- target）</strong>，它是由下一步得到的真实奖励以及对未来奖励的一个折扣。</p>
<p>PS：<br>$$<br>G_t&#x3D;r_{t+1}+\gamma r_{t+2}+\gamma ^2r_{t+3}+… \<br>G_t^1&#x3D;r_{t+1}+\gamma G_{t+1}\approx r_{t+1}+\gamma V_{t+1}<br>$$<br>时序差分方法有这些优点，使得它更加实用：</p>
<ol>
<li>运用了MP性质，也就是后续状态只跟当前有关</li>
<li>可以使用在<strong>没有终止</strong>的环境下</li>
<li>可以在不完整的序列上进行学习</li>
<li>TD可以在线学习，可以每走几步更新一次，而蒙特卡洛则需要游戏结束的时候，再进行更新。</li>
</ol>
<p>当步数趋近于无穷的时候，TD就会退化成MC。</p>
<img src="/2023/09/02/EasyRL/image-20230916141206633.png" alt="image-20230916141206633" style="zoom:50%;">

<p>这个G就是估计回报，也是TD-target。</p>
<p>另外，TD运用了MP性质，所以在MDP下有很好的效率，而MC没有运用MP性质，只是运用了采样，因此在马尔可夫环境下运用TD还是比较好的，但是非马尔可夫环境，可以考虑MC。</p>
<h2 id="免模型控制"><a href="#免模型控制" class="headerlink" title="免模型控制"></a>免模型控制</h2><p>在环境未知的时候（转移概率和奖励函数），无法使用策略&#x2F;价值迭代方法来改进策略（或者说是动态规划的方法）。结合蒙特卡洛&amp;时序差分，可以得到<strong>广义策略迭代</strong>（GPI）。</p>
<p>这里解释一下什么是<strong>探索性开始</strong>（exploring start）：以蒙塔卡洛为例，我们进行采样的时候，期望采样到的轨迹能够遍布整个空间，也就是尽可能采样到所有的轨迹（和大数定理是一个道理），这样我们取均值得到的回报就可以近似为当前状态的价值。这个条件就称为探索性开始。以下是原文中的一句话：<strong>探索性开始保证所有的状态和动作都在无限步的执行后能被采样到</strong>。</p>
<p>可以把Q函数看成一个Q表格，然后把采样得到的回报填入表格中，然后再依次选取不同的策略：Q表格大致如下</p>
<table>
<thead>
<tr>
<th align="center">-</th>
<th align="center">$\pi_1$</th>
<th align="center">$\pi_2$</th>
<th align="center">…</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$(s_1,a_1,…)$</td>
<td align="center">$G_{11}$</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr>
<td align="center">$(s_3,a_8,… )$</td>
<td align="center">$G_{12}$</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr>
<td align="center">E</td>
<td align="center">$Q_{\pi_1}$</td>
<td align="center">$Q_{\pi_2}$</td>
<td align="center">…</td>
</tr>
</tbody></table>
<p>这也正是下面这个公式的体现：<br>$$<br>\pi(s)&#x3D;arg max_aQ(s,a)<br>$$<br>注：arg max操作符：当取策略$\pi$的时候，能够使得动作价值函数Q取最大值。</p>
<p>$\epsilon$-贪心探索：有$1-\epsilon$的概率会按照Q函数来采取动作，另外的概率采取的是随机动作，确保足够的探索。</p>
<p>在最开始的时候，我们并不知道哪一个动作比较好，所以刚开始的$\epsilon$会比较大，随着时间的增大，$\epsilon$会慢慢变小。</p>
<h3 id="Sarsa：同策略时序差分控制"><a href="#Sarsa：同策略时序差分控制" class="headerlink" title="Sarsa：同策略时序差分控制"></a>Sarsa：同策略时序差分控制</h3><p>Sarsa算法跟简单，就是将原本的时序差分中的更新V的过程换成了更新Q的过程：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-18 20.10.06.png" alt="截屏2023-09-18 20.10.06" style="zoom:50%;">

<p>TD-traget，时序差分目标，意思就是我们想要逼近的那个目标值。我们这里使用的是one-step TD的思想，所以叫one-step Sarsa，只需知道$s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$即可。</p>
<p>给其加上$\lambda$得到了$Sarsa(\lambda)$，看不太懂。</p>
<h3 id="Q学习：异策略时序差分控制"><a href="#Q学习：异策略时序差分控制" class="headerlink" title="Q学习：异策略时序差分控制"></a>Q学习：异策略时序差分控制</h3><p>Sarsa其实还是用的时序差分的思想，直接通过上面的式子来更新策略。这样到最后Q收敛了，得到的是一个确定的策略（尽管我们可能是有一定的概率采取这个策略），这属于是<strong>同一个策略</strong>。</p>
<p>而Q学习有两个策略：</p>
<ul>
<li>$\pi$：目标策略，指的是我们需要去学习的策略。一般直接采取<strong>贪心策略</strong></li>
<li>$\mu$：行为策略，负责去探索环境，将采集到的轨迹投喂给$\pi$，并且采集到的数据不需要$a_{t+1}$。可以采取随机策略，但更好的选择是$\epsilon$-贪心策略</li>
</ul>
<img src="/2023/09/02/EasyRL/截屏2023-09-18 20.39.01.png" alt="截屏2023-09-18 20.39.01" style="zoom:50%;">

<h1 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h1><p>在model-free中，状态转移以及奖励都是由环境决定的，我们的agent可以和环境进行交互，但是能够改进的只有我们的策略本身。这一章中，我们可以就将智能体以及策略看成一个神经网络，然后输入我们的观测，得到动作作为输出。</p>
<p>假设参数为$\theta$的策略为$\pi _{\theta}$。</p>
<h2 id="策略梯度算法"><a href="#策略梯度算法" class="headerlink" title="策略梯度算法"></a>策略梯度算法</h2><p>强化学习的三个组成：</p>
<ul>
<li>演员（actor）</li>
<li>环境</li>
<li>奖励函数</li>
</ul>
<img src="/2023/09/02/EasyRL/截屏2023-09-20 12.37.58.png" alt="截屏2023-09-20 12.37.58" style="zoom:50%;">

<p>演员其实就是之前的agent，更进一步的说，其实就是我们的策略函数。将策略函数看成一个网络，网络中有很多参数，将这些参数计作$\theta$，可以对参数进行训练。</p>
<p>一般而言，我们并不会使用RNN来进行训练（尽管这可能更符合一般的逻辑，但那样会比较麻烦，RNN处理连续的图像可能会有相当大的计算量），反而使用的是类似CNN的网络，接受一张图片作为输入，有几个动作，就有几个输出神经元。（这可能并不适用于连续动作空间的问题）。</p>
<p>同理，我们可以讲环境也看作是一个神经网络，尽管它可能是一个rule-based的模型。环境接受actor的动作，然后输出state给actor，如此反复。知道到达环境的终止条件（例如打游戏时，所有的小怪都已经被消灭）。</p>
<p>在这个回合中，可以得到状态和动作组成的整个轨迹：<br>$$<br>\tau &#x3D; {s_1,a_1,s_2,a_2,…,s_t,a_t}<br>$$<br>在给定策略的参数$\theta$后，我们便可以计算出这条轨迹发生的概率了：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-20 12.54.33.png" alt="截屏2023-09-20 12.54.33" style="zoom: 50%;">

<p>上面算出的这条轨迹的概率取决于环境以及智能体采取的动作。</p>
<p>而环境是我们无法控制的，所以只能通过改变智能体的动作，也就是策略，来得到更好的轨迹。（长期奖励最大）</p>
<p>至于奖励函数，以游戏为例子，当agent采取一个动作后，环境输出一个状态，这时我们就可以得到一个奖励r了，r也取决于我们采取的动作a。待到游戏结束后，我们将所有的r加到一起，就得到了轨迹$\tau$的奖励：$R(\tau)$</p>
<p>在一个horizon结束之后，我们得到了轨迹的奖励，然后就可以来优化我们的策略参数$\theta$了。</p>
<p>另外，虽然说轨迹的奖励是一个标量值，但是其实其还是一个随机变量，因为轨迹的产生是取决于我们的动作的，是有一定的概率产生这个轨迹，于是，对其求期望：</p>
<img src="/2023/09/02/EasyRL/image-20230920130856256.png" alt="image-20230920130856256" style="zoom:50%;">

<p>我们进一步说明这个轨迹的概率是什么回事，举例：假设$\theta$对应的模型很强，能够做出很好的决策，那么若是有一回合agent很快就死掉了，那么这种概率是相当小的；相反，智能体能够很快的结束掉游戏，这是一种相当大的概率。</p>
<p>我们要最大化这个轨迹的期望奖励。选择使用<strong>梯度上升（gradient ascent）</strong>的方法。</p>
<p>我们对期望求微分：</p>
<img src="/2023/09/02/EasyRL/image-20230920140719345.png" alt="image-20230920140719345" style="zoom:50%;">

<p>然后利用这个性质：</p>
<img src="/2023/09/02/EasyRL/image-20230920140746494.png" alt="image-20230920140746494" style="zoom:50%;">

<p>得到：</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-20 14.08.17.png" alt="截屏2023-09-20 14.08.17" style="zoom:50%;">

<p>然后再做变换：</p>
<img src="/2023/09/02/EasyRL/image-20230920140913045.png" alt="image-20230920140913045" style="zoom:50%;">

<p>实际上上面这个最终的期望无法计算，所以我们选择使用采样的方式：</p>
<img src="/2023/09/02/EasyRL/image-20230920142307309.png" alt="image-20230920142307309" style="zoom:50%;">

<p>右半部分的计算方式：</p>
<img src="/2023/09/02/EasyRL/image-20230920142729662.png" alt="image-20230920142729662" style="zoom:50%;">

<p>更新梯度的方式为：</p>
<img src="/2023/09/02/EasyRL/image-20230920144424148.png" alt="image-20230920144424148" style="zoom:50%;">

<p>更新的意义是：假设我们在训练时，发现在状态$s_t$后执行动作$a_t$，然后得到的$\tau$的奖励是正的，那么我们就会修改$\pi _\theta$的参数，调高这个概率；反之，就会调低这个概率。</p>
<p>在训练过程中，智能体会不断和环境进行交互，得到很多的状态-动作对，然后拿来训练。训练完之后，这些数据将会被丢弃，智能体继续和环境进行交互。。。</p>
<p>上述以梯度上升的方法来更新策略参数，正是我们之前CNN中用的updater。</p>
<p>然后最后就是损失函数（目标函数）的确定了。损失函数用的是最小化交叉熵。</p>
<h2 id="蒙塔卡洛策略梯度"><a href="#蒙塔卡洛策略梯度" class="headerlink" title="蒙塔卡洛策略梯度"></a>蒙塔卡洛策略梯度</h2><p>也就是REINFORCE算法，REINFORCE算法用的是<strong>回合更新</strong>的方式（正如其名蒙特卡洛，每次更新单位是一个horizon）。</p>
<p>这个算法大致过程：在一个horizon内，获取每个步骤的reward，知道horizon结束。然后计算每个步骤的回报，带入到下面的公式中去，从而能够优化每一个动作。</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-20 20.45.10.png" alt="截屏2023-09-20 20.45.10" style="zoom:50%;">

<p>从代码上来讲，我们完成一个回合后，从最后一个动作开始，算出其回报，然后向前递归，得到前一个动作的回报，以此类推，知道$G_1$。</p>
<img src="/2023/09/02/EasyRL/截屏2023-09-20 20.48.00.png" alt="截屏2023-09-20 20.48.00" style="zoom:50%;">

<h1 id="近端策略优化"><a href="#近端策略优化" class="headerlink" title="近端策略优化"></a>近端策略优化</h1><h2 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h2><p>前言：上一章中提到的策略梯度上升，是对应的之前学习到的<strong>同策略</strong>。同策略有一个不好的点，当我们根据策略$\pi _{\theta}$采集到大量轨迹之后，对$\theta$进行更新之后，变成了$\theta ^{‘}$，这时之前采集到的数据就不管用了，我们需要重新采集数据。也就是说，使用<strong>同策略</strong>的方法的时候，大部分时间我们是在采集大量的、不可复用的数据。</p>
<p>假设是<strong>异策略</strong>，有一个策略$\theta$负责更新，另一个策略$\theta ^{‘}$负责采集数据，和环境交互，那么就可以采集一次数据，多次进行更新。具体的方法就是<strong>重要性采样</strong>。</p>
<p>考虑这样的情况：我们需要从分布p采样x，然后求f(x)的期望，但是我们不能直接从分布p采样，只能从另一个分布q采样，于是可以做一下变换：</p>
<img src="/2023/09/02/EasyRL/image-20230921093351256.png" alt="image-20230921093351256" style="zoom:50%;">

<p>于是：</p>
<img src="/2023/09/02/EasyRL/image-20230921093418352.png" alt="image-20230921093418352" style="zoom:50%;">

<p>称$\frac{p(x)}{q(x)}$为<strong>重要性权重</strong>。</p>
<p>上述过程就是<strong>重要性采样</strong>。</p>
<p>但是q分布的选取还是有限制的：</p>
<ul>
<li>p和q的差距不能太大，具体表现在方差上</li>
</ul>
<p>首先给出方差公式：$Var[X]&#x3D;E[X^2]-(E[X]^2)$</p>
<p>然后看经过重要性采样两个分布的均值虽然相同，方差却有差异：</p>
<img src="/2023/09/02/EasyRL/image-20230921094228980.png" alt="image-20230921094228980" style="zoom:50%;">

<p>虽然我们是计算期望，并不计算方差。但是，假设q和p差距非常大，但是我们可以实现exploring start，也就是能够采样到所有的x，那么期望其实还是一样的；但是exploring start是理想情况，现实中我们可能只能采样到一部分，在这种方差较大的情况下，我们最终得到的期望可能差别也非常大。</p>
<p>![截屏2023-09-21 09.49.50](.&#x2F;EasyRL&#x2F;截屏2023-09-21 09.49.50.png)</p>
<p>如上图，按照原分布p来采样，得到的期望应该是负的；但若是按照q分布采样，在这种p、q差距非常大的情况下，假设采样次数少，全部采样到右边了，那么最终算出的期望就是正的了，这种问题想要解决只能通过增加采样次数来解决了，但是这是很麻烦的一件事。</p>
<p>回归到异策略中，$\theta$就是p，$\theta ^{‘}$就是q，于是：</p>
<img src="/2023/09/02/EasyRL/image-20230921095638011.png" alt="image-20230921095638011" style="zoom:50%;">

<p>在实际做策略梯度的时候，并不是对整个奖励都做一个加和，而是会单独算某一个状态动作对$(s_t,a_t)$，如下：</p>
<img src="/2023/09/02/EasyRL/image-20230921100436205.png" alt="image-20230921100436205" style="zoom:50%;">

<p>$A^{\theta}(s_t,a_t)$即是用累计奖励减去基线baseline，若是为正，就调高概率</p>
<p>经过重要性采样，换了分布，那么就在前面加上一个修正项$\frac{p_{\theta}(s_t,a_t)}{p_{\theta ^{‘}}(s_t,a_t)}$：</p>
<p><img src="/2023/09/02/EasyRL/image-20230921101622477.png" alt="image-20230921101622477"></p>
<p>上面用到了一个假设：假设两个策略的A是差不多的。</p>
<p>然后再做一个假设，两个策略看到$s_t$的概率是差不多的：</p>
<p><img src="/2023/09/02/EasyRL/image-20230921102938686.png" alt="image-20230921102938686"></p>
<p>参考一下下面的解释：</p>
<p><img src="/2023/09/02/EasyRL/image-20230921103152512.png" alt="image-20230921103152512"></p>
<p>最终要优化的目标函数：？</p>
<p><img src="/2023/09/02/EasyRL/image-20230921103602015.png" alt="image-20230921103602015"></p>
<h2 id="近端策略优化-1"><a href="#近端策略优化-1" class="headerlink" title="近端策略优化"></a>近端策略优化</h2><p>也称<strong>PPO</strong>，PPO要解决的就是两个分布相差太大的问题</p>
<p>我们在训练的时候，给目标函数加了一个约束，称KL<strong>散度</strong>，这一项用来衡量两个策略的相似程度，我们希望两个策略尽可能相似，有点像正则化。</p>
<p>PPO是同策略算法，因为其行为策略和目标策略很相似。尽管PPO用到了重要性采样。</p>
<p><img src="/2023/09/02/EasyRL/image-20230921104133435.png" alt="image-20230921104133435"></p>
<p>KL散度的输入是$\theta$和$\theta ^{‘}$，然后再代入一个状态s，得到两套动作作为输出，然后计算这两个动作的差距。我们计算的并不是参数之间的距离（<strong>参数距离</strong>），而是两个网络输出的结果，最后得到的行为上的差异（<strong>行为距离</strong>）</p>
<p>至于为什么不直接计算参数距离，是因为我们更关心的是行为距离。参数变化一点或者变化很多对行为的影响是不确定的。</p>
<p>PPO有两个变种如下：</p>
<ul>
<li>PPO惩罚</li>
<li>PPO裁剪</li>
</ul>
<h3 id="PPO-penalty"><a href="#PPO-penalty" class="headerlink" title="PPO- penalty"></a>PPO- penalty</h3><p>具体做法就是，每次更新时，用前一个时刻的演员$\theta ^k$去跟环境交互，然后得到数据，然后对当前的演员$\theta$可以进行多次更新，想方设法最大化目标函数。</p>
<p><img src="/2023/09/02/EasyRL/image-20230921105753969.png" alt="image-20230921105753969"></p>
<p>然后这个$\beta$的设计也是有规则的：</p>
<ol>
<li>首先设置一个可以接受的最大的KL散度，然后完成一次优化。然后用优化后的策略和之前的策略做一次KL散度，若是这个散度大于我们设置的max，那么代表惩罚力度不够，需要增大$\beta$</li>
<li>设置一个可以接受的最小的KL散度，完成一次优化。若是优化后的KL散度小于min，那么认为惩罚效果太强了，因此选择增大$\beta$</li>
</ol>
<h3 id="PPO-clip"><a href="#PPO-clip" class="headerlink" title="PPO-clip"></a>PPO-clip</h3><p>POP-clip算法的目标函数是：</p>
<p><img src="/2023/09/02/EasyRL/image-20230921131336961.png" alt="image-20230921131336961"></p>
<p>clip函数的意思是：</p>
<ul>
<li><p>若第一项小于第二项，输出第二项</p>
</li>
<li><p>若第一项大于第三项，输出第三项</p>
</li>
<li><p>若介于之间，则输出本身</p>
</li>
</ul>
<p><img src="/2023/09/02/EasyRL/image-20230921131538520.png" alt="image-20230921131538520"></p>
<p>最终得到的目标函数的图如下：</p>
<ul>
<li>绿线是$\frac{p_{\theta}}{p_{\theta ^k}}$</li>
<li>蓝线是clip</li>
<li>红线是最终的结果</li>
</ul>
<h1 id="深度Q网络"><a href="#深度Q网络" class="headerlink" title="深度Q网络"></a>深度Q网络</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/02/EasyRL/" data-id="cma2hyvwi0013z4nnf3ve8a37" data-title="EasyRL" class="article-share-link">Share</a>
      
      
      
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RL/" rel="tag">RL</a></li></ul>


    </footer>
  </div>
  
    
  <nav id="article-nav" class="wow fadeInUp">
    
      <div class="article-nav-link-wrap article-nav-link-left">
        
          
          
            <img data-src="/covers/IMG_2585.JPG" data-sizes="auto" alt="NovelADS" class="lazyload">
          
        
        <a href="/2023/10/07/NovelADS/"></a>
        <div class="article-nav-caption">Newer</div>
        <h3 class="article-nav-title">
          
            NovelADS
          
        </h3>
      </div>
    
    
    <div class="article-nav-link-wrap article-nav-link-right">
      
        
        
          <img data-src="/covers/IMG_2621.PNG" data-sizes="auto" alt="d2l" class="lazyload">
        
      
      <a href="/2023/07/31/d2l/"></a>
      <div class="article-nav-caption">Older</div>
      <h3 class="article-nav-title">
        
          d2l
        
      </h3>
    </div>
    
  </nav>


  
</article>






</section>
          
            <aside id="sidebar">
  <div class="sidebar-wrap wow fadeInRight wrap-sticky">
    
      <div class="sidebar-toc-sidebar"><div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class" >
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">强化学习概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96"><span class="toc-number">1.2.</span> <span class="toc-text">序列决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="toc-number">1.3.</span> <span class="toc-text">动作空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BB%84%E6%88%90%E6%88%90%E5%88%86"><span class="toc-number">1.4.</span> <span class="toc-text">智能体的组成成分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.1.</span> <span class="toc-text">策略函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.5.</span> <span class="toc-text">智能体的类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#policy-or-value-based"><span class="toc-number">1.5.1.</span> <span class="toc-text">policy or value based</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#model-based"><span class="toc-number">1.5.2.</span> <span class="toc-text">model based</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%A7%84%E5%88%92"><span class="toc-number">1.6.</span> <span class="toc-text">学习和规划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8"><span class="toc-number">1.7.</span> <span class="toc-text">探索和利用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-process"><span class="toc-number">2.1.</span> <span class="toc-text">Markov process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-property"><span class="toc-number">2.1.1.</span> <span class="toc-text">Markov property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-chain"><span class="toc-number">2.1.2.</span> <span class="toc-text">Markov chain</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-reward-process"><span class="toc-number">2.2.</span> <span class="toc-text">Markov reward process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#return-and-value-function"><span class="toc-number">2.2.1.</span> <span class="toc-text">return and value function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bellman-equation"><span class="toc-number">2.2.2.</span> <span class="toc-text">bellman equation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97MRP%E4%BB%B7%E5%80%BC%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.3.</span> <span class="toc-text">计算MRP价值的迭代算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-decision-process"><span class="toc-number">2.3.</span> <span class="toc-text">Markov decision process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#policy-in-MDP"><span class="toc-number">2.3.1.</span> <span class="toc-text">policy in MDP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDP%E5%92%8CMP-MRP%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.3.2.</span> <span class="toc-text">MDP和MP&#x2F;MRP的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDP%E7%9A%84value-function"><span class="toc-number">2.3.3.</span> <span class="toc-text">MDP的value function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bellman-expectation-equation"><span class="toc-number">2.3.4.</span> <span class="toc-text">bellman expectation equation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE"><span class="toc-number">2.3.5.</span> <span class="toc-text">备份图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="toc-number">2.3.6.</span> <span class="toc-text">策略评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B-%E6%8E%A7%E5%88%B6"><span class="toc-number">2.3.7.</span> <span class="toc-text">预测&amp;控制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.3.8.</span> <span class="toc-text">策略迭代</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.3.9.</span> <span class="toc-text">价值迭代</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">表格型方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#model-based-model-free"><span class="toc-number">3.1.</span> <span class="toc-text">model-based &amp; model-free</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q%E8%A1%A8%E6%A0%BC"><span class="toc-number">3.2.</span> <span class="toc-text">Q表格</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">3.3.</span> <span class="toc-text">免模型预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="toc-number">3.3.1.</span> <span class="toc-text">蒙特卡洛策略评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="toc-number">3.3.2.</span> <span class="toc-text">时序差分</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6"><span class="toc-number">3.4.</span> <span class="toc-text">免模型控制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sarsa%EF%BC%9A%E5%90%8C%E7%AD%96%E7%95%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="toc-number">3.4.1.</span> <span class="toc-text">Sarsa：同策略时序差分控制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%82%E7%AD%96%E7%95%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="toc-number">3.4.2.</span> <span class="toc-text">Q学习：异策略时序差分控制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.</span> <span class="toc-text">策略梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">策略梯度算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E5%A1%94%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.2.</span> <span class="toc-text">蒙塔卡洛策略梯度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">近端策略优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">5.1.</span> <span class="toc-text">重要性采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96-1"><span class="toc-number">5.2.</span> <span class="toc-text">近端策略优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PPO-penalty"><span class="toc-number">5.2.1.</span> <span class="toc-text">PPO- penalty</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PPO-clip"><span class="toc-number">5.2.2.</span> <span class="toc-text">PPO-clip</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">深度Q网络</span></a></li></ol>
      
  </div>
</div>
</div>
      <div class="sidebar-common-sidebar hidden"><div class="sidebar-author">
  <img data-src="/avatar/rabbit_1.jpg" data-sizes="auto" alt="chengyiqiu" class="lazyload">
  <div class="sidebar-author-name">chengyiqiu</div>
  <div class="sidebar-description"></div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>Posts</div>
    <div class="sidebar-state-number">65</div>
  </div>
  <div class="sidebar-state-category">
    <div>Categories</div>
    <div class="sidebar-state-number">13</div>
  </div>
  <div class="sidebar-state-tag">
    <div>Tags</div>
    <div class="sidebar-state-number">17</div>
  </div>
</div>
<div class="sidebar-social">
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/" aria-label="Home"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Home</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/archives" aria-label="Archives"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Archives</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/about" aria-label="About"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">About</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/friend" aria-label="Friend"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Friend</div>
    </div>
  
</div>
</div>
    
    
      <div class="sidebar-btn-wrapper" style="position:static">
        <div class="sidebar-toc-btn current"></div>
        <div class="sidebar-common-btn"></div>
      </div>
    
  </div>

  
</aside>

          
        </div>
        <footer id="footer" class="wow fadeInUp">
  <div style="width: 100%; overflow: hidden">
    <div class="footer-line"></div>
  </div>
  <div class="outer">
    <div id="footer-info" class="inner">
      
      <div>
        <span class="icon-copyright"></span>
        2020-2025
        <span class="footer-info-sep"></span>
        chengyiqiu
      </div>
      
        <div>
          Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
          Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
        </div>
      
      
        <div>
          <span class="icon-brush"></span>
          142.4k
          &nbsp;|&nbsp;
          <span class="icon-coffee"></span>
          09:40
        </div>
      
      
        <div>
          <span class="icon-eye"></span>
          <span id="busuanzi_container_site_pv">Number of visits&nbsp;<span id="busuanzi_value_site_pv"></span></span>
          &nbsp;|&nbsp;
          <span class="icon-user"></span>
          <span id="busuanzi_container_site_uv">Number of visitors&nbsp;<span id="busuanzi_value_site_uv"></span></span>
        </div>
      
    </div>
  </div>
</footer>

        <div class="sidebar-top">
          <img src="/images/taichi.png" height="50" width="50" />
          <div class="arrow-up"></div>
        </div>
        <div id="mask"></div>
      </div>
      <nav id="mobile-nav">
  <div class="sidebar-wrap">
    
      <div class="sidebar-toc-sidebar"><div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class" >
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">强化学习概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96"><span class="toc-number">1.2.</span> <span class="toc-text">序列决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="toc-number">1.3.</span> <span class="toc-text">动作空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BB%84%E6%88%90%E6%88%90%E5%88%86"><span class="toc-number">1.4.</span> <span class="toc-text">智能体的组成成分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.1.</span> <span class="toc-text">策略函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.5.</span> <span class="toc-text">智能体的类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#policy-or-value-based"><span class="toc-number">1.5.1.</span> <span class="toc-text">policy or value based</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#model-based"><span class="toc-number">1.5.2.</span> <span class="toc-text">model based</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%A7%84%E5%88%92"><span class="toc-number">1.6.</span> <span class="toc-text">学习和规划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8"><span class="toc-number">1.7.</span> <span class="toc-text">探索和利用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-process"><span class="toc-number">2.1.</span> <span class="toc-text">Markov process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-property"><span class="toc-number">2.1.1.</span> <span class="toc-text">Markov property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-chain"><span class="toc-number">2.1.2.</span> <span class="toc-text">Markov chain</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-reward-process"><span class="toc-number">2.2.</span> <span class="toc-text">Markov reward process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#return-and-value-function"><span class="toc-number">2.2.1.</span> <span class="toc-text">return and value function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bellman-equation"><span class="toc-number">2.2.2.</span> <span class="toc-text">bellman equation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97MRP%E4%BB%B7%E5%80%BC%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.3.</span> <span class="toc-text">计算MRP价值的迭代算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-decision-process"><span class="toc-number">2.3.</span> <span class="toc-text">Markov decision process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#policy-in-MDP"><span class="toc-number">2.3.1.</span> <span class="toc-text">policy in MDP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDP%E5%92%8CMP-MRP%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.3.2.</span> <span class="toc-text">MDP和MP&#x2F;MRP的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDP%E7%9A%84value-function"><span class="toc-number">2.3.3.</span> <span class="toc-text">MDP的value function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bellman-expectation-equation"><span class="toc-number">2.3.4.</span> <span class="toc-text">bellman expectation equation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE"><span class="toc-number">2.3.5.</span> <span class="toc-text">备份图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="toc-number">2.3.6.</span> <span class="toc-text">策略评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B-%E6%8E%A7%E5%88%B6"><span class="toc-number">2.3.7.</span> <span class="toc-text">预测&amp;控制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.3.8.</span> <span class="toc-text">策略迭代</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.3.9.</span> <span class="toc-text">价值迭代</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">表格型方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#model-based-model-free"><span class="toc-number">3.1.</span> <span class="toc-text">model-based &amp; model-free</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q%E8%A1%A8%E6%A0%BC"><span class="toc-number">3.2.</span> <span class="toc-text">Q表格</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">3.3.</span> <span class="toc-text">免模型预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="toc-number">3.3.1.</span> <span class="toc-text">蒙特卡洛策略评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="toc-number">3.3.2.</span> <span class="toc-text">时序差分</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6"><span class="toc-number">3.4.</span> <span class="toc-text">免模型控制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sarsa%EF%BC%9A%E5%90%8C%E7%AD%96%E7%95%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="toc-number">3.4.1.</span> <span class="toc-text">Sarsa：同策略时序差分控制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%82%E7%AD%96%E7%95%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="toc-number">3.4.2.</span> <span class="toc-text">Q学习：异策略时序差分控制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.</span> <span class="toc-text">策略梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">策略梯度算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E5%A1%94%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.2.</span> <span class="toc-text">蒙塔卡洛策略梯度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">近端策略优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">5.1.</span> <span class="toc-text">重要性采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96-1"><span class="toc-number">5.2.</span> <span class="toc-text">近端策略优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PPO-penalty"><span class="toc-number">5.2.1.</span> <span class="toc-text">PPO- penalty</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PPO-clip"><span class="toc-number">5.2.2.</span> <span class="toc-text">PPO-clip</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">深度Q网络</span></a></li></ol>
      
  </div>
</div>
</div>
      <div class="sidebar-common-sidebar hidden"><div class="sidebar-author">
  <img data-src="/avatar/rabbit_1.jpg" data-sizes="auto" alt="chengyiqiu" class="lazyload">
  <div class="sidebar-author-name">chengyiqiu</div>
  <div class="sidebar-description"></div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>Posts</div>
    <div class="sidebar-state-number">65</div>
  </div>
  <div class="sidebar-state-category">
    <div>Categories</div>
    <div class="sidebar-state-number">13</div>
  </div>
  <div class="sidebar-state-tag">
    <div>Tags</div>
    <div class="sidebar-state-number">17</div>
  </div>
</div>
<div class="sidebar-social">
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/" aria-label="Home"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Home</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/archives" aria-label="Archives"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Archives</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/about" aria-label="About"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">About</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/friend" aria-label="Friend"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Friend</div>
    </div>
  
</div>
</div>
    
  </div>
  
    <div class="sidebar-btn-wrapper">
      <div class="sidebar-toc-btn current"></div>
      <div class="sidebar-common-btn"></div>
    </div>
  
</nav>

    </div>
    <div class="site-search">
      <div class="reimu-popup popup">
        <div class="reimu-search">
          <span class="reimu-search-input-icon"></span>
          <div class="reimu-search-input" id="reimu-search-input"></div>
        </div>
        <div class="reimu-results">
          <div id="reimu-stats"></div>
          <div id="reimu-hits"></div>
          <div id="reimu-pagination" class="reimu-pagination"></div>
        </div>
        <span class="popup-btn-close"></span>
      </div>
    </div>
    
<script src="https://npm.webcache.cn/jquery@3.7.1/dist/jquery.min.js"></script>


<script src="https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js"></script>



  
<script src="https://npm.webcache.cn/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" defer></script>



  
<script src="https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js" async></script>






<script src="/js/pjax_script.js" data-pjax></script>

















  
<script src="https://npm.webcache.cn/mouse-firework@0.0.4/dist/index.umd.js"></script>

  <script>
    firework(JSON.parse('{"excludeElements":["a","button"],"particles":[{"shape":"circle","move":["emit"],"easing":"easeOutExpo","colors":["#ff5252","#ff7c7c","#ffafaf","#ffd0d0"],"number":20,"duration":[1200,1800],"shapeOptions":{"radius":[16,32],"alpha":[0.3,0.5]}},{"shape":"circle","move":["diffuse"],"easing":"easeOutExpo","colors":["#ff0000"],"number":1,"duration":[1200,1800],"shapeOptions":{"radius":20,"alpha":[0.2,0.5],"lineWidth":6}}]}'))
  </script>







<script src="/js/script.js"></script>



  <script>
    console.log(String.raw`%c 
 ______     ______     __     __    __     __  __    
/\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
\ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
 \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
  \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                  
`,'color: #ff5252;')
    console.log('%c Theme.Reimu v' + '0.1.2' + ' %c https://github.com/D-Sketon/hexo-theme-reimu ', 'color: white; background: #ff5252; padding:5px 0;', 'padding:4px;border:1px solid #ff5252;')
  </script>
  

  <!-- hexo injector body_end start -->
<script src="/js/insert_highlight.js" data-pjax></script>
<!-- hexo injector body_end end --></body>
  </html>

