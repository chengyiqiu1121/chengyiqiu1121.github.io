
  <!DOCTYPE html>
  <html lang="en"  >
  <head>
  <meta charset="utf-8">
  

  

  

  
  <script>
    window.icon_font = '4552607_ikzjpc9jicn';
  </script>
  
  
  <title>
    d2l |
    
    Hexo
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CUbuntu%20Mono:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" as="style" onload="this.onload&#x3D;null;this.rel&#x3D;&#39;stylesheet&#39;">
  
  
<link rel="stylesheet" href="/css/loader.css">

  <meta name="description" content="d2l 经典QA问题1.怎么根据输入空间，选择最优的深度或者宽度假设问题背景：128 input，2 output  首先尝试线性回归模型，128输入2输出，不要隐藏层。 然后试试mlp（多层感知机）：128 input -&gt; 128&#x2F;64&#x2F;32&#x2F;16&#x2F;8 hide -&gt; 2 output。然后在此基础上挑出比较好的隐藏层个数，比如128和8不行">
<meta property="og:type" content="article">
<meta property="og:title" content="d2l">
<meta property="og:url" content="http://example.com/2023/07/31/d2l/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="d2l 经典QA问题1.怎么根据输入空间，选择最优的深度或者宽度假设问题背景：128 input，2 output  首先尝试线性回归模型，128输入2输出，不要隐藏层。 然后试试mlp（多层感知机）：128 input -&gt; 128&#x2F;64&#x2F;32&#x2F;16&#x2F;8 hide -&gt; 2 output。然后在此基础上挑出比较好的隐藏层个数，比如128和8不行">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-31T03:07:15.000Z">
<meta property="article:modified_time" content="2023-10-27T07:58:34.000Z">
<meta property="article:author" content="chengyiqiu">
<meta property="article:tag" content="cnn">
<meta property="article:tag" content="rnn">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="https://npm.webcache.cn/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="https://npm.webcache.cn/katex@0.16.9/dist/katex.min.css">

  
  
  
  
<script src="https://npm.webcache.cn/pace-js@1.2.4/pace.min.js"></script>

  
    
<link rel="stylesheet" href="https://npm.webcache.cn/wowjs@1.1.3/css/libs/animate.css">

    
<script src="https://npm.webcache.cn/wowjs@1.1.3/dist/wow.min.js"></script>

    <script>
      new WOW({
        offset: 0,
        mobile: true,
        live: false
      }).init();
    </script>
  
  
    <script src="/sw.js"></script>
  
<meta name="generator" content="Hexo 7.2.0"></head>

  <body>
    
  <div id='loader'>
    <div class="loading-left-bg"></div>
    <div class="loading-right-bg"></div>
    <div class="spinner-box">
      <div class="loading-taichi">
        <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
          <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
          <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
          <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
        </svg>
      </div>
      <div class="loading-word">少女祈祷中...</div>
    </div>
  </div>
  </div>
  <script>
    var time = null;
    const startLoading = () => {
      time = Date.now();
      document.getElementById('loader').classList.remove("loading");
    }
    const endLoading = () => {
      if (!time) {
        document.body.style.overflow = 'auto';
        document.getElementById('loader').classList.add("loading");
      } else {
        if (Date.now() - time > 500) {
          time = null;
          document.body.style.overflow = 'auto';
          document.getElementById('loader').classList.add("loading");
        } else {
          setTimeout(endLoading, 500 - (Date.now() - time));
          time = null;
        }
      }
    }
    window.addEventListener('load', endLoading);
    document.getElementById('loader').addEventListener('click', endLoading);
  </script>


    <div id="container">
      <div id="wrap">
        <div id="header-nav">
  <nav id="main-nav">
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/">Home</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/archives">Archives</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/about">About</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <span class="main-nav-icon"></span>
        <a class="main-nav-link" href="/friend">Friend</a>
      </span>
    
    <a id="main-nav-toggle" class="nav-icon"></a>
  </nav>
  <nav id="sub-nav">
    
      <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
    
    
    
  </nav>
</div>
<header id="header">
  
    <img fetchpriority="high" src="/images/banner.jpg" alt="d2l">
  
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <div id="logo-wrap">
        
          
          
            <a href="/" id="logo">
              <h1>d2l</h1>
            </a>
          
        
      </div>
      
        
        <h2 id="subtitle-wrap">
          
        </h2>
      
    </div>
  </div>
</header>

        <div id="content" class="outer">
          
          <section id="main"><article id="post-d2l" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    <div class="article-meta">
      <div class="article-date wow slideInLeft">
  <a href="/2023/07/31/d2l/" class="article-date-link">
    <time datetime="2023-07-31T03:07:15.000Z" itemprop="datePublished">2023-07-31</time>
  </a>
</div>

      
  <div class="article-category wow slideInLeft">
    <a class="article-category-link" href="/categories/d2l/">d2l</a>
  </div>


    </div>
    <div class="hr-line"></div>
    

    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>d2l</p>
<h1 id="经典QA"><a href="#经典QA" class="headerlink" title="经典QA"></a>经典QA</h1><h2 id="问题1-怎么根据输入空间，选择最优的深度或者宽度"><a href="#问题1-怎么根据输入空间，选择最优的深度或者宽度" class="headerlink" title="问题1.怎么根据输入空间，选择最优的深度或者宽度"></a>问题1.怎么根据输入空间，选择最优的深度或者宽度</h2><p>假设问题背景：128 input，2 output</p>
<ol>
<li>首先尝试线性回归模型，128输入2输出，不要隐藏层。</li>
<li>然后试试mlp（多层感知机）：128 input -&gt; 128&#x2F;64&#x2F;32&#x2F;16&#x2F;8 hide -&gt; 2 output。然后在此基础上挑出比较好的隐藏层个数，比如128和8不行，然后64&#x2F;32&#x2F;16效果都还不错。</li>
<li>再试试加一个隐藏层，比如128 -&gt; 64 -&gt; 16 -&gt; 2，或者: 128 -&gt; 32 -&gt; 8 -&gt; 2。多跑几轮。</li>
</ol>
<p>多调几次就有经验了。</p>
<h2 id="问题2-k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数数据吗？"><a href="#问题2-k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数数据吗？" class="headerlink" title="问题2.k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数数据吗？"></a>问题2.k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数数据吗？</h2><p>有三种做法：</p>
<ul>
<li>先在train set上训练，然后再用valid set上进行k折交叉验证，来选去hyper parameter，最后再在train set上训练w和b。</li>
<li>在train set上训练后，使用valid set进行k折验证，然后直接选hyper parameter，不训练了，直接作测试。</li>
<li>最贵的一种做法：在k折验证后，得到k个模型，在测试一个样本的时候，k个模型每个模型都跑一遍结果，最后进行平均&#x2F;投票。</li>
</ul>
<p>PS：测试集我们可能拿不到label。</p>
<h2 id="问题3-老师说的神经网络是一种语言，意思是利用神经网络去对万事万物建模吧？就是指的它理论上能拟合所有函数？"><a href="#问题3-老师说的神经网络是一种语言，意思是利用神经网络去对万事万物建模吧？就是指的它理论上能拟合所有函数？" class="headerlink" title="问题3.老师说的神经网络是一种语言，意思是利用神经网络去对万事万物建模吧？就是指的它理论上能拟合所有函数？"></a>问题3.老师说的神经网络是一种语言，意思是利用神经网络去对万事万物建模吧？就是指的它理论上能拟合所有函数？</h2><p>理论上来讲，只有一个隐藏层的、神经元足够多的mlp，就能够拟合所有的函数。但是，训练不出来，因此有了下面几种（原话：我知道mlp能拟合你，但是mlp训练不出来，所以我要做一个好的结构，来帮助你训练）：</p>
<ul>
<li>cnn：假设数据是有空间信息的</li>
<li>rnn：假设数据是有时序信息的</li>
<li>…</li>
</ul>
<p>我们引入先验，增加偏好，因此有了新的模型，试图帮出mlp去训练。</p>
<p>很多时候，我们都是先有一个想法&#x2F;理由，然后就去做了，很多优秀的论文也是这样，刚开始有个想法，但大部分都是错的，只不过满满的做最后的效果还不错。</p>
<p>三种元素：</p>
<ul>
<li>艺术：无法解释，就是比较好</li>
<li>工程：能有一套详细的流程</li>
<li>科学：能够解释</li>
</ul>
<p>神经网络，我们希望他是科学，但做起来是工程，实际上其中百分之五十都是艺术。可能</p>
<p>可能若干年后有人能科学的解释dl，但是，蒸汽机发明之后，100年后才出现了热力学。</p>
<h2 id="问题4-如果训练是不平衡的，是否要先考虑测试集是否也是不平衡的，再去决定是否使用一个平衡的验证集？"><a href="#问题4-如果训练是不平衡的，是否要先考虑测试集是否也是不平衡的，再去决定是否使用一个平衡的验证集？" class="headerlink" title="问题4.如果训练是不平衡的，是否要先考虑测试集是否也是不平衡的，再去决定是否使用一个平衡的验证集？"></a>问题4.如果训练是不平衡的，是否要先考虑测试集是否也是不平衡的，再去决定是否使用一个平衡的验证集？</h2><p>周志华老师讲过，可以通过加权来使得正类负类平衡，比如银行卡贷款，10000人中，5个人没有还款。</p>
<p>在李沐老师这里听到了新的做法：</p>
<ul>
<li>如果采样的数据是独立同分布的，也就是说，现实世界中就是这样分布的，那么其实就直接训练就行了，做好90%的正类，另外10%的负类尽量做好</li>
<li>如果采样不是独立同分布，那么就需要加权了。</li>
</ul>
<p>这里其实也引入了人为的偏好，采样数据在真实世界中究竟是如何分布，这个很难去界定。</p>
<h2 id="问题5-老师，为什么对16位浮点影响严重？32位或者64位就好了吗？那就是说所有通过fp16加速或者减小模型的方法都存在容易梯度爆炸或者消失的风险？"><a href="#问题5-老师，为什么对16位浮点影响严重？32位或者64位就好了吗？那就是说所有通过fp16加速或者减小模型的方法都存在容易梯度爆炸或者消失的风险？" class="headerlink" title="问题5.老师，为什么对16位浮点影响严重？32位或者64位就好了吗？那就是说所有通过fp16加速或者减小模型的方法都存在容易梯度爆炸或者消失的风险？"></a>问题5.老师，为什么对16位浮点影响严重？32位或者64位就好了吗？那就是说所有通过fp16加速或者减小模型的方法都存在容易梯度爆炸或者消失的风险？</h2><p>芯片大小一般是固定的，但是一个64位的浮点单元的面积是16位浮点单元的面积的4倍，因此16位浮点运算的速度也比64位浮点运算的速度快4倍。</p>
<p>另一方面，16位浮点更容易发生上溢或者下溢，所以加速&#x2F;减小模型更容易出现梯度消失或者爆炸。</p>
<h2 id="问题6-这几个超参数得影响重要程度排序是怎样得，核大小，填充，步幅"><a href="#问题6-这几个超参数得影响重要程度排序是怎样得，核大小，填充，步幅" class="headerlink" title="问题6.这几个超参数得影响重要程度排序是怎样得，核大小，填充，步幅"></a>问题6.这几个超参数得影响重要程度排序是怎样得，核大小，填充，步幅</h2><p>卷积神经网络中有kernel_size、填充、步幅三个超参数，首先给出结论：<strong>核大小最重要</strong></p>
<p>整个CNN就是在训练核大小，所以这个是最重要的。</p>
<p>然后就是步幅和填充了。</p>
<ul>
<li>填充主要是为了让我们的输入和输出的形状保持一致</li>
<li>而步幅一般选1或者2，再往上就取决于模型的复杂度了。当步幅为1时，每次我们的输出会比输入小一个维度，也就是说是<strong>线性的</strong>；而当将我们的步幅调成2，相当于一次就把我们的输入砍了一半，也就是说是<strong>指数下降</strong>的。当我们的输入太大时，在某几层增大步幅合一使得我们的输出直接少一个量级。</li>
</ul>
<p>比如，在CNN中我们要做100层的网络，然后我们通过计算要除几次，最后就可以把这几个折半砍的层均匀的插在中间；其余的层都是通过填充使得输入和输出是一样的维度。</p>
<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p>pip清华源：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<p>使用ubuntu22.04后，添加源后，可以通过apt来安装具体版本的python了：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:deadsnakes/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install python3.8</span><br><span class="line">sudo apt install python3.8-distutils</span><br></pre></td></tr></table></figure>

<p>安装好后，可以通过下面的指令来找到安装后的位置：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">which</span> python3.8</span><br></pre></td></tr></table></figure>

<p>然后由于aarch64使用conda有问题，所以就换了一种虚拟环境方案，使用的是：virtualenv+virtualenvwrapper。</p>
<p>操作就三个</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">chengyiqiu@chengyiqiu:~/Envs$ <span class="built_in">which</span> python3.8</span><br><span class="line">/usr/bin/python3.8</span><br><span class="line">chengyiqiu@chengyiqiu:~/Envs$ virtualenv -p /usr/bin/python3.8 d2l</span><br><span class="line">created virtual environment CPython3.8.17.final.0-64 <span class="keyword">in</span> 1184ms</span><br><span class="line">  creator CPython3Posix(dest=/home/chengyiqiu/Envs/d2l, clear=False, no_vcs_ignore=False, global=False)</span><br><span class="line">  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/chengyiqiu/.local/share/virtualenv)</span><br><span class="line">    added seed packages: pip==23.2.1, setuptools==68.0.0, wheel==0.41.0</span><br><span class="line">  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator</span><br><span class="line">chengyiqiu@chengyiqiu:~/Envs$ <span class="built_in">ls</span></span><br><span class="line">d2l  py38</span><br><span class="line">chengyiqiu@chengyiqiu:~/Envs$ <span class="built_in">source</span> ./d2l/bin/activate</span><br><span class="line">(d2l) chengyiqiu@chengyiqiu:~/Envs$</span><br></pre></td></tr></table></figure>

<p>发现使用pip装包时会报错：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">p.linux-aarch64-cpython-38/psutil/_psutil_common.o</span><br><span class="line">      psutil/_psutil_common.c:9:10: fatal error: Python.h: No such file or directory</span><br><span class="line">          9 | <span class="comment">#include &lt;Python.h&gt;</span></span><br><span class="line">            |          ^~~~~~~~~~</span><br><span class="line">      compilation terminated.</span><br><span class="line">      psutil could not be installed from sources. Perhaps Python header files are not installed. Try running:</span><br><span class="line">        sudo apt-get install gcc python3-dev</span><br><span class="line">      error: <span class="built_in">command</span> <span class="string">&#x27;/usr/bin/aarch64-linux-gnu-gcc&#x27;</span> failed with <span class="built_in">exit</span> code 1</span><br><span class="line">      [end of output]</span><br><span class="line"></span><br><span class="line">  note: This error originates from a subprocess, and is likely not a problem with pip.</span><br><span class="line">  ERROR: Failed building wheel <span class="keyword">for</span> psutil</span><br><span class="line">Failed to build psutil</span><br><span class="line">ERROR: Could not build wheels <span class="keyword">for</span> psutil, <span class="built_in">which</span> is required to install pyproject.toml-based projects</span><br></pre></td></tr></table></figure>

<p>然后尝试安装：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3.8-dev</span><br></pre></td></tr></table></figure>

<p>成功了！</p>
<p>因此，当所有步骤为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:deadsnakes/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment"># 下次创建新环境直接从这儿开始</span></span><br><span class="line">sudo apt install python3.8</span><br><span class="line">sudo apt install python3.8-distutils</span><br><span class="line">sudo apt-get install python3.8-dev</span><br></pre></td></tr></table></figure>

<p>打包requirement.txt：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip list --format=freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure>



<h2 id="开swap"><a href="#开swap" class="headerlink" title="开swap"></a>开swap</h2><p>首先要得到在哪个盘上开swap，输入df，然后最后有&#x2F;的表示有充足的空间开swap：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">chengyiqiu@chengyiqiu:~/myswapfile$ <span class="built_in">df</span> -m</span><br><span class="line">Filesystem     1M-blocks  Used Available Use% Mounted on</span><br><span class="line">tmpfs                380     4       376   1% /run</span><br><span class="line">/dev/mmcblk0p2    116950 11578    100565  11% /</span><br><span class="line">tmpfs               1896     0      1896   0% /dev/shm</span><br><span class="line">tmpfs                  5     0         5   0% /run/lock</span><br><span class="line">/dev/mmcblk0p1       253   149       104  60% /boot/firmware</span><br><span class="line">tmpfs                380     1       380   1% /run/user/1000</span><br></pre></td></tr></table></figure>

<p>然后选择mmcblk0p2：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> myswapfile</span><br><span class="line"><span class="built_in">cd</span> myswapfile/</span><br><span class="line">sudo <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/mmcblk0p2 of=swapfile bs=1G count=4</span><br><span class="line">sudo <span class="built_in">chmod</span> 600 swapfile</span><br><span class="line">sudo mkswap swapfile</span><br><span class="line">sudo swapon swapfile</span><br><span class="line">free -m</span><br><span class="line"></span><br><span class="line">               total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            3790         494        1081           3        2213        3098</span><br><span class="line">Swap:           4095           0        4095</span><br></pre></td></tr></table></figure>

<p>在服务器上（2G memory）经历过一次pip torch的时候被kill掉，原因就是爆内存了（OOM），这时可以通过开swap来解决。</p>
<p>做毕设的时候使用的python的后端框架flask，在读取大文件csv数据集时，也是因为OOM导致python进程被莫名其妙的杀掉了，应该也可以采取开swap的方法。</p>
<h2 id="htop"><a href="#htop" class="headerlink" title="htop"></a>htop</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">以下是CPU度量指标的颜色编码。</span><br><span class="line"></span><br><span class="line">蓝色：显示低优先级进程使用的CPU的百分比。</span><br><span class="line"></span><br><span class="line">绿色：显示普通用户拥有的进程使用的CPU的百分比。</span><br><span class="line"></span><br><span class="line">红色：显示系统进程使用的CPU的百分比。</span><br><span class="line"></span><br><span class="line">青色：显示Steal时间使用的CPU的百分比。</span><br><span class="line"></span><br><span class="line">以下是内存度量指标的颜色编码。</span><br><span class="line"></span><br><span class="line">绿色：显示已使用内存的百分比。</span><br><span class="line"></span><br><span class="line">蓝色：显示已使用缓冲区的百分比。</span><br><span class="line"></span><br><span class="line">橙色：显示已使用缓存的百分比。</span><br><span class="line"></span><br><span class="line">以下是SWAP度量指标的颜色编码。</span><br><span class="line"></span><br><span class="line">红色：显示已使用SWAP内存的百分比。</span><br></pre></td></tr></table></figure>



<h1 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归严格来说是一种<strong>仿射变换（affine transformation）</strong>：将特征通过加权（乘以参数w）然后再进行线性变换，最后通过偏置项（b）进行平移。</p>
<p>对于一个特定的样本，线性回归可表示为：![截屏2023-07-31 20.43.26](.&#x2F;d2l&#x2F;截屏2023-07-31 20.43.26.png)</p>
<p>写成向量的形式就是（向量和向量之间的点积）：</p>
<p>![截屏2023-07-31 20.44.01](.&#x2F;d2l&#x2F;截屏2023-07-31 20.44.01.png)</p>
<p>dl中读入的数据集一般都是矩阵，很多样本的集合，所以可以直接写成矩阵的形式。那么就是写成向量和矩阵的乘法形式：</p>
<p>![截屏2023-07-31 20.45.18](.&#x2F;d2l&#x2F;截屏2023-07-31 20.45.18.png)</p>
<p>线性回归模型比较特殊，因为其有<strong>解析解</strong>，能通过数学的方法求出解的表达式：</p>
<p>![截屏2023-07-31 21.14.07](.&#x2F;d2l&#x2F;截屏2023-07-31 21.14.07.png)</p>
<p>这是一个模型的优化思路，但绝大部分模型没有解析解（就算有，通过数学来求得解析解可能可行，但计算量特别大）。于是我们在没有解析解的情况下，可以通过从大量数据中，使用<strong>梯度下降（gradient descent）</strong>，得到<strong>数值解</strong>。</p>
<p>从整个数据集中随机取出一个batch size大小的数据，然后求loss，最后再反向传播更新参数，过程如下：</p>
<ul>
<li>(1)初始化模型参数的值，如随机初始化;</li>
<li>(2)从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。对于平方损失和仿射变换，我们可以明确地写成如下形式:</li>
</ul>
<p>![截屏2023-07-31 21.18.59](.&#x2F;d2l&#x2F;截屏2023-07-31 21.18.59.png)</p>
<p>线性回归可以看作为单层神经网络，因为输入层不进行计算，整个网络的计算神经元只有一个，一层，所以称为单层神经网络。该网络的特征纬度为d，标签维度为1。</p>
<p>![截屏2023-07-31 23.38.10](.&#x2F;d2l&#x2F;截屏2023-07-31 23.38.10.png)</p>
<p>DL启发于神经学，但更多的灵感 源自数学、统计、计算机。</p>
<p>![截屏2023-07-31 23.44.44](.&#x2F;d2l&#x2F;截屏2023-07-31 23.44.44.png)</p>
<p>线性回归从0开始中，有一段代码是手动生成的迭代器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="built_in">print</span>(batch_indices) <span class="comment"># 从数据集中选取了一组batch size大小的index</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices] <span class="comment"># tensor可以采取这种读的方式，tensor里面套tensor类型的index</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>这种方式实现的迭代器能使用，但是由于是随机读取一个batch size大小的数据，会用到很多次随机读取内存，这样有悖于<strong>局部性原理</strong>，时间较长性能较差。后续用到的框架中的迭代器就没有这个问题，</p>
<p>下面使用的代码，是利用torch中的normal函数来生成原始参数，原始参数服从正态分布，均值为0标准差为0.01，可以指定数据的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">199</span>, <span class="number">222</span>, <span class="number">123</span>, <span class="number">101</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(w)</span><br></pre></td></tr></table></figure>

<p>线性回归的简洁实现中，有这么一段代码，里面有一个*，没见过：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train) <span class="comment"># shuffle洗纸牌，表示随机打乱</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure>

<p>这个<em><strong>不能去掉</strong>，他的意义是：</em><em>括号中的一个星号，表示对list解开入参，即把列表元素分别当作参数传入</em>*</p>
<p>下面是一个简单的例子，输出是10：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a, b, c, d</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b + c + d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(add(*arr))</span><br></pre></td></tr></table></figure>

<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回归可以用来预测连续值，而分类问题也可以转化为回归问题，下面的图就可以看出，分类问题就是多了几个输出的结点。</p>
<p>回归到具体分类问题上：一张照片四个像素，可能是猫、狗、鸡。也就是：4个特征，3个标签。四个特征好量化，可以直接写成向量形式；但是标签怎么转变为数据呢，很容易就能想到[1, 2, 3]的形式，但是这样编码是有递增的自然顺序，但是显示数据中的标签并没有自然顺序，因此，采用<strong>独热编码（one hot encoding）</strong>的形式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">猫：[1, 0, 0]</span><br><span class="line">狗：[0, 1, 0]</span><br><span class="line">鸡：[0, 0, 1]</span><br></pre></td></tr></table></figure>





<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>下面是一个4层的感知机，中间有三个隐藏层，神经元的个数分别为：5，3，2。特征维度为4，标签维度为2。</p>
<p>可以看到从输入到第一层隐藏层，是增加了1的，然后是逐层递减。也可以直接开始递减。</p>
<p>为什么要这样做，里面是有工程经验的：<strong>深度学习本质上是一个压缩信息量的过程（回顾周志华老师的机器学习的课程，信息量是在逐层递减），隐藏层有三层，神经元的个数在递减，代表着数据中的信息量也是在递减的，最终到输出层。</strong>我们可以选择：</p>
<ul>
<li>只用一个隐藏层，但神经元个数非常多，如128，256<strong>（宽度学习doge）</strong>。</li>
<li>像下图一样神经元逐渐递减，每层都学一点东西。</li>
</ul>
<p>但是层数一定不能太深，并且层与层之间的神经元个数也不能衰减太快，不然整个网络可能<strong>坍塌</strong>，丢失掉重要的信息。</p>
<p>![截屏2023-08-01 20.53.22](.&#x2F;d2l&#x2F;截屏2023-08-01 21.53.34.png)</p>
<p>至于最后为什么我们选择的是深度学习而不是宽度学习（浅度学习），原因就是<strong>浅度学习并不好做</strong>：</p>
<ul>
<li>从人的直观上来讲，很难一口吃个胖子，学东西都是慢慢学，所以我们的网络每层都学一点东西，最后模型收敛。</li>
<li>从计算机的角度，只用一个隐藏层就将所有信息学到，这样难度很大，很难计算出来，也很难去调度所有的神经元应该怎么学（毕竟是黑盒）。</li>
</ul>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数的存在使得我们的mlp不再是仿射变化套娃仿射变换，当我们使用非线性的激活函数时，我们的模型就不会退化为仿射变换了。</p>
<ul>
<li>只有隐藏层才会加激活函数，输出层不会加激活函数</li>
<li>箭头-&gt;神经元+激活函数，这属于一个隐藏层</li>
<li>箭头-&gt;输出结点，输入输出层</li>
</ul>
<p>我们的模型长这样：</p>
<p>![截屏2023-08-04 12.52.27](.&#x2F;d2l&#x2F;截屏2023-08-04 12.52.27.png)</p>
<p>从下面这样：</p>
<p>![截屏2023-08-04 12.51.36](.&#x2F;d2l&#x2F;截屏2023-08-04 12.51.36.png)</p>
<p>变成了：</p>
<p>![截屏2023-08-04 12.51.59](.&#x2F;d2l&#x2F;截屏2023-08-04 12.51.59.png)</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>激活函数的意义在于将线性变为非线性，钟爱<strong>ReLU函数</strong>的很大一个原因是：<strong>简单</strong>。像其他两个函数里面都要做至少一次指数运算。<strong>而在CPU上，做一次指数运算的时间可以做十万次乘法运算！代价非常昂贵！</strong>而GPU中有相应的运算单元，比CPU就好很多了。</p>
<p>ReLU的函数图如下：</p>
<p>![截屏2023-08-04 12.58.14](.&#x2F;d2l&#x2F;截屏2023-08-04 12.58.14.png)</p>
<p>可以看到ReLU在0点不可导，我们默认使用0点的倒数为0（其实介于0和1之间都可以，对于这种边界的不可导点）但是，现实中不会出现输入为0的情况，有一句谚语：<strong>如果微妙的边界条件很重要，我们很可能是在研究数学而非工程</strong>。</p>
<p>![截屏2023-08-04 13.01.52](.&#x2F;d2l&#x2F;截屏2023-08-04 13.01.52.png)</p>
<p>ReLU还有一些变体：即使参数是负的，某些信息仍然可以通过</p>
<p>![截屏2023-08-04 13.03.06](.&#x2F;d2l&#x2F;截屏2023-08-04 13.03.06.png)</p>
<h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>表达式：<br>$$<br>sigmoid(x)&#x3D;\frac{1}{1+e^{-x}}<br>$$<br>图：</p>
<p>![截屏2023-08-04 13.07.18](.&#x2F;d2l&#x2F;截屏2023-08-04 13.07.18.png)</p>
<p>![截屏2023-08-04 12.51.36](.&#x2F;d2l&#x2F;截屏2023-08-04 12.51.36-1125646.png)</p>
<p>sigmoid有一个特性是：求导之后刚好可以表达成正例<em>反例<br>$$<br>\frac{d}{dx}sigmoid(x)&#x3D;sigmoid(x)</em>(1-sigmoid(x))<br>$$<br>导数图像：</p>
<p>![截屏2023-08-04 13.12.16](.&#x2F;d2l&#x2F;截屏2023-08-04 13.12.16.png)</p>
<h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><p>表达式：<br>$$<br>tanh(x)&#x3D;\frac{1-e^{-2x}}{1+e^{-2x}}<br>$$<br>图：</p>
<p>![截屏2023-08-04 13.13.56](.&#x2F;d2l&#x2F;截屏2023-08-04 13.13.56.png)</p>
<p>导数：<br>$$<br>\frac{d}{dx}tanh(x)&#x3D;1-tanh^2(x)<br>$$</p>
<p>![截屏2023-08-04 13.14.59](.&#x2F;d2l&#x2F;截屏2023-08-04 13.14.59.png)</p>
<h2 id="模型选择、欠拟合、过拟合"><a href="#模型选择、欠拟合、过拟合" class="headerlink" title="模型选择、欠拟合、过拟合"></a>模型选择、欠拟合、过拟合</h2><p>用于对抗过拟合的技术叫做<strong>正则化</strong>。</p>
<h3 id="统计学习理论"><a href="#统计学习理论" class="headerlink" title="统计学习理论"></a>统计学习理论</h3><p>一般情况我们假设我们抽取的数据是服从独立同分布的，也就是说2、3样本存在的相关性并不比2、200000两个样本的相关性强（和索引、抽取顺序无关）。但是这种假设很容易就被推翻，很容易找到假设失效的情况。</p>
<p>如果我们根据从加州大学旧金山分校医学中心的患者数据训练死亡风险预测模型， 并将其应用于马萨诸塞州综合医院的患者数据，结果会怎么样？ 这两个数据的分布可能不完全一样<strong>（可能有空间相关）</strong>。 此外，抽样过程<strong>可能与时间有关</strong>。 比如当我们对微博的主题进行分类时， 新闻周期会使得正在讨论的话题产生时间依赖性，从而违反独立性假设。</p>
<h3 id="模型复杂性"><a href="#模型复杂性" class="headerlink" title="模型复杂性"></a>模型复杂性</h3><p> <strong>对于一个分类模型，VC等于一个最大的数据集的大小，不管如何给定标号（label），都存在一个模型来对它进行完美分类</strong></p>
<p>可以计算出线性分类器的VC维：</p>
<p>![截屏2023-08-04 15.12.36](.&#x2F;d2l&#x2F;截屏2023-08-04 15.12.36.png)</p>
<p>模型容量和数据大小的关系如下：</p>
<ul>
<li>模型容量很低（比较简单），但是数据集比较大时，其实是学不到什么东西的，所以是<strong>欠拟合</strong></li>
<li>模型容量很高（神经元多，复杂），但是数据集比较小的时候，就很容易把噪声学进去，也就是容易<strong>过拟合</strong>。</li>
</ul>
<p>一般第二种情况比较常见，也就是很容易过拟合：比如给一组线性的数据，然后放入三四层的感知机里面去训练，很容易就把噪声当作数据的规律了，导致过拟合。</p>
<p>![截屏2023-08-02 14.46.35](.&#x2F;d2l&#x2F;截屏2023-08-02 14.46.35.png)</p>
<p>模型容量还可以定义为：拟合各种函数的能力。</p>
<p>下面对于同样的数据集，左边的模型比较简单，因此只学到了线性；而右边的模型比较复杂，学成了一个非常复杂的函数。</p>
<p>但事实上，这些数据服从的是一个二次函数的分布，</p>
<p>因此，从数据集的大小上我们能够感受到数据的复杂与否，然后再选择合适的模型来训练。</p>
<p>![截屏2023-08-02 14.50.41](.&#x2F;d2l&#x2F;截屏2023-08-02 14.50.41.png)</p>
<p>![截屏2023-08-03 09.27.21](.&#x2F;d2l&#x2F;截屏2023-08-03 09.27.21.png)</p>
<p>最后是书中的经验，哪些因素会影响模型泛化：</p>
<ol>
<li>可调整参数的数量。<strong>当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合</strong>。</li>
<li>参数采用的值。<strong>当权重的取值范围较大时，模型可能更容易过拟合</strong>。</li>
<li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。<strong>PS：也就是说还是得根据数据来选择模型</strong></li>
</ol>
<h3 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h3><p>神经网络中的w和b是在train set上训练出来的，而神经网络层数、神经元个数则是我们指定的超参数（这些东西对我们的模型容量有影响）。调整超参数有助于我们得到更适合我们数据的模型。</p>
<p>但是train set是用来调整参数，test set是用来测试训练出来的模型，都不好直接拿来调整超参数。因此，在数据集上再次划分一块出来，作为验证集，用来调整超参数。</p>
<p>因此，将我们的数据分成三份， 除了训练和测试数据集之外，还增加一个<em>验证数据集</em>（validation dataset）， 也叫<em>验证集</em>（validation set）</p>
<h3 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h3><p>下面是原书中的观点，在现实训练中的过拟合和欠拟合。</p>
<p>当我们比较训练和验证误差时，我们要注意两种常见的情况。 首先，我们要注意这样的情况：训练误差和验证误差都很严重， 但它们之间仅有一点差距。 如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足）， 无法捕获试图学习的模式。 此外，由于我们的训练和验证误差之间的<em>泛化误差</em>很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为<em>欠拟合</em>（underfitting）。</p>
<p>另一方面，当我们的训练误差明显低于验证误差时要小心， 这表明严重的<em>过拟合</em>（overfitting）。 注意，<em>过拟合</em>并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p>
<h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3><p>一般使用L2范数来约束w，作为特征向量的惩罚。在原本的loss上，加上L2范数<br>$$<br>L(w,b)+\frac{\lambda}{2}||w||^2<br>$$<br>当$\lambda&#x3D;0$时，恢复成了原来的损失函数；当$\lambda&gt;$0时，就会对w进行惩罚：<br>$$<br>w&lt;-(1-n\lambda)w-\frac{\eta}{\beta}\sum_{\beta}x^{i}(w^Tx^{(i)}+b-y^{(i)})<br>$$<br>表现出来就是：如果w变大太大了，那么L2范数就会把往小了拉。</p>
<p>至于L1范数，二者有不同的用途：</p>
<ul>
<li>L2范数对权重向量的大分量施加了巨大的惩罚。因此学习算法倾向于“<strong>在大量特征上均匀分布权重的模型</strong>”。</li>
<li>L1范数会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零（<strong>特征选择</strong>）</li>
</ul>
<h2 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h2><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>我们还是从线性模型出发。当样本很少、特征很多的时候，我们多跑几轮epoch，很可能就会过拟合；相反，  当特征数很少，而样本足够多时，这时就并不容易过拟合了。但是其代价是：只会关注样本的某几个特征，而不会关注这些特征之间的联系。</p>
<p>而神经网络就相反了，神经网络并不关注单个的特征，而是关注特征与特征之间的联系，举个例子：“神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。”但是正是由于这个，当我们有过多的样本时，就很容哦过拟合（特征之间明明没有关系，但是模型会误认为噪声是某种特种之间的关系，尤其是当阶数变高时）</p>
<h3 id="稳健性"><a href="#稳健性" class="headerlink" title="稳健性"></a>稳健性</h3><p>一个好的模型应该具有以下特点：</p>
<ul>
<li>模型简单，换言之就是模型的维度不应该太高</li>
<li>最终的模型应该比较平滑，也就是说对于样本有一定噪声的情况下能给出同样的输出。</li>
</ul>
<p>上面两点可以归结成模型的稳定性。</p>
<p>暂退法可以增加模型的稳健性，从下面两个方面：</p>
<ul>
<li>在中间的隐藏层与层之间，加入噪声。这样从输入输出的角度来看，模型会比较平滑。</li>
<li>忽略隐藏层中的某些神经元节点</li>
</ul>
<p>![截屏2023-08-07 13.13.49](.&#x2F;d2l&#x2F;截屏2023-08-07 13.13.49.png)</p>
<p>如上，毕晓普的方法就是给隐藏层加入噪声；而标准的dropout则是丢弃某些神经元的结点，如下：</p>
<p>![截屏2023-08-07 13.15.35](.&#x2F;d2l&#x2F;截屏2023-08-07 13.15.35.png)</p>
<h2 id="前向传播、反向传播、计算图"><a href="#前向传播、反向传播、计算图" class="headerlink" title="前向传播、反向传播、计算图"></a>前向传播、反向传播、计算图</h2><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>定义：<strong>从输入层，到输出层，计算和存储神经网络中每层的结果</strong>。</p>
<p>首先是输入到第一个隐藏层：<br>$$<br>z&#x3D;w_1x \<br>h_1&#x3D;\phi(z)<br>$$<br>然后就是隐藏层到输出层<br>$$<br>o&#x3D;W_2h \<br>L&#x3D;l(o,y)<br>$$<br>再加上L2范数<br>$$<br>s&#x3D;\frac{\lambda}{2}(||W_1||^2+||W_2||^2) \<br>J&#x3D;L+s<br>$$<br>J是最后的目标函数</p>
<p>![截屏2023-08-07 15.04.26](.&#x2F;d2l&#x2F;截屏2023-08-07 15.04.26.png)</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播就是从结果开始，满满的向前计算偏导数，最后利用链式法则复合到一起，然后得出对参数的梯度。</p>
<p>模型稳定性和模型初始化</p>
<h2 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h2><h3 id="梯度爆炸-消失"><a href="#梯度爆炸-消失" class="headerlink" title="梯度爆炸&#x2F;消失"></a>梯度爆炸&#x2F;消失</h3><p>参数的初值选取，以及后续激活函数的选定，都对我们模型的训练会造成很大的影响：</p>
<ul>
<li>参数更新过大，矩阵累乘后会造成梯度爆炸</li>
<li>参数更新过小，会造成梯度消失</li>
</ul>
<p>还有激活函数，如sigmoid函数，它符合神经学的认知，但是真正放在训练中：</p>
<p>![截屏2023-08-07 19.47.37](.&#x2F;d2l&#x2F;截屏2023-08-07 19.47.37.png)</p>
<p>当输入过大的时候，sigmoid函数的导数基本为0了，整个的梯度就消失了。</p>
<p>所以现在领域内主要是选择ReLU函数的变体来作为激活函数。</p>
<h3 id="打破对称性"><a href="#打破对称性" class="headerlink" title="打破对称性"></a>打破对称性</h3><p>对称性指的是：一个网络中的神经元具有相同的输入、初值权重、激活函数，那么这两个神经元在后续的训练中会保持完全相同的行为，他们的输出完全相同。换句话来说，<strong>一个神经元和两个神经元没什么区别</strong>。</p>
<p>因此，我们在初始化参数的时候，一定要避免这种情况。</p>
<p>在初始化的时候选择让参数服从一个均值为0方差0.01的正态分布，这样在一定程度上能得到一组较好的参数。</p>
<p>在实际编码时，可以直接选择<strong>Xavier初始化</strong>。</p>
<h2 id="环境和分布偏移"><a href="#环境和分布偏移" class="headerlink" title="环境和分布偏移"></a>环境和分布偏移</h2><p>有一句很好的话：<strong>有时模型的部署本身就是扰乱数据分布的催化剂</strong></p>
<p>举个很贴切的例子：假设我们训练了一个贷款申请人违约风险模型，用来预测谁将偿还贷款或违约。 这个模型发现申请人的鞋子与违约风险相关（穿牛津鞋申请人会偿还，穿运动鞋申请人会违约）。 此后，这个模型可能倾向于向所有穿着牛津鞋的申请人发放贷款，并拒绝所有穿着运动鞋的申请人。<strong>不久，所有的申请者都会穿牛津鞋，而信用度却没有相应的提高。</strong></p>
<p>上面的例子中，很容易看到：当我们的模型部署后，竟然让数据的分布发生了变化。</p>
<h3 id="协变量偏移"><a href="#协变量偏移" class="headerlink" title="协变量偏移"></a>协变量偏移</h3><p>定义：虽然输入的分布可能随时间而改变， 但标签函数（即条件分布𝑃(𝑦∣𝐱)）没有改变。 统计学家称之为<em>协变量偏移</em>（covariate shift）</p>
<p>协变量指的就是特征。</p>
<p>有点抽象，拿一个具体问题举例：</p>
<p>![截屏2023-08-08 15.53.23](.&#x2F;d2l&#x2F;截屏2023-08-08 15.53.23.png)</p>
<p>训练集是真实的照片，而测试集是卡通照片，对于这种训练集没有这个feature，而测试集有这个feature，可以称为协变量偏移，如果没有办法来适应这个，那么显然，模型会失去应有的效果。</p>
<h3 id="标签偏移"><a href="#标签偏移" class="headerlink" title="标签偏移"></a>标签偏移</h3><p>标签偏移刚好和协变量偏移相反。</p>
<h3 id="概念偏移"><a href="#概念偏移" class="headerlink" title="概念偏移"></a>概念偏移</h3><p>定义：<strong>当标签的定义发生变化时，就会出现这种问题。</strong></p>
<p>听着很奇怪，但举个例子：精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。随着时间的推移，这些概念的定义可能会发生偏移。</p>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>上一章节所描述的感知机貌似能解决很多问题，例如处理表格类型的数据这种特征数目不是很多的数据集。</p>
<p>但是，假如将我们的输入换成一张1200w*1200w像素的图片，那么按照之前softmax回归的处理思路。将照片的像素平铺成一个一位向量，那么我们就有1万亿个feature，然后第一层隐藏层的神经元个数也为1万亿的话，那么第一层的w就会爆炸，这样训练需要大量的GPU资源，以及人力成本。</p>
<p>因此可以看到，MLP在对于这种图片的处理基本是没什么效果的，因此我们引入<strong>卷积神经网络CNN</strong>。</p>
<h2 id="从MLP到CNN"><a href="#从MLP到CNN" class="headerlink" title="从MLP到CNN"></a>从MLP到CNN</h2><p>卷积本来是在DSP中使用的，FFT中使用，但是由于卷积的两种特性，使得它被运用到DL中，并产生了CNN：</p>
<ul>
<li>平移不变性：<strong>不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应</strong></li>
<li>局部性：<strong>神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</strong></li>
</ul>
<h2 id="互相关运算"><a href="#互相关运算" class="headerlink" title="互相关运算"></a>互相关运算</h2><p>其实不应该叫卷积运算，学过DSP里面，卷积的的运算中前面写的是符号，而我们深度学习中，使用的卷积前面的W的索引是正着来的，而数学中的卷积应该是翻着来的，严格的来讲，DL里的卷积应该叫<strong>互相关运算</strong></p>
<p>![截屏2023-08-09 10.25.42](.&#x2F;d2l&#x2F;截屏2023-08-09 10.25.42.png)</p>
<p>假设输入是 $n_h<em>n_w$，然后kernel是$k_h</em>k_w$，那么输出就为：<br>$$<br>(n_h-k_h+1)\times(n_w-k_w+1)<br>$$<br>下面我们按照书上的内容，手动写一下卷积层，以及对核函数进行一个简单的学习</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * ((y_hat - y) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, X, Y, epoch, lr</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        Y_hat = net.forward(X)</span><br><span class="line">        l = loss(y_hat=Y_hat, y=Y)</span><br><span class="line">        net.zero_grad()</span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        net.weight.data[:] -= lr * net.weight.grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  <span class="comment"># @save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>), device=get_device())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>), device=get_device())</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]], device=get_device())</span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line">net = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>, device=get_device())</span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">train(net, X, Y, epoch, lr)</span><br><span class="line"><span class="built_in">print</span>(net.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[[[ <span class="number">0.9750</span>, -<span class="number">0.9750</span>]]]], device=<span class="string">&#x27;mps:0&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>上面的代码中，我们的net、X、Y、weight、bias都是放在Mac的GPU上的，实际训练也是使用的GPU。</p>
<h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>上面几节讲的CNN中，经过和核函数运算后，我们的输入不断减小，这时有两种情况：</p>
<ol>
<li>当我们的输入非常大时，这时可以将我们的网络做的很深，但是仅仅靠核函数带来的线性衰减，我们的信息压缩的非常慢。</li>
<li>当我们的输入维度很小时，我们靠着线性衰减，我们可能做不了几层，无法把我们的网络做深。</li>
</ol>
<p>这时候可以对我们的输入做一个填充，并且在和核函数进行运算时，调整我们的步幅。</p>
<h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>通常会将我们的填充设置为核函数的维度减一，比如我们是3*3的核函数，那么可以将我们的填充设置为2，这样的话，我们的输入和输出的维度就是一样的了。</p>
<p>下面的例子中的核函数是大小是2*2的，所以就算填充是1，我们的输入输出维度并没有保持一致。</p>
<p>通常情况我们的卷积核的大小会设置为奇数，好处是：保持空间维度的同时，我们可以在顶部和底部填充相同数量的行</p>
<h3 id="截屏2023-08-09-20-54-35-d2l-截屏2023-08-09-20-54-35-png-步幅"><a href="#截屏2023-08-09-20-54-35-d2l-截屏2023-08-09-20-54-35-png-步幅" class="headerlink" title="![截屏2023-08-09 20.54.35](.&#x2F;d2l&#x2F;截屏2023-08-09 20.54.35.png)步幅"></a>![截屏2023-08-09 20.54.35](.&#x2F;d2l&#x2F;截屏2023-08-09 20.54.35.png)步幅</h3><p>步幅就是当我们在计算卷积的时候，for loop遍历的步长。一般我们的步幅是1，这样我们的输入和输出之间的关系就是一个线性的衰减了，但为了解决我们刚开始说的输入过大的这种情况，我们可以将步幅设置为2，这样我们的输出相对于输入就会减半，直接从一个线性衰减变成了指数衰减。</p>
<p>![截屏2023-08-09 21.00.15](.&#x2F;d2l&#x2F;截屏2023-08-09 21.00.15.png)</p>
<h2 id="多输入输出通道"><a href="#多输入输出通道" class="headerlink" title="多输入输出通道"></a>多输入输出通道</h2><p>上一节中我们的输入就是一个二维的矩阵，或者说是三维张量，但是现实中我们输入的图片并只是二维张量，例如RGB，每个像素还有红黄蓝三个值可以调，这使得不同程度的红黄蓝比例能够表示各种复杂的颜色。我们说RGB图片有三个通道，当RGB图片作为输入时，我们的输入就是一个$3\times h\times w$的三维张量。</p>
<h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>当我们的原始输入是一个RGB图像时，我们的输入就有3个通道，肯定不能用一个卷积核来处理这三个通道，因此，我们需要三个卷积核，做三次互相关运算，然后最终加在一起。</p>
<p>![截屏2023-08-12 19.49.13](.&#x2F;d2l&#x2F;截屏2023-08-12 19.49.13.png)</p>
<p>上面第一个是第一个卷积层，输入是2通道，所以卷积核也是两个，最后的输出将两个通道的内容合并在了一起，因此输出只有一个通道。</p>
<p>PS：这里每个通道输入和核函数做互相关运算之后，所有通道的结果加在一起了，两个思考：</p>
<ul>
<li>每个通道输出的结果需不需要乘一个系数，然后再相加到一起？答案是不需要，因为系数可以融进核函数里面，效果是一样的，不需要多此一举</li>
<li>为什么非要用加法操作将每个通道的结果融合在一起？<ul>
<li>其一是好算，加法操作比乘法等操作耗时短，卷积层的训练速度加快，这是站在训练的角度。</li>
<li>其二是，每个通道的输出结果其实是一个模式（pattern），把这些模式加在一起能够得出更复杂的模式，<strong>至于乘法运算能不能也有很好的效果，就不得而知了</strong></li>
</ul>
</li>
</ul>
<h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>上面有了多输入通道，但是我们的输出还是只是一个二维张量，这样如果作为下一层的输入那么就只有1维了，通道数降低了。</p>
<p>现实中，通道数多的话，能够学习的模式也就多，所以我们希望输出的通道维数也尽可能不要减少，因为神经网络要做很多层可能，如何增加输出通道呢，也很简单：上面有两个卷积核，也就是两个通道，我们可以设置很多卷积核，比如设计100个卷积核，然后分层，每层2个卷积核，一共50层，整个的卷积核的维度就是：$50\times 2\times 2\times2$，前面这个50，代表的就是我们的输出通道数有50个。下面是某层有多个输出通道，卷积核$2\times3\times1\times1$</p>
<p>![截屏2023-08-12 20.13.27](.&#x2F;d2l&#x2F;截屏2023-08-12 20.13.27.png)</p>
<h3 id="1-times1-卷积核"><a href="#1-times1-卷积核" class="headerlink" title="$1\times1$卷积核"></a>$1\times1$卷积核</h3><p>上图就是$1\times1$的卷积核。这样的卷积核一次只能看一个像素，也就是说失去了其空间能力。</p>
<p>我们之前了解到，从MLP到CNN，就是增加了空间性，所以我们 可以认为：如果某层的卷积核都是$1\times1$的话，那么可视作全连接层，其作用就是融合特征。</p>
<h2 id="汇聚层"><a href="#汇聚层" class="headerlink" title="汇聚层"></a>汇聚层</h2><p>汇聚层的目的：</p>
<ul>
<li>模式融合</li>
<li>降低卷积层对空间信息的敏感性， 增加对平移不变性的支持</li>
</ul>
<p>我们的卷积层通常是$3\times3$的，，对空间（这里指的是几个像素的偏移）十分敏感，但是，我们希望达到的效果是：在右边能识别出照片里的一只猫，在左边（换个环境）也同样能识别出。</p>
<p>另外，在最后一层中，我们希望能够直接识别出图像中有一只猫，而不是一些局部的纹理、毛色等信息。所以，需要对学到的模式进行汇聚。</p>
<p>汇聚层有最大汇聚层（max）和平均 汇聚层，最大汇聚层如下；</p>
<p>![截屏2023-08-14 12.33.03](.&#x2F;d2l&#x2F;截屏2023-08-14 12.33.03.png)</p>
<p>这里的汇聚层的大小是$2\times2$的，可以理解成允许物体向右边偏移一个像素；若是换成$3\times3$的话，可以理解成允许左右偏移一个像素。</p>
<h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><p>![截屏2023-08-14 14.30.43](.&#x2F;d2l&#x2F;截屏2023-08-14 14.30.43.png)</p>
<p>上面是完整的架构，下面是网络的定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>用的是sigmoid激活函数，下面是输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.469, train acc 0.823, test acc 0.813</span><br><span class="line">26794.6 examples/sec on cuda:0</span><br></pre></td></tr></table></figure>

<p>然后将激活函数换成ReLU，结果是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.531, train acc 0.808, test acc 0.767</span><br><span class="line">27731.5 examples/sec on cuda:0</span><br></pre></td></tr></table></figure>

<p>好像差不太多，然后把学习率从1e-2上调到1e-1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.318, train acc 0.882, test acc 0.868</span><br><span class="line">27809.8 examples/sec on cuda:0</span><br></pre></td></tr></table></figure>

<p>满意离开。</p>
<h1 id="现代卷积神经网络"><a href="#现代卷积神经网络" class="headerlink" title="现代卷积神经网络"></a>现代卷积神经网络</h1><p>![](.&#x2F;d2l&#x2F;截屏2023-08-15 13.28.57.png)</p>
<p>上图横坐标是训练难度，纵坐标是精度。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>![截屏2023-08-14 20.16.24](.&#x2F;d2l&#x2F;截屏2023-08-14 20.16.24.png)</p>
<p>上图左为LeNet，右为AlexNet，，可以看到其实AlexNet大致上就是一个更大更肥的LeNet，有几个细节：</p>
<ul>
<li>激活函数从sigmoid换成了ReLU</li>
<li>输入变成3通道了</li>
<li>采用了Dropout</li>
<li>双GPU</li>
</ul>
<p>下面是网络的代码形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>AlexNet在当时的提升是非常大的，但是卷积层设计的非常奇怪，没有提供通用的模版，给人一种感觉“我这样设置网络效果就很好”。</p>
<p>于是出现了VGG。</p>
<p>![截屏2023-08-15 12.23.50](.&#x2F;d2l&#x2F;截屏2023-08-15 12.23.50.png)</p>
<p>与AlexNet不同的是，VGG用的卷积核都是$3\times3$的，而不是$11\times11$的，更小的窗口，带来更深的网络。（从结果上来讲，这样的设计，最后模型的精度确实高很多）。</p>
<p>最原始的VGG：</p>
<ul>
<li>一个VGG快包含若干个卷积层（$3\times3$的卷积核，padding为1保证输入输出size一样，汇聚层的汇聚窗口为$2\times2$，stride为2，这样使得整个VGG块的输入到输出减半）</li>
</ul>
<p>然后可以根据设备的计算量，选择使用几个VGG块。</p>
<h2 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h2><p>NiN块的出现在于使用$1\times1$的卷积层来替换最后的全连接层的作用。</p>
<p>全连接层相比于卷积层，有更多的参数个数，这带来的计算量开销和显存开销是非常大的。</p>
<p>![截屏2023-08-15 13.02.06](.&#x2F;d2l&#x2F;截屏2023-08-15 13.02.06.png)</p>
<p>所以NiN的想法就是去掉全连接层：</p>
<p>![截屏2023-08-15 13.03.11](.&#x2F;d2l&#x2F;截屏2023-08-15 13.03.11.png)</p>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>。。。用“忙里”砸出来的，暂时没什么好看的，其核心就是用下面的Inception块：</p>
<p>![截屏2023-08-15 17.15.54](.&#x2F;d2l&#x2F;截屏2023-08-15 17.15.54.png)</p>
<p>整个网络的结构如下：</p>
<p>![截屏2023-08-15 17.16.19](.&#x2F;d2l&#x2F;截屏2023-08-15 17.16.19.png)</p>
<p>后续有些改进也就是改进的Inception块，比如把$5\times5$的卷积核改成$3\times3$，然后把$3\times3$换成2个$1\times7$。总之就是条蚕调出来的，至于能不能用，为什么，没有很多的说法，复现也不是很容易。</p>
<h2 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h2><p>批量正则化，他的出现是试图解决这个问题：随着模型的变深，当我们进行backward更新梯度时，靠近输出端的w可以很快的更新，这也就导致上层可以很快的收敛；相反，靠近数据的输入端，由于梯度一直累乘累乘（链式法则求导），所以下面的梯度就更新的很慢，并且，对于不同的batch（这些数据是随机抽取的，可能有很大的不同，在数据分布上），上层的权重能够很快的适应，但是下层，靠近数据输入端的并不能很快适应。越往下，越需要一种方法来解决这个。</p>
<p>批量正则化在干这样一件事，往我们的数据（输入或者输出）中增加噪声，以此来控制我们模型的复杂性。</p>
<ul>
<li>在MLP中，将batch normalization层放在仿射变换和激活函数之间</li>
<li>在CNN中，将其放在卷积层和激活函数之间</li>
</ul>
<p>值得注意的是，batch normalization并不一定是正确的，仅仅是现在很多人使用，原因是：精度不一定提升，但是模型的收敛速度很快。</p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>整个神经网络其实就是在学习、找到一个合适的函数，或者说学习到一种模式，来解决问题。我们之前了解到的网络可能会出现下图中左边的那种情况：</p>
<p>![截屏2023-08-15 19.51.56](.&#x2F;d2l&#x2F;截屏2023-08-15 19.51.56.png)</p>
<p>训练过程中离最优解越来越远了。但我们理想的效果反而是右图。残差块可以解决这个：</p>
<p>![截屏2023-08-15 19.56.14](.&#x2F;d2l&#x2F;截屏2023-08-15 19.56.14.png)</p>
<p>可以看到相较于前面的网络，残差块加上了一个前向反馈，这样不管中间有没有学到东西，至少之前学到的x是能够保留的。</p>
<p>另外，残差块就是将乘法转换成了加法，在反向传播求导的过程中，能够加快训练速度。</p>
<h1 id="代码训练–MLP-CNN"><a href="#代码训练–MLP-CNN" class="headerlink" title="代码训练–MLP+CNN"></a>代码训练–MLP+CNN</h1><p>这里对前面学到的一些知识用代码复现一下，加深理解，避免遗忘。</p>
<h2 id="线性回归-1"><a href="#线性回归-1" class="headerlink" title="线性回归"></a>线性回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">true_w = torch.Tensor([<span class="number">2</span>, - <span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">10000</span>, <span class="built_in">len</span>(true_w)))</span><br><span class="line">Y = torch.matmul(X, true_w) + true_b  <span class="comment"># x是一个矩阵，w是一个列向量，最后乘出来得到一个列向量</span></span><br><span class="line">Y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, Y.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num = <span class="built_in">len</span>(features)</span><br><span class="line">    indexes = <span class="built_in">list</span>(<span class="built_in">range</span>(num))</span><br><span class="line">    random.shuffle(indexes)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num, batch_size):</span><br><span class="line">        batch_indexes = torch.tensor(indexes[i: <span class="built_in">min</span>((i + batch_size, num))])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indexes], labels[batch_indexes]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred_w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=true_w.shape, requires_grad=<span class="literal">True</span>)</span><br><span class="line">pred_b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">lr = <span class="number">1e-2</span></span><br><span class="line">epoch = <span class="number">5</span></span><br><span class="line">batch = <span class="number">30</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> data_iter(batch_size=batch, features=X, labels=Y):</span><br><span class="line">        l = <span class="number">0.5</span> * (torch.matmul(x, pred_w) + pred_b - y) ** <span class="number">2</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> [pred_w, pred_b]:</span><br><span class="line">                param -= lr * param.grad / batch</span><br><span class="line">                param.grad.zero_()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 每一个epoch结束后，测试所有数据的loss</span></span><br><span class="line">        train_l = <span class="number">0.5</span> * (torch.matmul(X, pred_w) + pred_b - Y) ** <span class="number">2</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line">        losses.append(<span class="built_in">float</span>(train_l.mean()))</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, epoch + <span class="number">1</span>), losses)</span><br><span class="line">plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>![截屏2023-08-17 13.23.07](.&#x2F;d2l&#x2F;截屏2023-08-17 13.23.07.png)</p>
<h2 id="softmax线性回归"><a href="#softmax线性回归" class="headerlink" title="softmax线性回归"></a>softmax线性回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> d2l.torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">0</span></span><br><span class="line">trans = transforms.ToTensor()  <span class="comment"># 以tensor的形式存下来</span></span><br><span class="line">dev = torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line">minist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;./data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=trans,  <span class="comment"># 将下下来的图片以tensor的形式读进内存</span></span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">minist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;./data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=trans,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">train_iter = data.DataLoader(minist_train, batch_size, num_workers=num_workers, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_iter = data.DataLoader(minist_test, batch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">1</span> * <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>, device=dev)</span><br><span class="line">b = torch.zeros(size=(num_outputs,), requires_grad=<span class="literal">True</span>, device=dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    fen_mu = X_exp.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / fen_mu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape(-<span class="number">1</span>, w.shape[<span class="number">0</span>]), w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">0</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)  <span class="comment"># 取每行的最大值作为预测概率</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y  <span class="comment"># 将y_hat转成y同样的数据类型，然后比较得到bool类型，分类问题都可以这样做</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        如果模型是继承自torch.nn.Module的话，那么会自动求梯度，设置成评估模式的意思就是不要求梯度了</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置累加器，需要累加的有：正确预测数，预测总数&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        x, y = x.to(dev), y.to(dev)</span><br><span class="line">        metric.add(accuracy(net(x), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">net, data_iter, loss, updater, lr, params</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.train()  <span class="comment"># 累计梯度</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        x, y = x.to(dev), y.to(dev)</span><br><span class="line">        y_hat = net(x)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            如果采用的是torch的Optimizer，那么下面这样处理</span></span><br><span class="line"><span class="string">            1. 梯度清零</span></span><br><span class="line"><span class="string">            2. 反向传播</span></span><br><span class="line"><span class="string">            3. 自动更新参数</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            metric.add(</span><br><span class="line">                <span class="built_in">float</span>(l) * <span class="built_in">len</span>(y),</span><br><span class="line">                accuracy(y_hat, y),</span><br><span class="line">                y.size().numel(),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="string">&quot;&quot;&quot;如果自己手写，那么l出来就是一个向量&quot;&quot;&quot;</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(lr, x.shape[<span class="number">0</span>], *params)</span><br><span class="line">            metric.add(</span><br><span class="line">                <span class="built_in">float</span>(l.<span class="built_in">sum</span>()),</span><br><span class="line">                accuracy(y_hat, y),</span><br><span class="line">                y.numel()</span><br><span class="line">            )</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :returns loss/n, acc/ n</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, loss, num_epoch, updater, lr, params</span>):</span><br><span class="line">    train_acc_list, train_loss_list, test_acc_list = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">        train_metrics = train_epoch(</span><br><span class="line">            net=net,</span><br><span class="line">            data_iter=train_iter,</span><br><span class="line">            loss=loss, updater=updater,</span><br><span class="line">            lr=lr,</span><br><span class="line">            params=params</span><br><span class="line">        )</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        train_acc_list.append(train_metrics[<span class="number">1</span>])</span><br><span class="line">        train_loss_list.append(train_metrics[<span class="number">0</span>])</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, rain loss: <span class="subst">&#123;train_metrics[<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span>, train acc: <span class="subst">&#123;train_metrics[<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> train_acc_list, train_loss_list, test_acc_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">lr, batch, *params</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在我们手动实现的优化器里面，已经将梯度每次清零了&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch</span><br><span class="line">            param.grad.zero_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epoch = <span class="number">100</span></span><br><span class="line">lr = <span class="number">1e-1</span></span><br><span class="line">train_acc_list, train_loss_list, test_acc_list = train(</span><br><span class="line">    net=net,</span><br><span class="line">    lr=lr,</span><br><span class="line">    num_epoch=epoch,</span><br><span class="line">    params=(w, b),</span><br><span class="line">    train_iter=train_iter,</span><br><span class="line">    test_iter=test_iter,</span><br><span class="line">    loss=cross_entropy,</span><br><span class="line">    updater=updater</span><br><span class="line">)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(epoch), train_acc_list, label=<span class="string">&quot;train acc&quot;</span>, color=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(epoch), train_loss_list, label=<span class="string">&quot;train loss&quot;</span>, color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(epoch), test_acc_list, label=<span class="string">&quot;test acc&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;train/evaluate&quot;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>![截屏2023-08-18 09.15.49](.&#x2F;d2l&#x2F;截屏2023-08-18 09.15.49.png)</p>
<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> d2l.torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">minist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    download=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    root=<span class="string">&quot;./data&quot;</span></span><br><span class="line">)</span><br><span class="line">minist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    download=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    root=<span class="string">&quot;./data&quot;</span></span><br><span class="line">)</span><br><span class="line">dev = torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(minist_train, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(minist_test, batch_size)</span><br><span class="line"></span><br><span class="line">num_input, num_hiddens, num_output = <span class="number">1</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">256</span>, <span class="number">10</span></span><br><span class="line">w1 = nn.Parameter(torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_input, num_hiddens), device=dev, requires_grad=<span class="literal">True</span>))</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>, device=dev))</span><br><span class="line">w2 = nn.Parameter(torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_output), device=dev, requires_grad=<span class="literal">True</span>))</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_output, requires_grad=<span class="literal">True</span>, device=dev))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(x, torch.zeros(x.shape, device=dev))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">x</span>):</span><br><span class="line">    x = x.reshape(-<span class="number">1</span>, num_input)</span><br><span class="line">    h = ReLU(x @ w1 + b1)</span><br><span class="line">    o = h @ w2 + b2</span><br><span class="line">    <span class="keyword">return</span> o</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">num_epoch, lr = <span class="number">20</span>, <span class="number">1e-2</span></span><br><span class="line">updater = torch.optim.SGD(params=(w1, b1, w2, b2), lr=lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">acc</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    tmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(tmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">net, data_iter, loss, updater</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    accumulator = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        x, y = x.to(dev), y.to(dev)</span><br><span class="line">        y_hat = net(x)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            accumulator.add(</span><br><span class="line">                <span class="built_in">float</span>(l.<span class="built_in">sum</span>()),</span><br><span class="line">                acc(y_hat, y),</span><br><span class="line">                y.size().numel()</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;hahaha&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> accumulator[<span class="number">0</span>] / accumulator[<span class="number">2</span>], accumulator[<span class="number">1</span>] / accumulator[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loss_list, train_acc_list, test_acc_list, test_loss_list = [], [], [], []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, loss, num_epoch, updater</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">        train_loss, train_acc = train_epoch(net, train_iter, loss, updater)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">            net.<span class="built_in">eval</span>()</span><br><span class="line">        ctr = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> test_iter:</span><br><span class="line">            x, y = x.to(dev), y.to(dev)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            ctr.add(</span><br><span class="line">                <span class="built_in">float</span>(loss(y_hat, y).<span class="built_in">sum</span>()),</span><br><span class="line">                acc(y_hat, y),</span><br><span class="line">                y.numel()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        test_acc = ctr[<span class="number">1</span>] / ctr[<span class="number">2</span>]</span><br><span class="line">        test_loss = ctr[<span class="number">0</span>] / ctr[<span class="number">2</span>]</span><br><span class="line">        train_loss_list.append(train_loss)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        test_loss_list.append(test_loss)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;epoch: <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>, test loss: <span class="subst">&#123;test_loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train(net, train_iter, test_iter, loss, num_epoch, updater)</span><br><span class="line"><span class="comment"># plt.plot(range(num_epoch), train_acc_list, label=&quot;train acc&quot;, color=&quot;red&quot;)</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(num_epoch), train_loss_list, label=<span class="string">&quot;train loss&quot;</span>, color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line"><span class="comment"># plt.plot(range(num_epoch), test_acc_list, label=&quot;test acc&quot;, color=&quot;blue&quot;)</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(num_epoch), test_loss_list, label=<span class="string">&quot;test loss&quot;</span>, color=<span class="string">&quot;pink&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;train/evaluate&quot;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>![截屏2023-08-18 10.46.47](.&#x2F;d2l&#x2F;截屏2023-08-18 10.46.47.png)</p>
<p>跑了20轮，看曲线的走势，test loss快要上升了，可能快过拟合了。</p>
<p>然后试试m1pro的mps和cuda哪个快点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line">train(net, train_iter, test_iter, loss, num_epoch, updater)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;time: <span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&quot;</span>)</span><br><span class="line"></span><br><span class="line">m1pro: time: <span class="number">47.91</span> sec</span><br><span class="line">mx150: 模型太大了，直接爆显存。。。</span><br></pre></td></tr></table></figure>

<p>跑了100轮，看会不会overfitting：</p>
<p>![截屏2023-08-18 11.18.28](.&#x2F;d2l&#x2F;截屏2023-08-18 11.18.28.png)</p>
<p>貌似也并没有，只是越到后面，梯度越小了，近乎消失了。看样子训练20轮是比较好的了就，从20到100，test acc上升了3个点。</p>
<h2 id="LeNet-1"><a href="#LeNet-1" class="headerlink" title="LeNet"></a>LeNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> d2l.torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line">batch = <span class="number">512</span></span><br><span class="line">minist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                                 download=<span class="literal">False</span>)</span><br><span class="line">minist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                                download=<span class="literal">False</span>)</span><br><span class="line">train_iter = data.DataLoader(minist_train, batch_size=batch, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_iter = data.DataLoader(minist_test, batch_size=batch)</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">acc</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    tmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(tmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loss_list, test_loss_list = [], []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, epoch, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    updater = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    accumulator_train = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                accumulator_train.add(</span><br><span class="line">                    l * x.shape[<span class="number">0</span>],</span><br><span class="line">                    acc(y_hat, y),</span><br><span class="line">                    x.shape[<span class="number">0</span>]</span><br><span class="line">                )</span><br><span class="line">        train_acc = accumulator_train[<span class="number">1</span>] / accumulator_train[<span class="number">2</span>]</span><br><span class="line">        train_loss = accumulator_train[<span class="number">0</span>] / accumulator_train[<span class="number">2</span>]</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        accumulator_test = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> test_iter:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            accumulator_test.add(</span><br><span class="line">                l * x.shape[<span class="number">0</span>],</span><br><span class="line">                acc(y_hat, y),</span><br><span class="line">                x.shape[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line">        test_acc = accumulator_test[<span class="number">1</span>] / accumulator_test[<span class="number">2</span>]</span><br><span class="line">        test_loss = accumulator_test[<span class="number">0</span>] / accumulator_test[<span class="number">2</span>]</span><br><span class="line">        test_loss_list.append(test_loss)</span><br><span class="line">        train_loss_list.append(train_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, test_acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>, test_loss: <span class="subst">&#123;test_loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr, epoch = <span class="number">1e-2</span>, <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train(net, train_iter, test_iter, epoch, lr, device)</span><br><span class="line">    <span class="comment"># plt.plot(range(num_epoch), train_acc_list, label=&quot;train acc&quot;, color=&quot;red&quot;)</span></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epoch), train_loss_list, label=<span class="string">&quot;train loss&quot;</span>, color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">    <span class="comment"># plt.plot(range(num_epoch), test_acc_list, label=&quot;test acc&quot;, color=&quot;blue&quot;)</span></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epoch), test_loss_list, label=<span class="string">&quot;test loss&quot;</span>, color=<span class="string">&quot;pink&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;train/evaluate&quot;</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>![截屏2023-08-18 13.14.40](.&#x2F;d2l&#x2F;截屏2023-08-18 13.14.40.png)</p>
<p>貌似有点过拟合了，试着加上L2权重衰减：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">updater = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> net.parameters():</span><br><span class="line">    l += wd * <span class="number">0.5</span> * p.<span class="built_in">pow</span>(<span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>![截屏2023-08-18 13.27.50](.&#x2F;d2l&#x2F;截屏2023-08-18 13.27.50.png)</p>
<p>还是很抖动，降低学习率、增大batch试试。</p>
<p>![截屏2023-08-18 13.32.46](.&#x2F;d2l&#x2F;截屏2023-08-18 13.32.46.png)</p>
<p>曲线确实平滑了点。</p>
<p>然后lr调到4e-3</p>
<p>![截屏2023-08-18 13.39.03](.&#x2F;d2l&#x2F;截屏2023-08-18 13.39.03.png)</p>
<p>学习率1e-3</p>
<p>![截屏2023-08-18 13.45.47](.&#x2F;d2l&#x2F;截屏2023-08-18 13.45.47.png)</p>
<h2 id="AlexNet-1"><a href="#AlexNet-1" class="headerlink" title="AlexNet"></a>AlexNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> d2l.torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">batch = <span class="number">256</span></span><br><span class="line">trans = [transforms.ToTensor()]</span><br><span class="line">trans.insert(<span class="number">0</span>, transforms.Resize(<span class="number">224</span>))</span><br><span class="line">trans = transforms.Compose(trans)</span><br><span class="line">train_iter = data.DataLoader(</span><br><span class="line">    torchvision.datasets.FashionMNIST(<span class="string">&quot;./data&quot;</span>, <span class="literal">True</span>, transform=trans),</span><br><span class="line">    batch, <span class="literal">True</span>)</span><br><span class="line">test_iter = data.DataLoader(</span><br><span class="line">    torchvision.datasets.FashionMNIST(<span class="string">&quot;./data&quot;</span>, <span class="literal">False</span>, transform=trans),</span><br><span class="line">    batch, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, epoch, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    updater = torch.optim.SGD(params=net.parameters(), lr=lr)</span><br><span class="line">    accumulator_train = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    train_acc_l, train_loss_l, test_acc_l, test_loss_l = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            x = x.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                    y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">                tmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">                tmp = tmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>()</span><br><span class="line">                accumulator_train.add(</span><br><span class="line">                    tmp,</span><br><span class="line">                    <span class="built_in">float</span>(l) * x.shape[<span class="number">0</span>],</span><br><span class="line">                    x.shape[<span class="number">0</span>]</span><br><span class="line">                )</span><br><span class="line">        train_acc = accumulator_train[<span class="number">0</span>] / accumulator_train[<span class="number">2</span>]</span><br><span class="line">        train_loss = accumulator_train[<span class="number">1</span>] / accumulator_train[<span class="number">2</span>]</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        accumulator_test = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> test_iter:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">            tmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">            tmp = tmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>()</span><br><span class="line">            accumulator_test.add(</span><br><span class="line">                <span class="built_in">float</span>(tmp),</span><br><span class="line">                <span class="built_in">float</span>(l) * x.shape[<span class="number">0</span>],</span><br><span class="line">                x.shape[<span class="number">0</span>],</span><br><span class="line">            )</span><br><span class="line">        test_acc = accumulator_test[<span class="number">0</span>] / accumulator_test[<span class="number">2</span>]</span><br><span class="line">        test_loss = accumulator_test[<span class="number">1</span>] / accumulator_test[<span class="number">2</span>]</span><br><span class="line">        train_acc_l.append(train_acc)</span><br><span class="line">        train_loss_l.append(train_loss)</span><br><span class="line">        test_acc_l.append(test_acc)</span><br><span class="line">        test_loss_l.append(test_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, test_acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>, test_loss: <span class="subst">&#123;test_loss:<span class="number">.2</span>f&#125;</span>, train_acc:<span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, &quot;</span></span><br><span class="line">              <span class="string">f&quot;train_loss: <span class="subst">&#123;train_loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> train_acc_l, train_loss_l, test_acc_l, test_loss_l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t = d2l.Timer()</span><br><span class="line">    train_acc_l, train_loss_l, test_acc_l, test_loss_l = train(net, train_iter, test_iter, epoch, lr, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;time: <span class="subst">&#123;t.stop():<span class="number">.2</span>f&#125;</span> sec&quot;</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epoch), train_acc_l, label=<span class="string">&quot;train acc&quot;</span>, color=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epoch), train_loss_l, label=<span class="string">&quot;train loss&quot;</span>, color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epoch), test_acc_l, label=<span class="string">&quot;test acc&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epoch), test_loss_l, label=<span class="string">&quot;test loss&quot;</span>, color=<span class="string">&quot;pink&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;train/evaluate&quot;</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>跑了有点久，这大概就是本地机器的极限了，AlexNet</p>
<h2 id="ResNet-1"><a href="#ResNet-1" class="headerlink" title="ResNet"></a>ResNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[9]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> d2l.torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 读取数据集</span></span><br><span class="line"><span class="comment"># 读取Fashion minist数据集，将分辨率Resize得更大点，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># resize 图片</span></span><br><span class="line">trans = [torchvision.transforms.ToTensor()]</span><br><span class="line">trans.insert(<span class="number">0</span>, torchvision.transforms.Resize(<span class="number">128</span>))</span><br><span class="line">trans = torchvision.transforms.Compose(trans)</span><br><span class="line"></span><br><span class="line">minist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=trans)</span><br><span class="line">minist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=trans)</span><br><span class="line"></span><br><span class="line">batch = <span class="number">256</span></span><br><span class="line">train_iter = data.DataLoader(dataset=minist_train, batch_size=batch, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_iter = data.DataLoader(dataset=minist_test, batch_size=batch, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(x.shape, y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 构造Residual模块</span></span><br><span class="line"><span class="comment"># 一个residual模块应该包含下面几个部分：</span></span><br><span class="line"><span class="comment"># - 3*3 conv</span></span><br><span class="line"><span class="comment"># - bn</span></span><br><span class="line"><span class="comment"># - relu</span></span><br><span class="line"><span class="comment"># - 3*3 conv</span></span><br><span class="line"><span class="comment"># - bn</span></span><br><span class="line"><span class="comment"># - 输出部分和x同时作为加法器的输入，x可以通过1*1的conv变换通道</span></span><br><span class="line"><span class="comment"># - relu</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[14]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, output_channels, use_1_1conv=<span class="literal">False</span>, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1_1conv:  <span class="comment">#用1*1的卷积核给X做变换</span></span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=<span class="number">1</span>,</span><br><span class="line">                                   stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(output_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(output_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>))</span><br><span class="line">t(X).shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 构造ResNet块</span></span><br><span class="line"><span class="comment"># 有了residual模块，我们就可以构造自己的resnet block了，可以在其中添加若干个residual</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[39]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">in_channel, out_channel, num_residual, is_first=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residual):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> is_first == <span class="literal">False</span>:</span><br><span class="line">            blk.append(</span><br><span class="line">                Residual(in_channel, out_channel, use_1_1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(out_channel, out_channel))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 构造ResNet</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[31]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">kernel=3, padding=1</span></span><br><span class="line"><span class="string">kernel=7, padding=3</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">都不会改变矩阵的形状，经过该网络后，是stride把高宽减半了</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">1</span>, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">b1(X).shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[41]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, is_first=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[42]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, b2, b3, b4, b5,</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[46]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;i&#125;</span> layer input shape: <span class="subst">&#123;X.shape&#125;</span>&quot;</span>)</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">net.parameters()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 开始训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[71]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">acc</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回预测正确的类数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    res = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    res = res.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(res)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def init_weight(m):</span></span><br><span class="line">    <span class="comment">#     nn.init.xavier_uniform_(m.weight)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TTList</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num</span>):</span><br><span class="line">        self.ttl = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">            self.ttl.append([])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, elements</span>):</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> elements:</span><br><span class="line">            self.ttl[i].append(e)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_iter, test_iter, net, lr, epoch, device</span>):</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    ttl = TTList(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        net.train()</span><br><span class="line">        accumulator_train = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            x = x.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                accumulator_train.add(acc(y_hat, y), <span class="built_in">float</span>(l) * x.shape[<span class="number">0</span>], x.shape[<span class="number">0</span>])</span><br><span class="line">        train_acc = accumulator_train[<span class="number">0</span>] / accumulator_train[<span class="number">2</span>]</span><br><span class="line">        train_loss = accumulator_train[<span class="number">1</span>] / accumulator_train[<span class="number">2</span>]</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        accumulator_test = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> test_iter:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            y_hat = net(x)</span><br><span class="line">            accumulator_test.add(acc(y_hat, y), <span class="built_in">float</span>(loss(y_hat, y)) * x.shape[<span class="number">0</span>], x.shape[<span class="number">0</span>])</span><br><span class="line">        test_acc = accumulator_test[<span class="number">0</span>] / accumulator_test[<span class="number">2</span>]</span><br><span class="line">        test_loss = accumulator_test[<span class="number">1</span>] / accumulator_test[<span class="number">2</span>]</span><br><span class="line">        ttl.push([train_acc, train_loss, test_acc, test_loss])</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>\ntrain acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, train loss: <span class="subst">&#123;train_loss:<span class="number">.2</span>f&#125;</span>\ntest acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>, test_loss: <span class="subst">&#123;test_loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> ttl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[72]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch = <span class="number">256</span></span><br><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line">train(train_iter, test_iter, net, lr, epoch, device)</span><br></pre></td></tr></table></figure>

<p>跑了一轮就有很好的效果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1</span><br><span class="line">train acc: 0.81, train loss: 0.58</span><br><span class="line">test acc: 0.81, test_loss: 0.50</span><br></pre></td></tr></table></figure>

<p>CV首选！</p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>之前从MLP学到CNN，CNN的特点是很适合处理空间信息，能够捕捉到像素级别的特征。对于CNN的数据集，我们假设数据是服从独立同分布的，但是，现实生活中很多东西并不都是独立的：</p>
<ul>
<li>电影中的视频帧</li>
<li>一篇作文的每一个字</li>
<li>人的脑信号</li>
<li>在网上发言评论</li>
<li>…</li>
</ul>
<p>CNN并不能处理这样的数据，所以这时设计出了RNN：<strong>循环神经网络</strong></p>
<h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><p>有两种自回归模型：</p>
<ul>
<li>自回归模型：假设当前的x只和过去的$\tau$个变量有关，于是我们可以根据过去$\tau$个变量，训练出一个MLP或者NN，这样有个好处是每次我们的输入都是固定个数的，都是$\tau$个</li>
<li>隐变量自回归模型：隐变量模型是在当前引入了一个隐变量，当前的输入和隐变量h以及前一时刻的x有关，可以用下图表示</li>
</ul>
<p>![截屏2023-08-16 13.35.41](.&#x2F;d2l&#x2F;截屏2023-08-16 13.35.41.png)</p>
<p>马尔可夫模型就是根据前$\tau$个数据进行估计。如果$\tau&#x3D;1$我们就得到一个一阶的马尔可夫模型。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/07/31/d2l/" data-id="cm9shytpw004nv7nnftkee6ug" data-title="d2l" class="article-share-link">Share</a>
      
      
      
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cnn/" rel="tag">cnn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rnn/" rel="tag">rnn</a></li></ul>


    </footer>
  </div>
  
    
  <nav id="article-nav" class="wow fadeInUp">
    
      <div class="article-nav-link-wrap article-nav-link-left">
        
          
          
            <img data-src="/covers/IMG_2578.JPG" data-sizes="auto" alt="EasyRL" class="lazyload">
          
        
        <a href="/2023/09/02/EasyRL/"></a>
        <div class="article-nav-caption">Newer</div>
        <h3 class="article-nav-title">
          
            EasyRL
          
        </h3>
      </div>
    
    
    <div class="article-nav-link-wrap article-nav-link-right">
      
        
        
          <img data-src="/covers/IMG_2581.JPG" data-sizes="auto" alt="嵌入式操作系统-MicroCOS III" class="lazyload">
        
      
      <a href="/2022/05/26/MicroCOS-III/"></a>
      <div class="article-nav-caption">Older</div>
      <h3 class="article-nav-title">
        
          嵌入式操作系统-MicroCOS III
        
      </h3>
    </div>
    
  </nav>


  
</article>






</section>
          
            <aside id="sidebar">
  <div class="sidebar-wrap wow fadeInRight wrap-sticky">
    
      <div class="sidebar-toc-sidebar"><div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class" >
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8QA"><span class="toc-number">1.</span> <span class="toc-text">经典QA</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%981-%E6%80%8E%E4%B9%88%E6%A0%B9%E6%8D%AE%E8%BE%93%E5%85%A5%E7%A9%BA%E9%97%B4%EF%BC%8C%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%88%96%E8%80%85%E5%AE%BD%E5%BA%A6"><span class="toc-number">1.1.</span> <span class="toc-text">问题1.怎么根据输入空间，选择最优的深度或者宽度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%982-k%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E7%A1%AE%E5%AE%9A%E8%B6%85%E5%8F%82%E6%95%B0%E5%90%97%EF%BC%9F%E7%84%B6%E5%90%8E%E8%BF%98%E8%A6%81%E7%94%A8%E8%BF%99%E4%B8%AA%E8%B6%85%E5%8F%82%E6%95%B0%E5%86%8D%E8%AE%AD%E7%BB%83%E4%B8%80%E9%81%8D%E5%85%A8%E6%95%B0%E6%95%B0%E6%8D%AE%E5%90%97%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">问题2.k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数数据吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%983-%E8%80%81%E5%B8%88%E8%AF%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E4%B8%80%E7%A7%8D%E8%AF%AD%E8%A8%80%EF%BC%8C%E6%84%8F%E6%80%9D%E6%98%AF%E5%88%A9%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%BB%E5%AF%B9%E4%B8%87%E4%BA%8B%E4%B8%87%E7%89%A9%E5%BB%BA%E6%A8%A1%E5%90%A7%EF%BC%9F%E5%B0%B1%E6%98%AF%E6%8C%87%E7%9A%84%E5%AE%83%E7%90%86%E8%AE%BA%E4%B8%8A%E8%83%BD%E6%8B%9F%E5%90%88%E6%89%80%E6%9C%89%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">问题3.老师说的神经网络是一种语言，意思是利用神经网络去对万事万物建模吧？就是指的它理论上能拟合所有函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%984-%E5%A6%82%E6%9E%9C%E8%AE%AD%E7%BB%83%E6%98%AF%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%EF%BC%8C%E6%98%AF%E5%90%A6%E8%A6%81%E5%85%88%E8%80%83%E8%99%91%E6%B5%8B%E8%AF%95%E9%9B%86%E6%98%AF%E5%90%A6%E4%B9%9F%E6%98%AF%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%EF%BC%8C%E5%86%8D%E5%8E%BB%E5%86%B3%E5%AE%9A%E6%98%AF%E5%90%A6%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%B9%B3%E8%A1%A1%E7%9A%84%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">问题4.如果训练是不平衡的，是否要先考虑测试集是否也是不平衡的，再去决定是否使用一个平衡的验证集？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%985-%E8%80%81%E5%B8%88%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%B916%E4%BD%8D%E6%B5%AE%E7%82%B9%E5%BD%B1%E5%93%8D%E4%B8%A5%E9%87%8D%EF%BC%9F32%E4%BD%8D%E6%88%96%E8%80%8564%E4%BD%8D%E5%B0%B1%E5%A5%BD%E4%BA%86%E5%90%97%EF%BC%9F%E9%82%A3%E5%B0%B1%E6%98%AF%E8%AF%B4%E6%89%80%E6%9C%89%E9%80%9A%E8%BF%87fp16%E5%8A%A0%E9%80%9F%E6%88%96%E8%80%85%E5%87%8F%E5%B0%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%E9%83%BD%E5%AD%98%E5%9C%A8%E5%AE%B9%E6%98%93%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%88%96%E8%80%85%E6%B6%88%E5%A4%B1%E7%9A%84%E9%A3%8E%E9%99%A9%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">问题5.老师，为什么对16位浮点影响严重？32位或者64位就好了吗？那就是说所有通过fp16加速或者减小模型的方法都存在容易梯度爆炸或者消失的风险？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%986-%E8%BF%99%E5%87%A0%E4%B8%AA%E8%B6%85%E5%8F%82%E6%95%B0%E5%BE%97%E5%BD%B1%E5%93%8D%E9%87%8D%E8%A6%81%E7%A8%8B%E5%BA%A6%E6%8E%92%E5%BA%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E5%BE%97%EF%BC%8C%E6%A0%B8%E5%A4%A7%E5%B0%8F%EF%BC%8C%E5%A1%AB%E5%85%85%EF%BC%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">1.6.</span> <span class="toc-text">问题6.这几个超参数得影响重要程度排序是怎样得，核大小，填充，步幅</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">2.</span> <span class="toc-text">环境</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.</span> <span class="toc-text">安装环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80swap"><span class="toc-number">2.2.</span> <span class="toc-text">开swap</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#htop"><span class="toc-number">2.3.</span> <span class="toc-text">htop</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">3.2.</span> <span class="toc-text">softmax回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">4.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">4.1.</span> <span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU"><span class="toc-number">4.2.1.</span> <span class="toc-text">ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid"><span class="toc-number">4.2.2.</span> <span class="toc-text">sigmoid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tanh"><span class="toc-number">4.2.3.</span> <span class="toc-text">tanh</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.3.</span> <span class="toc-text">模型选择、欠拟合、过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="toc-number">4.3.1.</span> <span class="toc-text">统计学习理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="toc-number">4.3.2.</span> <span class="toc-text">模型复杂性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">4.3.3.</span> <span class="toc-text">验证集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.3.4.</span> <span class="toc-text">欠拟合和过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F"><span class="toc-number">4.4.</span> <span class="toc-text">权重衰减</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">4.4.1.</span> <span class="toc-text">范数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95%EF%BC%88Dropout%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">暂退法（Dropout）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.5.1.</span> <span class="toc-text">过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%B3%E5%81%A5%E6%80%A7"><span class="toc-number">4.5.2.</span> <span class="toc-text">稳健性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">4.6.</span> <span class="toc-text">前向传播、反向传播、计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.6.1.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.6.2.</span> <span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.7.</span> <span class="toc-text">数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-%E6%B6%88%E5%A4%B1"><span class="toc-number">4.7.1.</span> <span class="toc-text">梯度爆炸&#x2F;消失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%93%E7%A0%B4%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="toc-number">4.7.2.</span> <span class="toc-text">打破对称性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.</span> <span class="toc-text">环境和分布偏移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E5%8F%98%E9%87%8F%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.1.</span> <span class="toc-text">协变量偏移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E7%AD%BE%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.2.</span> <span class="toc-text">标签偏移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.3.</span> <span class="toc-text">概念偏移</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN"><span class="toc-number">5.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8EMLP%E5%88%B0CNN"><span class="toc-number">5.1.</span> <span class="toc-text">从MLP到CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%92%E7%9B%B8%E5%85%B3%E8%BF%90%E7%AE%97"><span class="toc-number">5.2.</span> <span class="toc-text">互相关运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">5.3.</span> <span class="toc-text">填充和步幅</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-number">5.3.1.</span> <span class="toc-text">填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%AA%E5%B1%8F2023-08-09-20-54-35-d2l-%E6%88%AA%E5%B1%8F2023-08-09-20-54-35-png-%E6%AD%A5%E5%B9%85"><span class="toc-number">5.3.2.</span> <span class="toc-text">![截屏2023-08-09 20.54.35](.&#x2F;d2l&#x2F;截屏2023-08-09 20.54.35.png)步幅</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">5.4.</span> <span class="toc-text">多输入输出通道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">5.4.1.</span> <span class="toc-text">多输入通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">5.4.2.</span> <span class="toc-text">多输出通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-times1-%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">5.4.3.</span> <span class="toc-text">$1\times1$卷积核</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">5.5.</span> <span class="toc-text">汇聚层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LeNet"><span class="toc-number">5.6.</span> <span class="toc-text">LeNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">现代卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet"><span class="toc-number">6.1.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG"><span class="toc-number">6.2.</span> <span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NiN"><span class="toc-number">6.3.</span> <span class="toc-text">NiN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GoogLeNet"><span class="toc-number">6.4.</span> <span class="toc-text">GoogLeNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-normalization"><span class="toc-number">6.5.</span> <span class="toc-text">batch normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet"><span class="toc-number">6.6.</span> <span class="toc-text">ResNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%AE%AD%E7%BB%83%E2%80%93MLP-CNN"><span class="toc-number">7.</span> <span class="toc-text">代码训练–MLP+CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1"><span class="toc-number">7.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">7.2.</span> <span class="toc-text">softmax线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MLP"><span class="toc-number">7.3.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LeNet-1"><span class="toc-number">7.4.</span> <span class="toc-text">LeNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet-1"><span class="toc-number">7.5.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet-1"><span class="toc-number">7.6.</span> <span class="toc-text">ResNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RNN"><span class="toc-number">8.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.1.</span> <span class="toc-text">序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.1.1.</span> <span class="toc-text">自回归模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.2.</span> <span class="toc-text">语言模型</span></a></li></ol></li></ol>
      
  </div>
</div>
</div>
      <div class="sidebar-common-sidebar hidden"><div class="sidebar-author">
  <img data-src="/avatar/rabbit_1.jpg" data-sizes="auto" alt="chengyiqiu" class="lazyload">
  <div class="sidebar-author-name">chengyiqiu</div>
  <div class="sidebar-description"></div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>Posts</div>
    <div class="sidebar-state-number">65</div>
  </div>
  <div class="sidebar-state-category">
    <div>Categories</div>
    <div class="sidebar-state-number">13</div>
  </div>
  <div class="sidebar-state-tag">
    <div>Tags</div>
    <div class="sidebar-state-number">17</div>
  </div>
</div>
<div class="sidebar-social">
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/" aria-label="Home"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Home</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/archives" aria-label="Archives"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Archives</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/about" aria-label="About"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">About</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/friend" aria-label="Friend"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Friend</div>
    </div>
  
</div>
</div>
    
    
      <div class="sidebar-btn-wrapper" style="position:static">
        <div class="sidebar-toc-btn current"></div>
        <div class="sidebar-common-btn"></div>
      </div>
    
  </div>

  
</aside>

          
        </div>
        <footer id="footer" class="wow fadeInUp">
  <div style="width: 100%; overflow: hidden">
    <div class="footer-line"></div>
  </div>
  <div class="outer">
    <div id="footer-info" class="inner">
      
      <div>
        <span class="icon-copyright"></span>
        2020-2025
        <span class="footer-info-sep"></span>
        chengyiqiu
      </div>
      
        <div>
          Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
          Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
        </div>
      
      
        <div>
          <span class="icon-brush"></span>
          138.7k
          &nbsp;|&nbsp;
          <span class="icon-coffee"></span>
          09:27
        </div>
      
      
        <div>
          <span class="icon-eye"></span>
          <span id="busuanzi_container_site_pv">Number of visits&nbsp;<span id="busuanzi_value_site_pv"></span></span>
          &nbsp;|&nbsp;
          <span class="icon-user"></span>
          <span id="busuanzi_container_site_uv">Number of visitors&nbsp;<span id="busuanzi_value_site_uv"></span></span>
        </div>
      
    </div>
  </div>
</footer>

        <div class="sidebar-top">
          <img src="/images/taichi.png" height="50" width="50" />
          <div class="arrow-up"></div>
        </div>
        <div id="mask"></div>
      </div>
      <nav id="mobile-nav">
  <div class="sidebar-wrap">
    
      <div class="sidebar-toc-sidebar"><div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class" >
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8QA"><span class="toc-number">1.</span> <span class="toc-text">经典QA</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%981-%E6%80%8E%E4%B9%88%E6%A0%B9%E6%8D%AE%E8%BE%93%E5%85%A5%E7%A9%BA%E9%97%B4%EF%BC%8C%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%88%96%E8%80%85%E5%AE%BD%E5%BA%A6"><span class="toc-number">1.1.</span> <span class="toc-text">问题1.怎么根据输入空间，选择最优的深度或者宽度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%982-k%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E7%A1%AE%E5%AE%9A%E8%B6%85%E5%8F%82%E6%95%B0%E5%90%97%EF%BC%9F%E7%84%B6%E5%90%8E%E8%BF%98%E8%A6%81%E7%94%A8%E8%BF%99%E4%B8%AA%E8%B6%85%E5%8F%82%E6%95%B0%E5%86%8D%E8%AE%AD%E7%BB%83%E4%B8%80%E9%81%8D%E5%85%A8%E6%95%B0%E6%95%B0%E6%8D%AE%E5%90%97%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">问题2.k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数数据吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%983-%E8%80%81%E5%B8%88%E8%AF%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E4%B8%80%E7%A7%8D%E8%AF%AD%E8%A8%80%EF%BC%8C%E6%84%8F%E6%80%9D%E6%98%AF%E5%88%A9%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%BB%E5%AF%B9%E4%B8%87%E4%BA%8B%E4%B8%87%E7%89%A9%E5%BB%BA%E6%A8%A1%E5%90%A7%EF%BC%9F%E5%B0%B1%E6%98%AF%E6%8C%87%E7%9A%84%E5%AE%83%E7%90%86%E8%AE%BA%E4%B8%8A%E8%83%BD%E6%8B%9F%E5%90%88%E6%89%80%E6%9C%89%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">问题3.老师说的神经网络是一种语言，意思是利用神经网络去对万事万物建模吧？就是指的它理论上能拟合所有函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%984-%E5%A6%82%E6%9E%9C%E8%AE%AD%E7%BB%83%E6%98%AF%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%EF%BC%8C%E6%98%AF%E5%90%A6%E8%A6%81%E5%85%88%E8%80%83%E8%99%91%E6%B5%8B%E8%AF%95%E9%9B%86%E6%98%AF%E5%90%A6%E4%B9%9F%E6%98%AF%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%EF%BC%8C%E5%86%8D%E5%8E%BB%E5%86%B3%E5%AE%9A%E6%98%AF%E5%90%A6%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%B9%B3%E8%A1%A1%E7%9A%84%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">问题4.如果训练是不平衡的，是否要先考虑测试集是否也是不平衡的，再去决定是否使用一个平衡的验证集？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%985-%E8%80%81%E5%B8%88%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%B916%E4%BD%8D%E6%B5%AE%E7%82%B9%E5%BD%B1%E5%93%8D%E4%B8%A5%E9%87%8D%EF%BC%9F32%E4%BD%8D%E6%88%96%E8%80%8564%E4%BD%8D%E5%B0%B1%E5%A5%BD%E4%BA%86%E5%90%97%EF%BC%9F%E9%82%A3%E5%B0%B1%E6%98%AF%E8%AF%B4%E6%89%80%E6%9C%89%E9%80%9A%E8%BF%87fp16%E5%8A%A0%E9%80%9F%E6%88%96%E8%80%85%E5%87%8F%E5%B0%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%E9%83%BD%E5%AD%98%E5%9C%A8%E5%AE%B9%E6%98%93%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%88%96%E8%80%85%E6%B6%88%E5%A4%B1%E7%9A%84%E9%A3%8E%E9%99%A9%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">问题5.老师，为什么对16位浮点影响严重？32位或者64位就好了吗？那就是说所有通过fp16加速或者减小模型的方法都存在容易梯度爆炸或者消失的风险？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%986-%E8%BF%99%E5%87%A0%E4%B8%AA%E8%B6%85%E5%8F%82%E6%95%B0%E5%BE%97%E5%BD%B1%E5%93%8D%E9%87%8D%E8%A6%81%E7%A8%8B%E5%BA%A6%E6%8E%92%E5%BA%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E5%BE%97%EF%BC%8C%E6%A0%B8%E5%A4%A7%E5%B0%8F%EF%BC%8C%E5%A1%AB%E5%85%85%EF%BC%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">1.6.</span> <span class="toc-text">问题6.这几个超参数得影响重要程度排序是怎样得，核大小，填充，步幅</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">2.</span> <span class="toc-text">环境</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.</span> <span class="toc-text">安装环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80swap"><span class="toc-number">2.2.</span> <span class="toc-text">开swap</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#htop"><span class="toc-number">2.3.</span> <span class="toc-text">htop</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">3.2.</span> <span class="toc-text">softmax回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">4.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">4.1.</span> <span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU"><span class="toc-number">4.2.1.</span> <span class="toc-text">ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid"><span class="toc-number">4.2.2.</span> <span class="toc-text">sigmoid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tanh"><span class="toc-number">4.2.3.</span> <span class="toc-text">tanh</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.3.</span> <span class="toc-text">模型选择、欠拟合、过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="toc-number">4.3.1.</span> <span class="toc-text">统计学习理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="toc-number">4.3.2.</span> <span class="toc-text">模型复杂性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">4.3.3.</span> <span class="toc-text">验证集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.3.4.</span> <span class="toc-text">欠拟合和过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F"><span class="toc-number">4.4.</span> <span class="toc-text">权重衰减</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">4.4.1.</span> <span class="toc-text">范数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95%EF%BC%88Dropout%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">暂退法（Dropout）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.5.1.</span> <span class="toc-text">过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%B3%E5%81%A5%E6%80%A7"><span class="toc-number">4.5.2.</span> <span class="toc-text">稳健性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">4.6.</span> <span class="toc-text">前向传播、反向传播、计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.6.1.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.6.2.</span> <span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.7.</span> <span class="toc-text">数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-%E6%B6%88%E5%A4%B1"><span class="toc-number">4.7.1.</span> <span class="toc-text">梯度爆炸&#x2F;消失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%93%E7%A0%B4%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="toc-number">4.7.2.</span> <span class="toc-text">打破对称性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.</span> <span class="toc-text">环境和分布偏移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E5%8F%98%E9%87%8F%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.1.</span> <span class="toc-text">协变量偏移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E7%AD%BE%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.2.</span> <span class="toc-text">标签偏移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E5%81%8F%E7%A7%BB"><span class="toc-number">4.8.3.</span> <span class="toc-text">概念偏移</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN"><span class="toc-number">5.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8EMLP%E5%88%B0CNN"><span class="toc-number">5.1.</span> <span class="toc-text">从MLP到CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%92%E7%9B%B8%E5%85%B3%E8%BF%90%E7%AE%97"><span class="toc-number">5.2.</span> <span class="toc-text">互相关运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">5.3.</span> <span class="toc-text">填充和步幅</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-number">5.3.1.</span> <span class="toc-text">填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%AA%E5%B1%8F2023-08-09-20-54-35-d2l-%E6%88%AA%E5%B1%8F2023-08-09-20-54-35-png-%E6%AD%A5%E5%B9%85"><span class="toc-number">5.3.2.</span> <span class="toc-text">![截屏2023-08-09 20.54.35](.&#x2F;d2l&#x2F;截屏2023-08-09 20.54.35.png)步幅</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">5.4.</span> <span class="toc-text">多输入输出通道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">5.4.1.</span> <span class="toc-text">多输入通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">5.4.2.</span> <span class="toc-text">多输出通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-times1-%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">5.4.3.</span> <span class="toc-text">$1\times1$卷积核</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">5.5.</span> <span class="toc-text">汇聚层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LeNet"><span class="toc-number">5.6.</span> <span class="toc-text">LeNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">现代卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet"><span class="toc-number">6.1.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG"><span class="toc-number">6.2.</span> <span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NiN"><span class="toc-number">6.3.</span> <span class="toc-text">NiN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GoogLeNet"><span class="toc-number">6.4.</span> <span class="toc-text">GoogLeNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-normalization"><span class="toc-number">6.5.</span> <span class="toc-text">batch normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet"><span class="toc-number">6.6.</span> <span class="toc-text">ResNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%AE%AD%E7%BB%83%E2%80%93MLP-CNN"><span class="toc-number">7.</span> <span class="toc-text">代码训练–MLP+CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1"><span class="toc-number">7.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">7.2.</span> <span class="toc-text">softmax线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MLP"><span class="toc-number">7.3.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LeNet-1"><span class="toc-number">7.4.</span> <span class="toc-text">LeNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet-1"><span class="toc-number">7.5.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet-1"><span class="toc-number">7.6.</span> <span class="toc-text">ResNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RNN"><span class="toc-number">8.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.1.</span> <span class="toc-text">序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.1.1.</span> <span class="toc-text">自回归模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.2.</span> <span class="toc-text">语言模型</span></a></li></ol></li></ol>
      
  </div>
</div>
</div>
      <div class="sidebar-common-sidebar hidden"><div class="sidebar-author">
  <img data-src="/avatar/rabbit_1.jpg" data-sizes="auto" alt="chengyiqiu" class="lazyload">
  <div class="sidebar-author-name">chengyiqiu</div>
  <div class="sidebar-description"></div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>Posts</div>
    <div class="sidebar-state-number">65</div>
  </div>
  <div class="sidebar-state-category">
    <div>Categories</div>
    <div class="sidebar-state-number">13</div>
  </div>
  <div class="sidebar-state-tag">
    <div>Tags</div>
    <div class="sidebar-state-number">17</div>
  </div>
</div>
<div class="sidebar-social">
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/" aria-label="Home"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Home</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/archives" aria-label="Archives"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Archives</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/about" aria-label="About"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">About</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/friend" aria-label="Friend"></a>
      <span class="sidebar-menu-icon"></span>
      <div class="sidebar-menu-link">Friend</div>
    </div>
  
</div>
</div>
    
  </div>
  
    <div class="sidebar-btn-wrapper">
      <div class="sidebar-toc-btn current"></div>
      <div class="sidebar-common-btn"></div>
    </div>
  
</nav>

    </div>
    <div class="site-search">
      <div class="reimu-popup popup">
        <div class="reimu-search">
          <span class="reimu-search-input-icon"></span>
          <div class="reimu-search-input" id="reimu-search-input"></div>
        </div>
        <div class="reimu-results">
          <div id="reimu-stats"></div>
          <div id="reimu-hits"></div>
          <div id="reimu-pagination" class="reimu-pagination"></div>
        </div>
        <span class="popup-btn-close"></span>
      </div>
    </div>
    
<script src="https://npm.webcache.cn/jquery@3.7.1/dist/jquery.min.js"></script>


<script src="https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js"></script>



  
<script src="https://npm.webcache.cn/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" defer></script>



  
<script src="https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js" async></script>






<script src="/js/pjax_script.js" data-pjax></script>

















  
<script src="https://npm.webcache.cn/mouse-firework@0.0.4/dist/index.umd.js"></script>

  <script>
    firework(JSON.parse('{"excludeElements":["a","button"],"particles":[{"shape":"circle","move":["emit"],"easing":"easeOutExpo","colors":["#ff5252","#ff7c7c","#ffafaf","#ffd0d0"],"number":20,"duration":[1200,1800],"shapeOptions":{"radius":[16,32],"alpha":[0.3,0.5]}},{"shape":"circle","move":["diffuse"],"easing":"easeOutExpo","colors":["#ff0000"],"number":1,"duration":[1200,1800],"shapeOptions":{"radius":20,"alpha":[0.2,0.5],"lineWidth":6}}]}'))
  </script>







<script src="/js/script.js"></script>



  <script>
    console.log(String.raw`%c 
 ______     ______     __     __    __     __  __    
/\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
\ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
 \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
  \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                  
`,'color: #ff5252;')
    console.log('%c Theme.Reimu v' + '0.1.2' + ' %c https://github.com/D-Sketon/hexo-theme-reimu ', 'color: white; background: #ff5252; padding:5px 0;', 'padding:4px;border:1px solid #ff5252;')
  </script>
  

  <!-- hexo injector body_end start -->
<script src="/js/insert_highlight.js" data-pjax></script>
<!-- hexo injector body_end end --></body>
  </html>

